{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bigram model using probablities based on normalized counts has it's limitations.\n",
    "\n",
    "To extend it to have more context like a two characters as input the probabalities matrix will have (27*27) possilities and for three characters (27 * 27 * 27) and becomes too big.\n",
    "\n",
    "To overcome this we're gonna try out [Bengion et al.2003 MLP model paper](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbVhPSHNrVkluYVY5elh4RDZrOWd4R2xVNVRQd3xBQ3Jtc0tsUDE0UFVRQURUTTlWckExZWp4eGxFa3lRYlQ3amtYX3kxdDI1ZW5uU1pxZERidUYyZkJjSVlzd21rMndCMFlYRW5kYmZISkxfSDR1TzhaOXI1bXptUnUxU0xyUXJYeEpTZlRrTkRjTS0wTkMxNjFnSQ&q=https%3A%2F%2Fwww.jmlr.org%2Fpapers%2Fvolume3%2Fbengio03a%2Fbengio03a.pdf&v=TCH_1BHY58I)\n",
    "\n",
    "* This paper uses words but we'll proceed with characters\n",
    "* Each character will be represented as a 30 dimensional vector \n",
    "* The advantages of embeddings is knowledge transference, for examples animals like dog, cat might be closer to each other in 30 dimensional space. If cat was not in training set but this knowledge transfer will help in this case.\n",
    "\n",
    "Let's implement the below architecture in this notebook\n",
    "![fully connected MLP](https://pbs.twimg.com/media/Fhzl42hVUAI9U8V?format=jpg&name=large)\n",
    "\n",
    "* Three input characters with 30 dimensional embedding each\n",
    "* A Lookup table for characters\n",
    "* Tanh activation connected to three inputs\n",
    "* since we have 27 characters a final layer with 27 units(logits)\n",
    "* softmax on top of it to normalize the probabality\n",
    "* pluck the label based on probabality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:31.313008Z",
     "start_time": "2023-02-08T05:47:25.987840Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rebuilding training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:31.330066Z",
     "start_time": "2023-02-08T05:47:31.317289Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read all words\n",
    "def read_words():\n",
    "    words = open(\"names.txt\").read().splitlines()\n",
    "    return words\n",
    "words = read_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:31.376304Z",
     "start_time": "2023-02-08T05:47:31.339831Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:31.403383Z",
     "start_time": "2023-02-08T05:47:31.390165Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi  = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:31.426980Z",
     "start_time": "2023-02-08T05:47:31.413084Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_dataset(block_size, number_of_words: int, logs=False):\n",
    "\n",
    "    block_size = block_size # Context ength: how many characters do we take to predict the next one?\n",
    "    X, Y = [], []\n",
    "    for w in words[:number_of_words]:\n",
    "\n",
    "        print(w)\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            print(f\"Context: {context}\")\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            if logs:\n",
    "                print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "            context = context[1:] + [ix]\n",
    "            print(f\"Context after append: {context}\")\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:31.442227Z",
     "start_time": "2023-02-08T05:47:31.434452Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "Context: [0, 0, 0]\n",
      "... ---> e\n",
      "Context after append: [0, 0, 5]\n",
      "Context: [0, 0, 5]\n",
      "..e ---> m\n",
      "Context after append: [0, 5, 13]\n",
      "Context: [0, 5, 13]\n",
      ".em ---> m\n",
      "Context after append: [5, 13, 13]\n",
      "Context: [5, 13, 13]\n",
      "emm ---> a\n",
      "Context after append: [13, 13, 1]\n",
      "Context: [13, 13, 1]\n",
      "mma ---> .\n",
      "Context after append: [13, 1, 0]\n",
      "olivia\n",
      "Context: [0, 0, 0]\n",
      "... ---> o\n",
      "Context after append: [0, 0, 15]\n",
      "Context: [0, 0, 15]\n",
      "..o ---> l\n",
      "Context after append: [0, 15, 12]\n",
      "Context: [0, 15, 12]\n",
      ".ol ---> i\n",
      "Context after append: [15, 12, 9]\n",
      "Context: [15, 12, 9]\n",
      "oli ---> v\n",
      "Context after append: [12, 9, 22]\n",
      "Context: [12, 9, 22]\n",
      "liv ---> i\n",
      "Context after append: [9, 22, 9]\n",
      "Context: [9, 22, 9]\n",
      "ivi ---> a\n",
      "Context after append: [22, 9, 1]\n",
      "Context: [22, 9, 1]\n",
      "via ---> .\n",
      "Context after append: [9, 1, 0]\n",
      "ava\n",
      "Context: [0, 0, 0]\n",
      "... ---> a\n",
      "Context after append: [0, 0, 1]\n",
      "Context: [0, 0, 1]\n",
      "..a ---> v\n",
      "Context after append: [0, 1, 22]\n",
      "Context: [0, 1, 22]\n",
      ".av ---> a\n",
      "Context after append: [1, 22, 1]\n",
      "Context: [1, 22, 1]\n",
      "ava ---> .\n",
      "Context after append: [22, 1, 0]\n",
      "isabella\n",
      "Context: [0, 0, 0]\n",
      "... ---> i\n",
      "Context after append: [0, 0, 9]\n",
      "Context: [0, 0, 9]\n",
      "..i ---> s\n",
      "Context after append: [0, 9, 19]\n",
      "Context: [0, 9, 19]\n",
      ".is ---> a\n",
      "Context after append: [9, 19, 1]\n",
      "Context: [9, 19, 1]\n",
      "isa ---> b\n",
      "Context after append: [19, 1, 2]\n",
      "Context: [19, 1, 2]\n",
      "sab ---> e\n",
      "Context after append: [1, 2, 5]\n",
      "Context: [1, 2, 5]\n",
      "abe ---> l\n",
      "Context after append: [2, 5, 12]\n",
      "Context: [2, 5, 12]\n",
      "bel ---> l\n",
      "Context after append: [5, 12, 12]\n",
      "Context: [5, 12, 12]\n",
      "ell ---> a\n",
      "Context after append: [12, 12, 1]\n",
      "Context: [12, 12, 1]\n",
      "lla ---> .\n",
      "Context after append: [12, 1, 0]\n",
      "sophia\n",
      "Context: [0, 0, 0]\n",
      "... ---> s\n",
      "Context after append: [0, 0, 19]\n",
      "Context: [0, 0, 19]\n",
      "..s ---> o\n",
      "Context after append: [0, 19, 15]\n",
      "Context: [0, 19, 15]\n",
      ".so ---> p\n",
      "Context after append: [19, 15, 16]\n",
      "Context: [19, 15, 16]\n",
      "sop ---> h\n",
      "Context after append: [15, 16, 8]\n",
      "Context: [15, 16, 8]\n",
      "oph ---> i\n",
      "Context after append: [16, 8, 9]\n",
      "Context: [16, 8, 9]\n",
      "phi ---> a\n",
      "Context after append: [8, 9, 1]\n",
      "Context: [8, 9, 1]\n",
      "hia ---> .\n",
      "Context after append: [9, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "X, Y = build_dataset(block_size=3, number_of_words=5, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:31.459477Z",
     "start_time": "2023-02-08T05:47:31.448995Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:31.471780Z",
     "start_time": "2023-02-08T05:47:31.462583Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0],\n",
       "        [ 0,  0,  5],\n",
       "        [ 0,  5, 13],\n",
       "        [ 5, 13, 13],\n",
       "        [13, 13,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 15],\n",
       "        [ 0, 15, 12],\n",
       "        [15, 12,  9],\n",
       "        [12,  9, 22],\n",
       "        [ 9, 22,  9],\n",
       "        [22,  9,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  1],\n",
       "        [ 0,  1, 22],\n",
       "        [ 1, 22,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  9],\n",
       "        [ 0,  9, 19],\n",
       "        [ 9, 19,  1],\n",
       "        [19,  1,  2],\n",
       "        [ 1,  2,  5],\n",
       "        [ 2,  5, 12],\n",
       "        [ 5, 12, 12],\n",
       "        [12, 12,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 19],\n",
       "        [ 0, 19, 15],\n",
       "        [19, 15, 16],\n",
       "        [15, 16,  8],\n",
       "        [16,  8,  9],\n",
       "        [ 8,  9,  1]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:31.487751Z",
     "start_time": "2023-02-08T05:47:31.475262Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,\n",
       "         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've the dataset, let's build the embedding lookup table\n",
    "\n",
    "## Embedding lookup table\n",
    "\n",
    "For 1700 words, 30 dimension space was used in paper. For 27 possiblities(characters) let's try a 2 dimensionsal embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:31.528393Z",
     "start_time": "2023-02-08T05:47:31.509917Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 2.0444, -0.8643],\n",
       "         [ 0.4949,  0.6256],\n",
       "         [ 0.3523, -1.5919],\n",
       "         [-0.5518,  1.1153],\n",
       "         [ 0.0844,  0.6198],\n",
       "         [ 1.2884, -0.5162],\n",
       "         [ 0.3585, -0.8834],\n",
       "         [-0.8051, -0.1403],\n",
       "         [-0.7723, -0.6009],\n",
       "         [-0.8732, -0.0418],\n",
       "         [ 0.0856,  1.7633],\n",
       "         [ 0.1055, -1.2005],\n",
       "         [ 0.8034,  1.5798],\n",
       "         [ 1.5923,  0.8133],\n",
       "         [-0.8429, -1.2364],\n",
       "         [ 0.4664, -1.3050],\n",
       "         [-0.2633, -0.9519],\n",
       "         [ 1.0584, -0.9919],\n",
       "         [ 0.0866,  0.9613],\n",
       "         [ 0.7803,  1.2683],\n",
       "         [ 1.9862, -0.5937],\n",
       "         [ 0.8093, -0.0125],\n",
       "         [-0.3031, -0.5510],\n",
       "         [ 0.5475,  1.2947],\n",
       "         [ 1.0933, -0.8364],\n",
       "         [-0.6993,  1.9711],\n",
       "         [ 0.0128, -0.2101]]),\n",
       " torch.Size([27, 2]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialized randomnly\n",
    "C = torch.randn((27, 2))\n",
    "C, C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:31.552439Z",
     "start_time": "2023-02-08T05:47:31.537264Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.2884, -0.5162])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The lookup of embedding for single character can be done two ways\n",
    "# 1. Indexing\n",
    "C[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:31.574585Z",
     "start_time": "2023-02-08T05:47:31.560462Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.2884, -0.5162])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Onehot\n",
    "F.one_hot(torch.tensor(5), num_classes=27).float() @ C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing and one hot encoding gives the same results. We'll use indexing as it's faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:31.600985Z",
     "start_time": "2023-02-08T05:47:31.585045Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2884, -0.5162],\n",
      "        [ 0.3585, -0.8834],\n",
      "        [-0.8051, -0.1403]])\n",
      "tensor([[ 1.2884, -0.5162],\n",
      "        [ 0.3585, -0.8834],\n",
      "        [-0.8051, -0.1403]])\n"
     ]
    }
   ],
   "source": [
    "# Indexing multiple values\n",
    "# Singce our shape of input is 32, 3\n",
    "print(C[[5, 6, 7]])\n",
    "# Works also with tensor\n",
    "print(C[torch.tensor([5, 6, 7])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:31.623028Z",
     "start_time": "2023-02-08T05:47:31.607657Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.0444, -0.8643],\n",
       "         [ 2.0444, -0.8643],\n",
       "         [ 2.0444, -0.8643]],\n",
       "\n",
       "        [[ 2.0444, -0.8643],\n",
       "         [ 2.0444, -0.8643],\n",
       "         [ 1.2884, -0.5162]],\n",
       "\n",
       "        [[ 2.0444, -0.8643],\n",
       "         [ 1.2884, -0.5162],\n",
       "         [ 1.5923,  0.8133]],\n",
       "\n",
       "        [[ 1.2884, -0.5162],\n",
       "         [ 1.5923,  0.8133],\n",
       "         [ 1.5923,  0.8133]],\n",
       "\n",
       "        [[ 1.5923,  0.8133],\n",
       "         [ 1.5923,  0.8133],\n",
       "         [ 0.4949,  0.6256]],\n",
       "\n",
       "        [[ 2.0444, -0.8643],\n",
       "         [ 2.0444, -0.8643],\n",
       "         [ 2.0444, -0.8643]],\n",
       "\n",
       "        [[ 2.0444, -0.8643],\n",
       "         [ 2.0444, -0.8643],\n",
       "         [ 0.4664, -1.3050]],\n",
       "\n",
       "        [[ 2.0444, -0.8643],\n",
       "         [ 0.4664, -1.3050],\n",
       "         [ 0.8034,  1.5798]],\n",
       "\n",
       "        [[ 0.4664, -1.3050],\n",
       "         [ 0.8034,  1.5798],\n",
       "         [-0.8732, -0.0418]],\n",
       "\n",
       "        [[ 0.8034,  1.5798],\n",
       "         [-0.8732, -0.0418],\n",
       "         [-0.3031, -0.5510]],\n",
       "\n",
       "        [[-0.8732, -0.0418],\n",
       "         [-0.3031, -0.5510],\n",
       "         [-0.8732, -0.0418]],\n",
       "\n",
       "        [[-0.3031, -0.5510],\n",
       "         [-0.8732, -0.0418],\n",
       "         [ 0.4949,  0.6256]],\n",
       "\n",
       "        [[ 2.0444, -0.8643],\n",
       "         [ 2.0444, -0.8643],\n",
       "         [ 2.0444, -0.8643]],\n",
       "\n",
       "        [[ 2.0444, -0.8643],\n",
       "         [ 2.0444, -0.8643],\n",
       "         [ 0.4949,  0.6256]],\n",
       "\n",
       "        [[ 2.0444, -0.8643],\n",
       "         [ 0.4949,  0.6256],\n",
       "         [-0.3031, -0.5510]],\n",
       "\n",
       "        [[ 0.4949,  0.6256],\n",
       "         [-0.3031, -0.5510],\n",
       "         [ 0.4949,  0.6256]],\n",
       "\n",
       "        [[ 2.0444, -0.8643],\n",
       "         [ 2.0444, -0.8643],\n",
       "         [ 2.0444, -0.8643]],\n",
       "\n",
       "        [[ 2.0444, -0.8643],\n",
       "         [ 2.0444, -0.8643],\n",
       "         [-0.8732, -0.0418]],\n",
       "\n",
       "        [[ 2.0444, -0.8643],\n",
       "         [-0.8732, -0.0418],\n",
       "         [ 0.7803,  1.2683]],\n",
       "\n",
       "        [[-0.8732, -0.0418],\n",
       "         [ 0.7803,  1.2683],\n",
       "         [ 0.4949,  0.6256]],\n",
       "\n",
       "        [[ 0.7803,  1.2683],\n",
       "         [ 0.4949,  0.6256],\n",
       "         [ 0.3523, -1.5919]],\n",
       "\n",
       "        [[ 0.4949,  0.6256],\n",
       "         [ 0.3523, -1.5919],\n",
       "         [ 1.2884, -0.5162]],\n",
       "\n",
       "        [[ 0.3523, -1.5919],\n",
       "         [ 1.2884, -0.5162],\n",
       "         [ 0.8034,  1.5798]],\n",
       "\n",
       "        [[ 1.2884, -0.5162],\n",
       "         [ 0.8034,  1.5798],\n",
       "         [ 0.8034,  1.5798]],\n",
       "\n",
       "        [[ 0.8034,  1.5798],\n",
       "         [ 0.8034,  1.5798],\n",
       "         [ 0.4949,  0.6256]],\n",
       "\n",
       "        [[ 2.0444, -0.8643],\n",
       "         [ 2.0444, -0.8643],\n",
       "         [ 2.0444, -0.8643]],\n",
       "\n",
       "        [[ 2.0444, -0.8643],\n",
       "         [ 2.0444, -0.8643],\n",
       "         [ 0.7803,  1.2683]],\n",
       "\n",
       "        [[ 2.0444, -0.8643],\n",
       "         [ 0.7803,  1.2683],\n",
       "         [ 0.4664, -1.3050]],\n",
       "\n",
       "        [[ 0.7803,  1.2683],\n",
       "         [ 0.4664, -1.3050],\n",
       "         [-0.2633, -0.9519]],\n",
       "\n",
       "        [[ 0.4664, -1.3050],\n",
       "         [-0.2633, -0.9519],\n",
       "         [-0.7723, -0.6009]],\n",
       "\n",
       "        [[-0.2633, -0.9519],\n",
       "         [-0.7723, -0.6009],\n",
       "         [-0.8732, -0.0418]],\n",
       "\n",
       "        [[-0.7723, -0.6009],\n",
       "         [-0.8732, -0.0418],\n",
       "         [ 0.4949,  0.6256]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The total equivalent would be\n",
    "C[X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:31.638506Z",
     "start_time": "2023-02-08T05:47:31.627352Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's verify this\n",
    "C[X].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "32 is total number of inputs with shape 3 and dimensional embedding 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:31.653540Z",
     "start_time": "2023-02-08T05:47:31.644716Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[13, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:31.678282Z",
     "start_time": "2023-02-08T05:47:31.662373Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4949, 0.6256])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X][13, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:31.697727Z",
     "start_time": "2023-02-08T05:47:31.684945Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4949, 0.6256])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:31.716937Z",
     "start_time": "2023-02-08T05:47:31.705431Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X]\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the embedding lookup table is completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the hidden layer plus internals of torch.Tensor, storage and views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:31.735053Z",
     "start_time": "2023-02-08T05:47:31.727489Z"
    }
   },
   "outputs": [],
   "source": [
    "# Intitializing weights and biases\n",
    "W1 = torch.randn((\n",
    "    6, # 3(inputs) * 2(embedding dim)\n",
    "    100 # Number of neurons\n",
    "))\n",
    "b1 = torch.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:31.753513Z",
     "start_time": "2023-02-08T05:47:31.742269Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 100])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:32.434474Z",
     "start_time": "2023-02-08T05:47:31.760805Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (96x2 and 6x100)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [44]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Inputs * weights + bias will not work  now\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# as dimensions of weighs and input doesn't abide\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# by matrix multiplication rulees\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# shape of input [32, 3, 2], weights [6, 100]\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43memb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m \u001b[38;5;241m+\u001b[39m b1\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (96x2 and 6x100)"
     ]
    }
   ],
   "source": [
    "# Inputs * weights + bias will not work  now\n",
    "# as dimensions of weighs and input doesn't abide\n",
    "# by matrix multiplication rulees\n",
    "# shape of input [32, 3, 2], weights [6, 100]\n",
    "emb @ W1 + b1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch's tensor's a really powerful, because ut has tons of methods to allow us to create modify and perfom lot's of operations on it.\n",
    "\n",
    "We're gonna use [torch.cat](https://pytorch.org/docs/stable/generated/torch.cat.html) to tackle the above problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:32.440581Z",
     "start_time": "2023-02-08T05:47:32.440548Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_tensors = torch.cat([emb[:, 0, :], emb[:, 1, :], emb[:, 2, :]], 1)\n",
    "cat_tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:32.444904Z",
     "start_time": "2023-02-08T05:47:32.444856Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To generalize this in case of diffrent block size\n",
    "# We'll use unbind with cat\n",
    "unbind_tensors = torch.unbind(emb, 1)\n",
    "# Gives a list which is exactly the same\n",
    "# as cat_tensors abov\n",
    "len(unbind_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:32.448000Z",
     "start_time": "2023-02-08T05:47:32.447968Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_unbind_tensors = torch.cat(unbind_tensors, 1)\n",
    "cat_unbind_tensors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now irrespective of block size the above code will run.\n",
    "\n",
    "But there's an efficient way to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:32.499894Z",
     "start_time": "2023-02-08T05:47:32.499844Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(18)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:32.504939Z",
     "start_time": "2023-02-08T05:47:32.504899Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:32.510427Z",
     "start_time": "2023-02-08T05:47:32.510355Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1],\n",
       "         [ 2,  3],\n",
       "         [ 4,  5]],\n",
       "\n",
       "        [[ 6,  7],\n",
       "         [ 8,  9],\n",
       "         [10, 11]],\n",
       "\n",
       "        [[12, 13],\n",
       "         [14, 15],\n",
       "         [16, 17]]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(3, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:32.518799Z",
     "start_time": "2023-02-08T05:47:32.518763Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 6\n",
       " 7\n",
       " 8\n",
       " 9\n",
       " 10\n",
       " 11\n",
       " 12\n",
       " 13\n",
       " 14\n",
       " 15\n",
       " 16\n",
       " 17\n",
       "[torch.storage._TypedStorage(dtype=torch.int64, device=cpu) of size 18]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.storage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every tensor has view and storage,\n",
    "* Using tensor.view(shape) we can manipulate the shape of an tensor\n",
    "* But tensor.storage() in memory will still remain a single dimension vector\n",
    "* And using view just changes some attributes like offest etc and tensor in memory remains same to the multiples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:32.522136Z",
     "start_time": "2023-02-08T05:47:32.522104Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0444, -0.8643,  2.0444, -0.8643,  2.0444, -0.8643],\n",
       "        [ 2.0444, -0.8643,  2.0444, -0.8643,  1.2884, -0.5162],\n",
       "        [ 2.0444, -0.8643,  1.2884, -0.5162,  1.5923,  0.8133],\n",
       "        [ 1.2884, -0.5162,  1.5923,  0.8133,  1.5923,  0.8133],\n",
       "        [ 1.5923,  0.8133,  1.5923,  0.8133,  0.4949,  0.6256],\n",
       "        [ 2.0444, -0.8643,  2.0444, -0.8643,  2.0444, -0.8643],\n",
       "        [ 2.0444, -0.8643,  2.0444, -0.8643,  0.4664, -1.3050],\n",
       "        [ 2.0444, -0.8643,  0.4664, -1.3050,  0.8034,  1.5798],\n",
       "        [ 0.4664, -1.3050,  0.8034,  1.5798, -0.8732, -0.0418],\n",
       "        [ 0.8034,  1.5798, -0.8732, -0.0418, -0.3031, -0.5510],\n",
       "        [-0.8732, -0.0418, -0.3031, -0.5510, -0.8732, -0.0418],\n",
       "        [-0.3031, -0.5510, -0.8732, -0.0418,  0.4949,  0.6256],\n",
       "        [ 2.0444, -0.8643,  2.0444, -0.8643,  2.0444, -0.8643],\n",
       "        [ 2.0444, -0.8643,  2.0444, -0.8643,  0.4949,  0.6256],\n",
       "        [ 2.0444, -0.8643,  0.4949,  0.6256, -0.3031, -0.5510],\n",
       "        [ 0.4949,  0.6256, -0.3031, -0.5510,  0.4949,  0.6256],\n",
       "        [ 2.0444, -0.8643,  2.0444, -0.8643,  2.0444, -0.8643],\n",
       "        [ 2.0444, -0.8643,  2.0444, -0.8643, -0.8732, -0.0418],\n",
       "        [ 2.0444, -0.8643, -0.8732, -0.0418,  0.7803,  1.2683],\n",
       "        [-0.8732, -0.0418,  0.7803,  1.2683,  0.4949,  0.6256],\n",
       "        [ 0.7803,  1.2683,  0.4949,  0.6256,  0.3523, -1.5919],\n",
       "        [ 0.4949,  0.6256,  0.3523, -1.5919,  1.2884, -0.5162],\n",
       "        [ 0.3523, -1.5919,  1.2884, -0.5162,  0.8034,  1.5798],\n",
       "        [ 1.2884, -0.5162,  0.8034,  1.5798,  0.8034,  1.5798],\n",
       "        [ 0.8034,  1.5798,  0.8034,  1.5798,  0.4949,  0.6256],\n",
       "        [ 2.0444, -0.8643,  2.0444, -0.8643,  2.0444, -0.8643],\n",
       "        [ 2.0444, -0.8643,  2.0444, -0.8643,  0.7803,  1.2683],\n",
       "        [ 2.0444, -0.8643,  0.7803,  1.2683,  0.4664, -1.3050],\n",
       "        [ 0.7803,  1.2683,  0.4664, -1.3050, -0.2633, -0.9519],\n",
       "        [ 0.4664, -1.3050, -0.2633, -0.9519, -0.7723, -0.6009],\n",
       "        [-0.2633, -0.9519, -0.7723, -0.6009, -0.8732, -0.0418],\n",
       "        [-0.7723, -0.6009, -0.8732, -0.0418,  0.4949,  0.6256]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's use view() to reshape the tensor from [32, 3, 2] to [32, 6]\n",
    "emb.view(32, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way this happens is dimension 1 get stacked up as a single dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:32.530603Z",
     "start_time": "2023-02-08T05:47:32.530557Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.view(32, 6).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:32.535899Z",
     "start_time": "2023-02-08T05:47:32.535862Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.view(32, 6) == torch.cat(torch.unbind(emb, 1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Element wise comparison proves that view is equal to cat(unbind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:32.538876Z",
     "start_time": "2023-02-08T05:47:32.538838Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000, -1.0000, -1.0000,  ...,  0.9884, -1.0000, -0.9874],\n",
       "        [ 0.9971, -0.9999, -1.0000,  ...,  0.9814, -1.0000, -0.9979],\n",
       "        [ 0.9997, -1.0000, -1.0000,  ...,  0.9335, -0.9999, -0.9999],\n",
       "        ...,\n",
       "        [-0.9523, -0.9799, -0.8882,  ...,  0.5090, -0.9854, -0.9972],\n",
       "        [-0.9684, -0.6631,  0.0346,  ..., -0.1564, -0.7355, -0.9919],\n",
       "        [ 0.9191, -0.9560,  0.7704,  ..., -0.3943, -0.8343, -0.8931]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:32.545348Z",
     "start_time": "2023-02-08T05:47:32.545317Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure broadcasting is done right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:32.550674Z",
     "start_time": "2023-02-08T05:47:32.550632Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(emb.view(32, 6) @ W1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:32.554705Z",
     "start_time": "2023-02-08T05:47:32.554665Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "32, 100\n",
    "1 , 100\n",
    "\n",
    "* broadcasting 32, 100 to 100\n",
    "* broadcasting aligns from right abd creates a  fake dimension (1)\n",
    "* Then 32 will be copied vertically for every element of 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = torch.randn((100, # Inputs layer size\n",
    "                  27 # Output layer 27 characters\n",
    "                 ))\n",
    "b2 = torch.randn(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = h @ W2 + b2\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implmenting negative log likelihood loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake counts -> logits exp\n",
    "counts = logits.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize fake counts\n",
    "prob = counts / counts.sum(1, keepdims=True)\n",
    "prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.1116e-06, 6.3760e-11, 4.6058e-11, 2.8328e-08, 2.5840e-03, 5.8854e-08,\n",
       "        5.4178e-04, 8.8482e-15, 2.6611e-05, 2.1139e-08, 1.7233e-09, 1.2606e-06,\n",
       "        6.3865e-11, 2.2311e-04, 2.4064e-09, 2.3015e-09, 1.8519e-10, 6.3482e-08,\n",
       "        8.9236e-14, 1.1066e-07, 7.8817e-02, 1.3576e-05, 1.0586e-08, 4.3965e-09,\n",
       "        8.3002e-04, 2.7862e-08, 3.1586e-03, 6.8665e-13, 7.5278e-11, 2.8178e-09,\n",
       "        9.1849e-08, 6.5022e-06])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indexing probabalites based on Y\n",
    "# This probabalities in future will be the probabalities by neural network\n",
    "prob[torch.arange(32), Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.8291)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = - prob[torch.arange(32), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of full network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "W1 = torch.randn((6, 100), generator=g)\n",
    "b1 = torch.randn(100, generator=g)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "paramerters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of parameters in total\n",
    "sum(p.nelement() for p in paramerters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.7697)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X]\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "counts = logits.exp()\n",
    "prob = counts / counts.sum(1, keepdim=True)\n",
    "loss = - prob[torch.arange(32), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross entropy why?\n",
    "\n",
    "```\n",
    "counts = logits.exp()\n",
    "prob = counts / counts.sum(1, keepdim=True)\n",
    "loss = - prob[torch.arange(32), Y].log().mean()\n",
    "```\n",
    "\n",
    "PyTorch creates a seperate tensor for each of these step\n",
    "\n",
    "1. Uses a fused kernel which combines all the above operations\n",
    "2. In backward pass, expression takes much simpler form mathametically\n",
    "3. Under the hood, this is numerically well behaved\n",
    "4. Forward pass and backward pass are much more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.7697)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = F.cross_entropy(logits, Y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.0466e-04, 3.3281e-04, 6.6846e-03, 9.9208e-01])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numerical stability difference\n",
    "logits = torch.tensor([-2, -3, 0, 5])\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum()\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts: tensor([3.7835e-44, 4.9787e-02, 1.0000e+00,        inf])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., nan])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numerical stability difference\n",
    "# With more extreme values, which will occur in backpropgation\n",
    "logits = torch.tensor([-100, -3, 0, 100])\n",
    "counts = logits.exp()\n",
    "print(f\"Counts: {counts}\")\n",
    "probs = counts / counts.sum()\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's happening above is the floating point ran out of dynamic range for exp(100) returning inf\n",
    "and for negative 100 the probs is near zero.\n",
    "\n",
    "So we cannot pass very larger number to our logits --> loss expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts: tensor([0.0000e+00, 1.4013e-45, 3.7835e-44, 1.0000e+00])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 1.4013e-45, 3.7835e-44, 1.0000e+00])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How PyTorch handles this is\n",
    "# By finding maximum of the logits and offsets it from the logits to avoid it\n",
    "logits = torch.tensor([-100, -3, 0, 100]) - 100\n",
    "counts = logits.exp()\n",
    "print(f\"Counts: {counts}\")\n",
    "probs = counts / counts.sum()\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing training loop, overfitting one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2552148103713989\n"
     ]
    }
   ],
   "source": [
    "# Set requires grad\n",
    "for p in paramerters:\n",
    "    p.requires_grad = True\n",
    "for _ in range(1000):\n",
    "    # Forward pass\n",
    "    emb = C[X] # [32, 3, 2]\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (100, 27)\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    # Backward pass\n",
    "    for p in paramerters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update parametrs\n",
    "    for p in paramerters:\n",
    "        p.data += -0.1 * p.grad\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've achieved a very good loss. Why?\n",
    "Because we're fitting the model for only 5 words i.e 32 inputs and with 3481 parameters.\n",
    "Lots of paramters for very less data.\n",
    "What we're doing it overfitting the model for one batch of data.\n",
    "\n",
    "> Note: Based on this overfitting can be defined as tuning many parameters for few samples or a batch of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why loss of 0 is not achieved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([13.4802, 18.1070, 20.7401, 20.8217, 16.9567, 13.4802, 16.2105, 14.3530,\n",
       "        16.0952, 18.6156, 16.1835, 21.1566, 13.4802, 17.3799, 17.3764, 20.3181,\n",
       "        13.4802, 16.8035, 15.3868, 17.3206, 18.7822, 16.2195, 11.0971, 10.8824,\n",
       "        15.6570, 13.4802, 16.3803, 17.1817, 12.8921, 16.3671, 19.3422, 16.3281],\n",
       "       grad_fn=<MaxBackward0>),\n",
       "indices=tensor([19, 13, 13,  1,  0, 19, 12,  9, 22,  9,  1,  0, 19, 22,  1,  0, 19, 19,\n",
       "         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,\n",
       "         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the indices of logits and output are same for most of the cases, these are inputs overfitted to outputs. But the missing ones are \n",
    "\n",
    "* ... --> e (emma)\n",
    "* ... --> o (olivia)\n",
    "* ... --> a (ava)\n",
    "* ... --> s (sophia)\n",
    "\n",
    "for different outptus for the same input.\n",
    "\n",
    "To overcome this, let's train on the full dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on full dataset, minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "206px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "21a124e163c92121797d725bed844fa6fdaf2c4e47bf1f149ef174ae791c682a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
