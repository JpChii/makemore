{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context:\n",
    "\n",
    "In previous notebook (4-activations...ipynb), we utilized PyTorch's autograd(loss.backwards()) for backpropogation. It's bad to use autograd from frameworks without learning it's internals, becuase we won't know why it's performing well or not. We've implemented our own backpropogation for scalars in micrograd but implementing backpropogation instead of frameworks autograd will help to improve debugging neural nets.\n",
    "As we'll learn the internals of backpropgation it will help more on our undersanding.\n",
    "\n",
    "# Makemore 5: Becoming a backprop ninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open(\"names.txt\").read().splitlines()\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build dataset\n",
    "block_size = 3 # contet length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boilerplate done,to the action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to compare manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad)\n",
    "  maxdiff = (dt - t.grad).abs().max().item()\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embed = 10 # dimensionality of character embedding vectors\n",
    "n_hidden = 64 # number of neurons in hidden layer of MLP\n",
    "torch_seed = 2147483647\n",
    "\n",
    "g = torch.Generator().manual_seed(torch_seed) # for reproducability\n",
    "C = torch.randn(vocab_size, n_embed)\n",
    "\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embed * block_size, n_hidden), generator=g) * (5/3)/((n_embed * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden, generator=g) * 0.1 # just for understanding, useless because of batch normalization\n",
    "#Layer 2 \n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size, generator=g) * 0.1\n",
    "# Batch norm paramters\n",
    "bngain = torch.randn((1, n_hidden)) * 0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden)) * 0.1 \n",
    "\n",
    "# Instead of zeros, retaining a samll number, \n",
    "# because sometimes initializing with all zeros could mask an incorrect implementation of backward pass\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size # shorter variable for conveniance\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3826, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
    "emb = C[Xb] # embed chars into vectos\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenat the vectors\n",
    "\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "\n",
    "# BatchNorm layer\n",
    "bnmeani = 1 / n*hprebn.sum(0, keepdim=True) # equivalvelnt of torch.mean(0, keepdim=True)\n",
    "\n",
    "# hprebn - hprebn_mean\n",
    "bndiff = hprebn - bnmeani\n",
    "\n",
    "# Variance - average squared deviations from mean\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction dividing by n-1 not n\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5 # 1 /square roor -> -0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact) # hidden layer\n",
    "\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2 # output layer\n",
    "\n",
    "# Cross entropy loss (same as F.cross_entropy(logits))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # Subrac max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv,logits,\n",
    "          norm_logits, logit_maxes, h, hpreact, bnraw,\n",
    "          bnvar_inv, bnvar, bndiff2, hprebn, bnmeani, embcat, emb]:\n",
    "    t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercis 1: Backpropogating atomic compute graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dlogprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dlogprobs\n",
    "# dlogprobs is logprobs derivate with respect to loss\n",
    "# how loss is influenced by dlogprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32]))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs.shape, Yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.6687, -4.0135, -4.1496, -3.2626, -4.6948, -3.9192, -2.8769, -2.8014,\n",
       "        -3.4888, -2.8769, -4.4138, -3.2692, -3.1819, -3.4690, -3.3758, -2.9790,\n",
       "        -3.2626, -3.4090, -3.0184, -3.7526, -2.7172, -2.5837, -3.4161, -3.3098,\n",
       "        -3.2492, -4.2265, -2.8275, -3.3941, -3.1399, -2.9950, -3.0721, -3.4284],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plucking out correct index for character out of (27) for each input in the batch based on Yb(correct index)\n",
    "# doing a mean of these values and negative\n",
    "logprobs[range(n), Yb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = -(a + b + c)/3\n",
    "# We've 32 characters so which is batch\n",
    "# loss = -(a + b + ....)/32\n",
    "# loss = -a/32 + -b/32 +......\n",
    "# dloss/da = -1/32\n",
    "# -1/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of logprobs where indexes are plucked out is -1/n. What about the other indexes which are not plucked out. Since they don't participate in loss. The derivative of those indices will be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "# Setting those indices to 1/n\n",
    "dlogprobs[range(n), Yb] = -1.0/n\n",
    "cmp('logprobs', dlogprobs, logprobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probs\n",
    "# how probs is affecting logprobs\n",
    "# logprobs is log of probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_10(x)\n",
    "# 1 / x * ln(10)\n",
    "# ln(10) = log_e(10)\n",
    "# e = 2.71288\n",
    "# log(x)\n",
    "# 1 / x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logprobs = probs.log()\n",
    "# dlogprobs/respect to probs == 1/probs ln(probs)\n",
    "# ln(probs) = log_e(probs) where e = 2.71288\n",
    "# torch.log() - uses natural log\n",
    "# So dlogprobs/probs = 1 / probs * local_derivative(by chain rule\n",
    "# dlogprobs/probs = 1 / probs * dlogporbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above derivative, i initially assume torch.log() is base 10 and concluded the derivative of torch.log(x) as 1 / x ln (10).\n",
    "After reading [torch.log](https://pytorch.org/docs/stable/generated/torch.log.html#torch.log) the implementation itself is natural log. Derivate of log(x) will simply be 1/x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "dprobs = 1 / probs * dlogprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dprobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dprobs          | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "cmp('dprobs', dprobs, probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### counts_sum_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how probs is affected by counts_sum_inv\n",
    "# probs = counts * counts_sum_inv\n",
    "# dprobs / counts_sum_inv = counts * local_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcounts_sum_inv = counts * dprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32, 1]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.shape, counts_sum_inv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the shapes, in forward pass implict tensor broadcasting is done by PyTorch to perform matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1]), torch.Size([32, 27]))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_sum_inv.shape, dcounts_sum_inv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_sum_inv.grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Doing a sum at dim 1 to match shape to accomodate tensor broadcasting\n",
    "dcounts_sum_inv = dcounts_sum_inv.sum(1, keepdims=True)\n",
    "dcounts_sum_inv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dcounts_sum_inv | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "cmp('dcounts_sum_inv', dcounts_sum_inv, counts_sum_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### counts_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts_sum with respect to counts_sum_inv\n",
    "# dcounts_sum_inv / dcounts_sum = ??\n",
    "# counts_sum_inv = counts_sum**-1\n",
    "# derivative of x**-1 -> -1/x**2\n",
    "# dcounts_sum_inv / dcounts_sum = -1 / counts_sum ** 2 * dcounts_sum_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1]), torch.Size([32, 1]))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_sum.shape, counts_sum_inv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcounts_sum = (-1.0/counts_sum**2) * dcounts_sum_inv\n",
    "dcounts_sum_inv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shapes hold good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "cmp('counts_sum', dcounts_sum, counts_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dcounts\n",
    "# dcounts has two gradients as it influences probs and counts_sum\n",
    "# we'll need dprobs/dcounts and dcounts_sum/dcounts\n",
    "# dprobs/dcounts It's multiplication\n",
    "# dprobs/dcounts = counts_sum_inv * local_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcounts = counts_sum_inv * dprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32, 27]))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcounts.shape, counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dcounts_sum/dcounts\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "# Derivative of addition is 1, so gradients just passes through\n",
    "# to keep shapes, we'll create ones of counts shape and multiply local gradient with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32, 1]))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.shape, counts_sum.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've to derive array of gradients from counts_sum.\n",
    "Addition just routes local gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# local gradient\n",
    "dcounts_sum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ones of counts shape\n",
    "torch.ones_like(counts).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcasting [32, 27] with [32, 1] to create array of gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# += to add previous gradinet dprobs/dcount\n",
    "dcounts1 = torch.ones_like(counts) * dcounts_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32, 27]))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcounts.shape, dcounts1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcounts = dcounts + dcounts1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dcounts         | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "cmp('dcounts', dcounts, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### norm_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts = norm_logits.exp()\n",
    "# derivative of exp() is exp() itself\n",
    "# dcounts / dnorm_logits = norm_logits.exp() * dcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcounts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnorm_logits = norm_logits.exp() * dcounts\n",
    "dnorm_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dnorm_logits    | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "cmp('dnorm_logits', dnorm_logits, norm_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logit_maxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm_logits = logits - logit_maxes\n",
    "# dnorm_logits / dlogit_maxes = logits/dlogit_maxes - dlogit_maxex/dlogit_maxes\n",
    "# = - 1 * dnorm_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1]), torch.Size([32, 27]), torch.Size([32, 27]))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_maxes.shape,logits.shape, dnorm_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogit_maxes = -1 * dnorm_logits\n",
    "dlogit_maxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogit_maxes = dlogit_maxes.sum(1, keepdim=True)\n",
    "dlogit_maxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dlogit_maxes    | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "cmp('dlogit_maxes', dlogit_maxes, logit_maxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dlogits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits has two dreivatives\n",
    "# dnorm_logits/dlogits and dlogit_maxes / dlogits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dnorm_logits/dlogits = 1 * dnorm_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits = 1 * dnorm_logits\n",
    "dlogits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dlogit_maxes / dlogits\n",
    "# torch.max() takes max of the dimension\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# 0th dimension has zero gradients and 1th dimension has gradient of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits_logit_maxes = torch.zeros_like(norm_logits)\n",
    "dlogits_logit_maxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits_logit_maxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits_logit_maxes[range(n), logits.max(1).indices] = 1\n",
    "dlogits_logit_maxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits_logit_maxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fae0a841720>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(dlogits_logit_maxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32, 1]))"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits_logit_maxes.shape, dlogit_maxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlogits_logit_maxes = dlogits_logit_maxes * dlogit_maxes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlogits = dnorm_logits.clone() + dlogits_logit_maxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dlogits         | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "cmp('dlogits', dlogits, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "all(): argument 'input' (position 1) must be Tensor, not bool",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [72]\u001b[0m, in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m cmp(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnorm_logits\u001b[39m\u001b[38;5;124m'\u001b[39m, dnorm_logits, norm_logits)\n\u001b[1;32m     19\u001b[0m cmp(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogit_maxes\u001b[39m\u001b[38;5;124m'\u001b[39m, dlogit_maxes, logit_maxes)\n\u001b[0;32m---> 20\u001b[0m \u001b[43mcmp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlogits\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mcmp\u001b[0;34m(s, dt, t)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcmp\u001b[39m(s, dt, t):\n\u001b[0;32m----> 3\u001b[0m   ex \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m      4\u001b[0m   app \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mallclose(dt, t\u001b[38;5;241m.\u001b[39mgrad)\n\u001b[1;32m      5\u001b[0m   maxdiff \u001b[38;5;241m=\u001b[39m (dt \u001b[38;5;241m-\u001b[39m t\u001b[38;5;241m.\u001b[39mgrad)\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mTypeError\u001b[0m: all(): argument 'input' (position 1) must be Tensor, not bool"
     ]
    }
   ],
   "source": [
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(n), Yb] = -1.0/n\n",
    "dprobs = 1/probs * dlogprobs\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdims=True)\n",
    "dcounts_sum = (-1.0/counts_sum**2) * dcounts_sum_inv\n",
    "dcounts = counts_sum_inv * dprobs\n",
    "dcounts += torch.ones_like(counts) * dcounts_sum\n",
    "dnorm_logits = counts * dcounts\n",
    "dlogits =  dnorm_logits.clone() # equivalent to 1 * dnorm_logits\n",
    "dlogit_maxes = (-dnorm_logits).sum(1, keepdims=True)\n",
    "# Another way of implementing \n",
    "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
    "\n",
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "cmp('counts', dcounts, counts)\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('logits', dlogits, logits)\n",
    "# cmp('h', dh, h)\n",
    "# cmp('W2', dW2, W2)\n",
    "# cmp('b2', db2, b2)\n",
    "# cmp('hpreact', dhpreact, hpreact)\n",
    "# cmp('bngain', dbngain, bngain)\n",
    "# cmp('bnbias', dbnbias, bnbias)\n",
    "# cmp('bnraw', dbnraw, bnraw)\n",
    "# cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "# cmp('bnvar', dbnvar, bnvar)\n",
    "# cmp('bndiff2', dbndiff2, bndiff2)\n",
    "# cmp('bndiff', dbndiff, bndiff)\n",
    "# cmp('bnmeani', dbnmeani, bnmeani)\n",
    "# cmp('hprebn', dhprebn, hprebn)\n",
    "# cmp('embcat', dembcat, embcat)\n",
    "# cmp('W1', dW1, W1)\n",
    "# cmp('b1', db1, b1)\n",
    "# cmp('emb', demb, emb)\n",
    "# cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(inf)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(torch.tensor(90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32, 27]))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits.shape, logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to compare manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad)\n",
    "  maxdiff = (dt - t.grad).abs().max().item()\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "all(): argument 'input' (position 1) must be Tensor, not bool",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [75]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdlogits\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mTypeError\u001b[0m: all(): argument 'input' (position 1) must be Tensor, not bool"
     ]
    }
   ],
   "source": [
    "torch.all(dlogits == logits.grad).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0012,  0.0005,  0.0013,  0.0013,  0.0005,  0.0009,  0.0015, -0.0305,\n",
       "          0.0008,  0.0004,  0.0008,  0.0015,  0.0004,  0.0018,  0.0027,  0.0022,\n",
       "          0.0017,  0.0006,  0.0008,  0.0007,  0.0008,  0.0011,  0.0010,  0.0012,\n",
       "          0.0026,  0.0018,  0.0003],\n",
       "        [ 0.0008,  0.0007,  0.0015,  0.0009,  0.0026,  0.0021,  0.0011,  0.0014,\n",
       "          0.0005,  0.0021,  0.0015,  0.0004, -0.0307,  0.0006,  0.0009,  0.0021,\n",
       "          0.0007,  0.0009,  0.0022,  0.0016,  0.0009,  0.0013,  0.0007,  0.0012,\n",
       "          0.0008,  0.0005,  0.0007],\n",
       "        [ 0.0007,  0.0008,  0.0019,  0.0006,  0.0015,  0.0009,  0.0005,  0.0005,\n",
       "          0.0008,  0.0009,  0.0009,  0.0006,  0.0026,  0.0008, -0.0308,  0.0007,\n",
       "          0.0022,  0.0012,  0.0029,  0.0012,  0.0007,  0.0008,  0.0007,  0.0017,\n",
       "          0.0015,  0.0009,  0.0021],\n",
       "        [-0.0301,  0.0014,  0.0002,  0.0010,  0.0028,  0.0006,  0.0012,  0.0010,\n",
       "          0.0013,  0.0012,  0.0015,  0.0014,  0.0012,  0.0013,  0.0005,  0.0010,\n",
       "          0.0015,  0.0008,  0.0015,  0.0016,  0.0008,  0.0010,  0.0017,  0.0010,\n",
       "          0.0008,  0.0008,  0.0011],\n",
       "        [ 0.0012,  0.0006,  0.0006,  0.0027,  0.0011,  0.0011,  0.0014,  0.0007,\n",
       "          0.0007,  0.0006,  0.0010,  0.0024, -0.0310,  0.0006,  0.0018,  0.0015,\n",
       "          0.0014,  0.0008,  0.0006,  0.0015,  0.0006,  0.0014,  0.0016,  0.0020,\n",
       "          0.0009,  0.0014,  0.0008],\n",
       "        [ 0.0026,  0.0018,  0.0021,  0.0010,  0.0008,  0.0013,  0.0007,  0.0023,\n",
       "          0.0015,  0.0006,  0.0004,  0.0007,  0.0012,  0.0014,  0.0010,  0.0007,\n",
       "          0.0006,  0.0016,  0.0009,  0.0005, -0.0306,  0.0015,  0.0003,  0.0007,\n",
       "          0.0009,  0.0013,  0.0022],\n",
       "        [ 0.0026, -0.0295,  0.0021,  0.0010,  0.0008,  0.0013,  0.0007,  0.0023,\n",
       "          0.0015,  0.0006,  0.0004,  0.0007,  0.0012,  0.0014,  0.0010,  0.0007,\n",
       "          0.0006,  0.0016,  0.0009,  0.0005,  0.0006,  0.0015,  0.0003,  0.0007,\n",
       "          0.0009,  0.0013,  0.0022],\n",
       "        [ 0.0008, -0.0294,  0.0009,  0.0007,  0.0016,  0.0004,  0.0010,  0.0006,\n",
       "          0.0012,  0.0007,  0.0012,  0.0009,  0.0015,  0.0017,  0.0004,  0.0008,\n",
       "          0.0029,  0.0010,  0.0009,  0.0018,  0.0007,  0.0005,  0.0006,  0.0023,\n",
       "          0.0022,  0.0009,  0.0011],\n",
       "        [-0.0303,  0.0021,  0.0007,  0.0012,  0.0012,  0.0008,  0.0011,  0.0014,\n",
       "          0.0007,  0.0010,  0.0010,  0.0011,  0.0020,  0.0013,  0.0006,  0.0006,\n",
       "          0.0015,  0.0010,  0.0009,  0.0021,  0.0006,  0.0011,  0.0015,  0.0009,\n",
       "          0.0016,  0.0013,  0.0010],\n",
       "        [ 0.0026, -0.0295,  0.0021,  0.0010,  0.0008,  0.0013,  0.0007,  0.0023,\n",
       "          0.0015,  0.0006,  0.0004,  0.0007,  0.0012,  0.0014,  0.0010,  0.0007,\n",
       "          0.0006,  0.0016,  0.0009,  0.0005,  0.0006,  0.0015,  0.0003,  0.0007,\n",
       "          0.0009,  0.0013,  0.0022],\n",
       "        [ 0.0011, -0.0309,  0.0006,  0.0014,  0.0006,  0.0010,  0.0008,  0.0004,\n",
       "          0.0005,  0.0007,  0.0010,  0.0023,  0.0015,  0.0013,  0.0020,  0.0018,\n",
       "          0.0007,  0.0004,  0.0013,  0.0006,  0.0019,  0.0009,  0.0038,  0.0016,\n",
       "          0.0013,  0.0009,  0.0007],\n",
       "        [-0.0301,  0.0012,  0.0006,  0.0036,  0.0006,  0.0006,  0.0011,  0.0008,\n",
       "          0.0008,  0.0005,  0.0007,  0.0028,  0.0003,  0.0011,  0.0019,  0.0010,\n",
       "          0.0009,  0.0018,  0.0005,  0.0013,  0.0008,  0.0008,  0.0014,  0.0024,\n",
       "          0.0008,  0.0009,  0.0010],\n",
       "        [ 0.0023,  0.0010,  0.0014,  0.0007,  0.0009,  0.0018,  0.0006,  0.0016,\n",
       "          0.0009,  0.0013,  0.0009,  0.0007,  0.0021,  0.0021,  0.0013,  0.0010,\n",
       "          0.0009,  0.0003, -0.0300,  0.0006,  0.0006,  0.0013,  0.0005,  0.0008,\n",
       "          0.0016,  0.0016,  0.0011],\n",
       "        [ 0.0010, -0.0303,  0.0004,  0.0019,  0.0023,  0.0005,  0.0011,  0.0006,\n",
       "          0.0005,  0.0022,  0.0013,  0.0009,  0.0015,  0.0004,  0.0011,  0.0006,\n",
       "          0.0005,  0.0021,  0.0009,  0.0026,  0.0011,  0.0016,  0.0004,  0.0008,\n",
       "          0.0003,  0.0008,  0.0028],\n",
       "        [ 0.0008,  0.0032,  0.0009,  0.0011,  0.0006,  0.0007,  0.0025,  0.0012,\n",
       "          0.0028,  0.0007,  0.0010,  0.0017,  0.0012,  0.0012,  0.0009,  0.0008,\n",
       "          0.0009,  0.0015,  0.0004, -0.0302,  0.0012,  0.0008,  0.0005,  0.0014,\n",
       "          0.0005,  0.0010,  0.0009],\n",
       "        [ 0.0005,  0.0047,  0.0005,  0.0020,  0.0006,  0.0004,  0.0010,  0.0010,\n",
       "          0.0016,  0.0006,  0.0014,  0.0009, -0.0297,  0.0007,  0.0004,  0.0003,\n",
       "          0.0024,  0.0009,  0.0007,  0.0010,  0.0008,  0.0017,  0.0014,  0.0004,\n",
       "          0.0004,  0.0011,  0.0021],\n",
       "        [-0.0301,  0.0014,  0.0002,  0.0010,  0.0028,  0.0006,  0.0012,  0.0010,\n",
       "          0.0013,  0.0012,  0.0015,  0.0014,  0.0012,  0.0013,  0.0005,  0.0010,\n",
       "          0.0015,  0.0008,  0.0015,  0.0016,  0.0008,  0.0010,  0.0017,  0.0010,\n",
       "          0.0008,  0.0008,  0.0011],\n",
       "        [-0.0302,  0.0002,  0.0004,  0.0004,  0.0014,  0.0007,  0.0011,  0.0004,\n",
       "          0.0006,  0.0010,  0.0020,  0.0006,  0.0018,  0.0024,  0.0009,  0.0016,\n",
       "          0.0016,  0.0004,  0.0016,  0.0018,  0.0014,  0.0007,  0.0013,  0.0012,\n",
       "          0.0032,  0.0009,  0.0005],\n",
       "        [ 0.0006, -0.0297,  0.0004,  0.0013,  0.0016,  0.0004,  0.0005,  0.0013,\n",
       "          0.0024,  0.0010,  0.0021,  0.0011,  0.0010,  0.0006,  0.0004,  0.0006,\n",
       "          0.0029,  0.0007,  0.0008,  0.0009,  0.0007,  0.0015,  0.0007,  0.0003,\n",
       "          0.0004,  0.0009,  0.0047],\n",
       "        [ 0.0014,  0.0007,  0.0007,  0.0012,  0.0035,  0.0005,  0.0009,  0.0011,\n",
       "          0.0005,  0.0018,  0.0013,  0.0008,  0.0012,  0.0009,  0.0005,  0.0018,\n",
       "          0.0013,  0.0015,  0.0022,  0.0016,  0.0007, -0.0305,  0.0005,  0.0015,\n",
       "          0.0008,  0.0006,  0.0012],\n",
       "        [ 0.0013,  0.0007,  0.0008,  0.0003, -0.0292,  0.0025,  0.0010,  0.0007,\n",
       "          0.0017,  0.0002,  0.0006,  0.0016,  0.0003,  0.0016,  0.0011,  0.0008,\n",
       "          0.0011,  0.0006,  0.0034,  0.0004,  0.0010,  0.0010,  0.0020,  0.0018,\n",
       "          0.0008,  0.0011,  0.0006],\n",
       "        [ 0.0011,  0.0005,  0.0007,  0.0006,  0.0014, -0.0289,  0.0015,  0.0008,\n",
       "          0.0007,  0.0003,  0.0009,  0.0010,  0.0003,  0.0011,  0.0023,  0.0009,\n",
       "          0.0004,  0.0006,  0.0043,  0.0005,  0.0011,  0.0010,  0.0031,  0.0012,\n",
       "          0.0008,  0.0012,  0.0006],\n",
       "        [ 0.0011,  0.0023,  0.0008,  0.0007,  0.0008,  0.0007,  0.0007,  0.0013,\n",
       "          0.0011,  0.0012,  0.0012,  0.0006,  0.0047,  0.0015,  0.0010,  0.0006,\n",
       "          0.0014,  0.0004,  0.0011,  0.0012, -0.0302,  0.0011,  0.0005,  0.0005,\n",
       "          0.0012,  0.0013,  0.0013],\n",
       "        [ 0.0004, -0.0301,  0.0008,  0.0015,  0.0015,  0.0023,  0.0011,  0.0015,\n",
       "          0.0011,  0.0038,  0.0012,  0.0008,  0.0003,  0.0004,  0.0017,  0.0008,\n",
       "          0.0013,  0.0007,  0.0013,  0.0011,  0.0004,  0.0024,  0.0009,  0.0006,\n",
       "          0.0006,  0.0010,  0.0005],\n",
       "        [ 0.0013,  0.0006,  0.0008,  0.0014,  0.0004,  0.0007,  0.0014,  0.0010,\n",
       "          0.0010,  0.0005,  0.0012,  0.0015,  0.0004,  0.0010,  0.0025,  0.0013,\n",
       "          0.0021,  0.0004,  0.0011,  0.0007, -0.0300,  0.0016,  0.0020,  0.0008,\n",
       "          0.0019,  0.0018,  0.0007],\n",
       "        [ 0.0017, -0.0308,  0.0008,  0.0008,  0.0005,  0.0004,  0.0009,  0.0004,\n",
       "          0.0007,  0.0004,  0.0011,  0.0012,  0.0010,  0.0018,  0.0022,  0.0012,\n",
       "          0.0018,  0.0004,  0.0010,  0.0008,  0.0014,  0.0005,  0.0016,  0.0017,\n",
       "          0.0034,  0.0022,  0.0006],\n",
       "        [ 0.0020,  0.0008,  0.0007,  0.0019,  0.0007,  0.0008,  0.0024,  0.0007,\n",
       "          0.0009,  0.0003,  0.0006,  0.0026,  0.0002,  0.0010, -0.0294,  0.0015,\n",
       "          0.0008,  0.0014,  0.0004,  0.0014,  0.0013,  0.0010,  0.0011,  0.0014,\n",
       "          0.0015,  0.0013,  0.0006],\n",
       "        [ 0.0009,  0.0008,  0.0004,  0.0011,  0.0009,  0.0005,  0.0038,  0.0010,\n",
       "          0.0009,  0.0019,  0.0014,  0.0003,  0.0025, -0.0302,  0.0006,  0.0011,\n",
       "          0.0021,  0.0004,  0.0004,  0.0028,  0.0013,  0.0006,  0.0008,  0.0011,\n",
       "          0.0012,  0.0010,  0.0005],\n",
       "        [ 0.0015,  0.0025,  0.0019,  0.0009,  0.0008, -0.0299,  0.0012,  0.0020,\n",
       "          0.0014,  0.0008,  0.0007,  0.0022,  0.0010,  0.0015,  0.0017,  0.0009,\n",
       "          0.0003,  0.0010,  0.0008,  0.0006,  0.0010,  0.0011,  0.0003,  0.0008,\n",
       "          0.0006,  0.0010,  0.0014],\n",
       "        [ 0.0020, -0.0297,  0.0026,  0.0007,  0.0007,  0.0019,  0.0007,  0.0020,\n",
       "          0.0014,  0.0007,  0.0005,  0.0009,  0.0013,  0.0014,  0.0015,  0.0009,\n",
       "          0.0005,  0.0015,  0.0009,  0.0005,  0.0005,  0.0017,  0.0003,  0.0010,\n",
       "          0.0007,  0.0015,  0.0012],\n",
       "        [ 0.0011,  0.0007,  0.0015,  0.0012,  0.0015,  0.0007,  0.0009,  0.0012,\n",
       "          0.0014,  0.0016,  0.0011,  0.0010,  0.0024,  0.0007,  0.0013, -0.0298,\n",
       "          0.0006,  0.0011,  0.0017,  0.0008,  0.0014,  0.0013,  0.0004,  0.0011,\n",
       "          0.0005,  0.0008,  0.0020],\n",
       "        [ 0.0008,  0.0014, -0.0302,  0.0009,  0.0005,  0.0014,  0.0012,  0.0010,\n",
       "          0.0025,  0.0009,  0.0012,  0.0012,  0.0023,  0.0014,  0.0023,  0.0007,\n",
       "          0.0005,  0.0005,  0.0014,  0.0007,  0.0016,  0.0011,  0.0011,  0.0011,\n",
       "          0.0008,  0.0011,  0.0006]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.grad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "21a124e163c92121797d725bed844fa6fdaf2c4e47bf1f149ef174ae791c682a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
