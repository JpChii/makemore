{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise\n",
    "\n",
    "We'll be solving below excercises learned from 1-bigram notebook,\n",
    "\n",
    "1. train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?\n",
    "2. split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?\n",
    "3. use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?\n",
    "4. we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?\n",
    "5. look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 1: Trigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing dataset\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mma'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[0][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s: i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 196113\n"
     ]
    }
   ],
   "source": [
    "xs1, xs2, ys = [], [], []\n",
    "for word in words:\n",
    "    chs = ['.'] + list(word) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "        xs1.append(ix1)\n",
    "        xs2.append(ix2)\n",
    "        ys.append(ix3)\n",
    "xs1 = torch.tensor(xs1)\n",
    "xs2 = torch.tensor(xs2)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs1.nelement()\n",
    "print(f\"Number of examples: {num}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For trigram, we now have two inputs and one outpt. So in the forward pass, we'll initialize two weights(neurons) and perform a sum of xs1 @ W1 + xs2 @ W2. Assuming weighted sum of inputs is logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights\n",
    "g = torch.Generator().manual_seed(42)\n",
    "W1 = torch.randn((27, 27), generator=g, requires_grad=True)\n",
    "W2 = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([27, 27]), torch.Size([27, 27]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1.shape, W2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.03549337387085\n",
      "3.38352632522583\n",
      "3.066603422164917\n",
      "2.884695053100586\n",
      "2.772024393081665\n",
      "2.6933724880218506\n",
      "2.634974241256714\n",
      "2.5894908905029297\n",
      "2.552961587905884\n",
      "2.5229411125183105\n",
      "2.4978718757629395\n",
      "2.4766249656677246\n",
      "2.458418130874634\n",
      "2.4426538944244385\n",
      "2.4289000034332275\n",
      "2.416811466217041\n",
      "2.4061248302459717\n",
      "2.3966212272644043\n",
      "2.388127565383911\n",
      "2.3804965019226074\n",
      "2.373607635498047\n",
      "2.3673579692840576\n",
      "2.3616628646850586\n",
      "2.356449842453003\n",
      "2.3516578674316406\n",
      "2.347235918045044\n",
      "2.3431406021118164\n",
      "2.339334011077881\n",
      "2.3357856273651123\n",
      "2.332468032836914\n",
      "2.329357862472534\n",
      "2.3264355659484863\n",
      "2.323683500289917\n",
      "2.3210864067077637\n",
      "2.318631649017334\n",
      "2.3163068294525146\n",
      "2.3141021728515625\n",
      "2.3120081424713135\n",
      "2.310016632080078\n",
      "2.3081207275390625\n",
      "2.3063132762908936\n",
      "2.3045878410339355\n",
      "2.3029398918151855\n",
      "2.301363945007324\n",
      "2.2998554706573486\n",
      "2.298410654067993\n",
      "2.297025203704834\n",
      "2.2956955432891846\n",
      "2.2944185733795166\n",
      "2.29319167137146\n",
      "2.292011260986328\n",
      "2.2908754348754883\n",
      "2.289781332015991\n",
      "2.288726806640625\n",
      "2.287710189819336\n",
      "2.286728620529175\n",
      "2.285781145095825\n",
      "2.2848658561706543\n",
      "2.2839810848236084\n",
      "2.2831249237060547\n",
      "2.282297134399414\n",
      "2.2814953327178955\n",
      "2.2807188034057617\n",
      "2.279966115951538\n",
      "2.2792365550994873\n",
      "2.278528928756714\n",
      "2.2778422832489014\n",
      "2.2771754264831543\n",
      "2.2765281200408936\n",
      "2.2758991718292236\n",
      "2.2752881050109863\n",
      "2.274693727493286\n",
      "2.274115562438965\n",
      "2.2735536098480225\n",
      "2.2730062007904053\n",
      "2.2724735736846924\n",
      "2.2719550132751465\n",
      "2.2714498043060303\n",
      "2.2709572315216064\n",
      "2.270477294921875\n",
      "2.2700092792510986\n",
      "2.2695531845092773\n",
      "2.2691078186035156\n",
      "2.2686736583709717\n",
      "2.268249750137329\n",
      "2.267836093902588\n",
      "2.2674319744110107\n",
      "2.2670371532440186\n",
      "2.2666518688201904\n",
      "2.26627516746521\n",
      "2.265906810760498\n",
      "2.2655467987060547\n",
      "2.2651946544647217\n",
      "2.264850616455078\n",
      "2.2645137310028076\n",
      "2.2641842365264893\n",
      "2.263861656188965\n",
      "2.2635459899902344\n",
      "2.2632369995117188\n",
      "2.262934446334839\n"
     ]
    }
   ],
   "source": [
    "for k in range(100):\n",
    "\n",
    "    x1enc = F.one_hot(xs1, num_classes=27).float()\n",
    "    x2enc = F.one_hot(xs2, num_classes=27).float()\n",
    "    logits = x1enc @ W1 + x2enc @ W2\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(num), ys].log().mean()\n",
    "    print(loss.item())\n",
    "\n",
    "    W1.grad = None\n",
    "    W2.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    W1.data += -50 * W1.grad\n",
    "    W2.data += -50 * W2.grad\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss is reduced to 2.26 compared to 2.47 loss of trigram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la.\n",
      "melliud.\n",
      "vin.\n",
      "riuni.\n",
      "minomebrr.\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "\n",
    "    out = []\n",
    "    ix1 = 0\n",
    "    ix2 = 1\n",
    "    while True:\n",
    "        x1enc = F.one_hot(torch.tensor([ix1]), num_classes=27).float()\n",
    "        x2enc = F.one_hot(torch.tensor([ix2]), num_classes=27).float()\n",
    "        logits = x1enc @ W1 + x2enc @ W2\n",
    "        counts = logits.exp()\n",
    "        p = counts / counts.sum(1, keepdims=True)\n",
    "        next_ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[next_ix])\n",
    "        ix1 = ix2\n",
    "        ix2 = next_ix\n",
    "        if ix2 == 0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're getting something like names but some of them are as bad as bigram model. But some improvement can be seen."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 2: Splitting up dataset into train, dev and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing dataset\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('mma', 'ma')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[0][1:], words[0][2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s: i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 196113\n"
     ]
    }
   ],
   "source": [
    "xs1, xs2 , ys = [], [], []\n",
    "for word in words:\n",
    "    chs = ['.'] + list(word) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "        xs1.append(ix1)\n",
    "        xs2.append(ix2)\n",
    "        ys.append(ix3)\n",
    "xs1 = torch.tensor(xs1)\n",
    "xs2 = torch.tensor(xs2)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs1.nelement()\n",
    "print(f\"Number of examples: {num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights\n",
    "g = torch.Generator().manual_seed(42)\n",
    "W1 = torch.randn((27, 27), generator=g, requires_grad=True)\n",
    "W2 = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train test split from sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "X1_train, X1_test, X2_train, X2_test, y_train, y_test = train_test_split(xs1, xs2, ys, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(137279, 58834, 137279, 58834, 137279, 58834)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X1_train), len(X1_test), len(X2_train), len(X2_test),len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting test to dev and test sets\n",
    "X1_dev, X1_test, X2_dev, X2_test, y_dev, y_test = train_test_split(X1_test, X2_test, y_test, test_size=(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29417, 29417, 29417, 29417, 29417, 29417)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X1_dev), len(X1_test), len(X2_dev), len(X2_test),len(y_dev), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(137279, 137279)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X1_train), len(X2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num = X1_train.nelement()\n",
    "dev_num = X1_dev.nelement()\n",
    "test_num = X1_test.nelement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.038381576538086\n",
      "3.3869142532348633\n",
      "3.069572687149048\n",
      "2.8877241611480713\n",
      "2.774827480316162\n",
      "2.695857048034668\n",
      "2.6371712684631348\n",
      "2.591456174850464\n",
      "2.554763078689575\n",
      "2.524624824523926\n",
      "2.49947190284729\n",
      "2.4781596660614014\n",
      "2.459901809692383\n",
      "2.4440951347351074\n",
      "2.4303061962127686\n",
      "2.418186664581299\n",
      "2.407472610473633\n",
      "2.3979437351226807\n",
      "2.3894259929656982\n",
      "2.3817708492279053\n",
      "2.3748583793640137\n",
      "2.368584632873535\n",
      "2.362865447998047\n",
      "2.3576278686523438\n",
      "2.352811813354492\n",
      "2.348365545272827\n",
      "2.3442459106445312\n",
      "2.340416193008423\n",
      "2.3368449211120605\n",
      "2.333505630493164\n",
      "2.3303747177124023\n",
      "2.327432155609131\n",
      "2.3246614933013916\n",
      "2.322047233581543\n",
      "2.3195760250091553\n",
      "2.3172361850738525\n",
      "2.3150177001953125\n",
      "2.312910795211792\n",
      "2.3109078407287598\n",
      "2.3090012073516846\n",
      "2.3071839809417725\n",
      "2.305450439453125\n",
      "2.3037946224212646\n",
      "2.3022119998931885\n",
      "2.3006973266601562\n",
      "2.2992467880249023\n",
      "2.2978568077087402\n",
      "2.296523094177246\n",
      "2.2952427864074707\n",
      "2.2940125465393066\n",
      "2.292829751968384\n",
      "2.291691541671753\n",
      "2.290595769882202\n",
      "2.2895395755767822\n",
      "2.2885217666625977\n",
      "2.287539482116699\n",
      "2.286591053009033\n",
      "2.285675525665283\n",
      "2.284790515899658\n",
      "2.2839345932006836\n",
      "2.2831063270568848\n",
      "2.2823047637939453\n",
      "2.2815284729003906\n",
      "2.280776262283325\n",
      "2.2800471782684326\n",
      "2.2793397903442383\n",
      "2.278654098510742\n",
      "2.2779881954193115\n",
      "2.277341604232788\n",
      "2.2767136096954346\n",
      "2.2761030197143555\n",
      "2.27551007270813\n",
      "2.274933099746704\n",
      "2.274371862411499\n",
      "2.2738258838653564\n",
      "2.27329421043396\n",
      "2.2727766036987305\n",
      "2.2722725868225098\n",
      "2.2717819213867188\n",
      "2.2713029384613037\n",
      "2.270836114883423\n",
      "2.270381450653076\n",
      "2.2699379920959473\n",
      "2.2695047855377197\n",
      "2.269082546234131\n",
      "2.268669843673706\n",
      "2.2682673931121826\n",
      "2.267873764038086\n",
      "2.2674901485443115\n",
      "2.2671148777008057\n",
      "2.2667477130889893\n",
      "2.2663896083831787\n",
      "2.2660388946533203\n",
      "2.265695810317993\n",
      "2.2653608322143555\n",
      "2.2650327682495117\n",
      "2.264711856842041\n",
      "2.264397621154785\n",
      "2.264089822769165\n",
      "2.2637887001037598\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for k in range(100):\n",
    "\n",
    "    x1enc = F.one_hot(X1_train, num_classes=27).float()\n",
    "    x2enc = F.one_hot(X2_train, num_classes=27).float()\n",
    "    logits = x1enc @ W1 + x2enc @ W2\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(train_num), y_train].log().mean()\n",
    "    print(loss.item())\n",
    "\n",
    "    W1.grad = None\n",
    "    W2.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    W1.data += -50 * W1.grad\n",
    "    W2.data += -50 * W2.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daahheoisaatin.\n",
      "letkn.\n",
      "ilivjj.\n",
      "lr.\n",
      "ial.\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "\n",
    "    out = []\n",
    "    ix1 = 0\n",
    "    ix2 = 1\n",
    "    while True:\n",
    "        x1enc = F.one_hot(torch.tensor([ix1]), num_classes=27).float()\n",
    "        x2enc = F.one_hot(torch.tensor([ix2]), num_classes=27).float()\n",
    "        logits = x1enc @ W1 + x2enc @ W2\n",
    "        counts = logits.exp()\n",
    "        p = counts / counts.sum(1, keepdims=True)\n",
    "        next_ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[next_ix])\n",
    "        ix1 = ix2\n",
    "        ix2 = next_ix\n",
    "        if ix2 == 0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2646405696868896\n"
     ]
    }
   ],
   "source": [
    "x1enc = F.one_hot(X1_dev, num_classes=27).float()\n",
    "x2enc = F.one_hot(X2_dev, num_classes=27).float()\n",
    "logits = x1enc @ W1 + x2enc @ W2\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "loss = -probs[torch.arange(y_dev.nelement()), y_dev].log().mean()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2625491619110107\n"
     ]
    }
   ],
   "source": [
    "x1enc = F.one_hot(X1_test, num_classes=27).float()\n",
    "x2enc = F.one_hot(X2_test, num_classes=27).float()\n",
    "logits = x1enc @ W1 + x2enc @ W2\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "loss = -probs[torch.arange(y_test.nelement()), y_test].log().mean()\n",
    "print(loss.item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss on test and dev sets are same as that on trianing set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 3: Smoothing\n",
    "\n",
    "Let's smooth the training loss by evaluating it on dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoothing: 0.9912816286087036\n",
      "4.07726526260376\n",
      "Smoothing: 0.8227971792221069\n",
      "3.6025888919830322\n",
      "Smoothing: 0.717277467250824\n",
      "3.348339319229126\n",
      "Smoothing: 0.6491332054138184\n",
      "3.1914966106414795\n",
      "Smoothing: 0.5999585390090942\n",
      "3.092525005340576\n",
      "Smoothing: 0.5618353486061096\n",
      "3.0270206928253174\n",
      "Smoothing: 0.5303149819374084\n",
      "2.979785442352295\n",
      "Smoothing: 0.5036755800247192\n",
      "2.9434385299682617\n",
      "Smoothing: 0.48085352778434753\n",
      "2.914311170578003\n",
      "Smoothing: 0.46114856004714966\n",
      "2.8903310298919678\n",
      "Smoothing: 0.44398871064186096\n",
      "2.870206832885742\n",
      "Smoothing: 0.42895182967185974\n",
      "2.8530781269073486\n",
      "Smoothing: 0.41568833589553833\n",
      "2.8383398056030273\n",
      "Smoothing: 0.4039267897605896\n",
      "2.825549364089966\n",
      "Smoothing: 0.393439918756485\n",
      "2.814371347427368\n",
      "Smoothing: 0.3840446472167969\n",
      "2.8045456409454346\n",
      "Smoothing: 0.3755863606929779\n",
      "2.7958645820617676\n",
      "Smoothing: 0.36793753504753113\n",
      "2.7881579399108887\n",
      "Smoothing: 0.3609901964664459\n",
      "2.781287431716919\n",
      "Smoothing: 0.35465413331985474\n",
      "2.775134801864624\n",
      "Smoothing: 0.3488527834415436\n",
      "2.7696027755737305\n",
      "Smoothing: 0.3435216546058655\n",
      "2.76460862159729\n",
      "Smoothing: 0.33860599994659424\n",
      "2.760082483291626\n",
      "Smoothing: 0.3340590000152588\n",
      "2.7559654712677\n",
      "Smoothing: 0.3298407793045044\n",
      "2.7522077560424805\n",
      "Smoothing: 0.3259170949459076\n",
      "2.7487659454345703\n",
      "Smoothing: 0.3222583830356598\n",
      "2.745603561401367\n",
      "Smoothing: 0.31883883476257324\n",
      "2.742689847946167\n",
      "Smoothing: 0.31563612818717957\n",
      "2.739997625350952\n",
      "Smoothing: 0.3126307427883148\n",
      "2.7375035285949707\n",
      "Smoothing: 0.30980509519577026\n",
      "2.735186815261841\n",
      "Smoothing: 0.30714425444602966\n",
      "2.7330310344696045\n",
      "Smoothing: 0.3046344220638275\n",
      "2.7310192584991455\n",
      "Smoothing: 0.3022637963294983\n",
      "2.7291393280029297\n",
      "Smoothing: 0.30002138018608093\n",
      "2.7273776531219482\n",
      "Smoothing: 0.29789766669273376\n",
      "2.725724697113037\n",
      "Smoothing: 0.29588398337364197\n",
      "2.724170446395874\n",
      "Smoothing: 0.2939724922180176\n",
      "2.722707509994507\n",
      "Smoothing: 0.29215604066848755\n",
      "2.721327066421509\n",
      "Smoothing: 0.2904283106327057\n",
      "2.7200229167938232\n",
      "Smoothing: 0.288783460855484\n",
      "2.718789577484131\n",
      "Smoothing: 0.2872161269187927\n",
      "2.7176215648651123\n",
      "Smoothing: 0.28572145104408264\n",
      "2.716513156890869\n",
      "Smoothing: 0.28429505228996277\n",
      "2.715461015701294\n",
      "Smoothing: 0.2829327881336212\n",
      "2.7144601345062256\n",
      "Smoothing: 0.28163087368011475\n",
      "2.713508367538452\n",
      "Smoothing: 0.2803858816623688\n",
      "2.7126011848449707\n",
      "Smoothing: 0.27919453382492065\n",
      "2.711735725402832\n",
      "Smoothing: 0.278053879737854\n",
      "2.7109100818634033\n",
      "Smoothing: 0.27696114778518677\n",
      "2.710120916366577\n",
      "Smoothing: 0.2759137749671936\n",
      "2.7093663215637207\n",
      "Smoothing: 0.27490943670272827\n",
      "2.708643674850464\n",
      "Smoothing: 0.27394574880599976\n",
      "2.707951545715332\n",
      "Smoothing: 0.27302077412605286\n",
      "2.7072882652282715\n",
      "Smoothing: 0.2721325159072876\n",
      "2.7066524028778076\n",
      "Smoothing: 0.27127909660339355\n",
      "2.706041097640991\n",
      "Smoothing: 0.2704588770866394\n",
      "2.7054545879364014\n",
      "Smoothing: 0.2696702480316162\n",
      "2.704890489578247\n",
      "Smoothing: 0.2689116895198822\n",
      "2.704347848892212\n",
      "Smoothing: 0.26818177103996277\n",
      "2.7038252353668213\n",
      "Smoothing: 0.26747927069664\n",
      "2.703322410583496\n",
      "Smoothing: 0.2668027877807617\n",
      "2.7028379440307617\n",
      "Smoothing: 0.26615116000175476\n",
      "2.7023708820343018\n",
      "Smoothing: 0.26552337408065796\n",
      "2.7019202709198\n",
      "Smoothing: 0.26491835713386536\n",
      "2.7014846801757812\n",
      "Smoothing: 0.264335036277771\n",
      "2.7010645866394043\n",
      "Smoothing: 0.26377251744270325\n",
      "2.7006587982177734\n",
      "Smoothing: 0.26322993636131287\n",
      "2.7002665996551514\n",
      "Smoothing: 0.262706458568573\n",
      "2.6998870372772217\n",
      "Smoothing: 0.2622012794017792\n",
      "2.6995198726654053\n",
      "Smoothing: 0.2617136240005493\n",
      "2.6991639137268066\n",
      "Smoothing: 0.2612428665161133\n",
      "2.698820114135742\n",
      "Smoothing: 0.2607882022857666\n",
      "2.698486328125\n",
      "Smoothing: 0.2603490650653839\n",
      "2.6981632709503174\n",
      "Smoothing: 0.25992491841316223\n",
      "2.6978495121002197\n",
      "Smoothing: 0.2595149874687195\n",
      "2.6975457668304443\n",
      "Smoothing: 0.2591189444065094\n",
      "2.6972503662109375\n",
      "Smoothing: 0.25873610377311707\n",
      "2.6969635486602783\n",
      "Smoothing: 0.2583661377429962\n",
      "2.696685314178467\n",
      "Smoothing: 0.2580084204673767\n",
      "2.6964149475097656\n",
      "Smoothing: 0.25766250491142273\n",
      "2.6961517333984375\n",
      "Smoothing: 0.2573281526565552\n",
      "2.6958961486816406\n",
      "Smoothing: 0.25700464844703674\n",
      "2.6956472396850586\n",
      "Smoothing: 0.25669199228286743\n",
      "2.6954052448272705\n",
      "Smoothing: 0.25638943910598755\n",
      "2.6951701641082764\n",
      "Smoothing: 0.25609689950942993\n",
      "2.6949403285980225\n",
      "Smoothing: 0.255813866853714\n",
      "2.6947171688079834\n",
      "Smoothing: 0.2555401623249054\n",
      "2.6944992542266846\n",
      "Smoothing: 0.2552753686904907\n",
      "2.694287061691284\n",
      "Smoothing: 0.2550192177295685\n",
      "2.694080352783203\n",
      "Smoothing: 0.25477147102355957\n",
      "2.693878650665283\n",
      "Smoothing: 0.2545318603515625\n",
      "2.6936819553375244\n",
      "Smoothing: 0.2543001174926758\n",
      "2.6934897899627686\n",
      "Smoothing: 0.25407588481903076\n",
      "2.693302631378174\n",
      "Smoothing: 0.25385910272598267\n",
      "2.693119764328003\n",
      "Smoothing: 0.25364950299263\n",
      "2.6929407119750977\n",
      "Smoothing: 0.2534467577934265\n",
      "2.692765951156616\n",
      "Smoothing: 0.25325074791908264\n",
      "2.6925952434539795\n",
      "Smoothing: 0.2530612647533417\n",
      "2.6924288272857666\n",
      "Smoothing: 0.2528780996799469\n",
      "2.692265510559082\n"
     ]
    }
   ],
   "source": [
    "# Initialize weights\n",
    "g = torch.Generator().manual_seed(42)\n",
    "W1 = torch.randn((27, 27), generator=g, requires_grad=True)\n",
    "W2 = torch.randn((27, 27), generator=g, requires_grad=True)\n",
    "# Training\n",
    "for k in range(100):\n",
    "\n",
    "    x1enc = F.one_hot(X1_train, num_classes=27).float()\n",
    "    x2enc = F.one_hot(X2_train, num_classes=27).float()\n",
    "    logits = x1enc @ W1 + x2enc @ W2\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    smoothing = (W1**2).mean() * (W2**2).mean()\n",
    "    loss = -probs[torch.arange(dev_num), y_dev].log().mean() + 0.01 * smoothing\n",
    "    print(f\"Smoothing: {smoothing}\")\n",
    "    print(loss.item())\n",
    "\n",
    "    W1.grad = None\n",
    "    W2.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    W1.data += -50 * W1.grad\n",
    "    W2.data += -50 * W2.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Am not sure about smoothing used above with using dev to evaluate loss. Let's learn that in details next notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 4: One hot encoding to indexing to increase effeciency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1enc[0].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1enc[0].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1[x1enc[0].argmax()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x1enc[0] @ W1).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the argmax index of encoded input on weights gives the same output as matrix multiplications which is just selecting the row in weights based on index.\n",
    "\n",
    "Let's do this to improve efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of logits: torch.Size([137279, 27])\n",
      "4.010873317718506\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "3.4076037406921387\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "3.154132843017578\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.9991979598999023\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.9097750186920166\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.8524322509765625\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.8865444660186768\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.8798012733459473\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "3.034534215927124\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.73941707611084\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.788029193878174\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.965228319168091\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.7140512466430664\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.7134971618652344\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.8758704662323\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.663717269897461\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.659118890762329\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.7162866592407227\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.9095022678375244\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.6510910987854004\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.6698734760284424\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.821089029312134\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.6413185596466064\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.650430679321289\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.7319371700286865\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.918769359588623\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.6614127159118652\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.7056891918182373\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.9072959423065186\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.6445703506469727\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.693711757659912\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.8820528984069824\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.6339426040649414\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.660421133041382\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.852571725845337\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.6119956970214844\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.606433868408203\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.6869232654571533\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.6924917697906494\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.8612780570983887\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.6027207374572754\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.643404483795166\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.831354856491089\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.6024961471557617\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.5835883617401123\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.5854954719543457\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.644421339035034\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.813777446746826\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.5943357944488525\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.5733847618103027\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.5788750648498535\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.5765180587768555\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.6110684871673584\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.7387919425964355\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.6226110458374023\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.7289576530456543\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.6107571125030518\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.733628034591675\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.6202871799468994\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.726377010345459\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.6103286743164062\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.73478102684021\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.61667537689209\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.7172632217407227\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.6160902976989746\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.7544612884521484\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.6014745235443115\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.656013250350952\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.6770732402801514\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.8625903129577637\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.609100341796875\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.6485373973846436\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.8491952419281006\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.6028835773468018\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.6166701316833496\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.7661023139953613\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.594890594482422\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.6215922832489014\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.695502758026123\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.8803133964538574\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.6206612586975098\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.668833017349243\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.875511646270752\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.618292808532715\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.665613889694214\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.8517329692840576\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.6020798683166504\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.630842685699463\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.8213040828704834\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.58986496925354\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.574070930480957\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.6043381690979004\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.702341318130493\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.876115322113037\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.6060447692871094\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.6617863178253174\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.87016224861145\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.6153881549835205\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.655740976333618\n",
      "Shape of logits: torch.Size([137279, 27])\n",
      "2.8377792835235596\n"
     ]
    }
   ],
   "source": [
    "# Initialize weights\n",
    "g = torch.Generator().manual_seed(42)\n",
    "W1 = torch.randn((27, 27), generator=g, requires_grad=True)\n",
    "W2 = torch.randn((27, 27), generator=g, requires_grad=True)\n",
    "\n",
    "# Training\n",
    "for k in range(100):\n",
    "\n",
    "    # x1enc = F.one_hot(X1_train, num_classes=27).float()\n",
    "    # x2enc = F.one_hot(X2_train, num_classes=27).float()\n",
    "    logits = W1[X1_train] + W2[X1_train]\n",
    "    print(f\"Shape of logits: {logits.shape}\")\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(train_num), y_train].log().mean()\n",
    "    print(loss.item())\n",
    "\n",
    "    W1.grad = None\n",
    "    W2.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    W1.data += -50 * W1.grad\n",
    "    W2.data += -50 * W2.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.038381576538086\n",
      "3.3869142532348633\n",
      "3.069572687149048\n",
      "2.8877241611480713\n",
      "2.774827480316162\n",
      "2.695857048034668\n",
      "2.6371712684631348\n",
      "2.591456174850464\n",
      "2.554763078689575\n",
      "2.524624824523926\n",
      "2.49947190284729\n",
      "2.4781596660614014\n",
      "2.459901809692383\n",
      "2.4440951347351074\n",
      "2.4303061962127686\n",
      "2.418186664581299\n",
      "2.407472610473633\n",
      "2.3979437351226807\n",
      "2.3894259929656982\n",
      "2.3817708492279053\n",
      "2.3748583793640137\n",
      "2.368584632873535\n",
      "2.362865447998047\n",
      "2.3576278686523438\n",
      "2.352811813354492\n",
      "2.348365545272827\n",
      "2.3442459106445312\n",
      "2.340416193008423\n",
      "2.3368449211120605\n",
      "2.333505630493164\n",
      "2.3303747177124023\n",
      "2.327432155609131\n",
      "2.3246614933013916\n",
      "2.322047233581543\n",
      "2.3195760250091553\n",
      "2.3172361850738525\n",
      "2.3150177001953125\n",
      "2.312910795211792\n",
      "2.3109078407287598\n",
      "2.3090012073516846\n",
      "2.3071839809417725\n",
      "2.305450439453125\n",
      "2.3037946224212646\n",
      "2.3022119998931885\n",
      "2.3006973266601562\n",
      "2.2992467880249023\n",
      "2.2978568077087402\n",
      "2.296523094177246\n",
      "2.2952427864074707\n",
      "2.2940125465393066\n",
      "2.292829751968384\n",
      "2.291691541671753\n",
      "2.290595769882202\n",
      "2.2895395755767822\n",
      "2.2885217666625977\n",
      "2.287539482116699\n",
      "2.286591053009033\n",
      "2.285675525665283\n",
      "2.284790515899658\n",
      "2.2839345932006836\n",
      "2.2831063270568848\n",
      "2.2823047637939453\n",
      "2.2815284729003906\n",
      "2.280776262283325\n",
      "2.2800471782684326\n",
      "2.2793397903442383\n",
      "2.278654098510742\n",
      "2.2779881954193115\n",
      "2.277341604232788\n",
      "2.2767136096954346\n",
      "2.2761030197143555\n",
      "2.27551007270813\n",
      "2.274933099746704\n",
      "2.274371862411499\n",
      "2.2738258838653564\n",
      "2.27329421043396\n",
      "2.2727766036987305\n",
      "2.2722725868225098\n",
      "2.2717819213867188\n",
      "2.2713029384613037\n",
      "2.270836114883423\n",
      "2.270381450653076\n",
      "2.2699379920959473\n",
      "2.2695047855377197\n",
      "2.269082546234131\n",
      "2.268669843673706\n",
      "2.2682673931121826\n",
      "2.267873764038086\n",
      "2.2674901485443115\n",
      "2.2671148777008057\n",
      "2.2667477130889893\n",
      "2.2663896083831787\n",
      "2.2660388946533203\n",
      "2.265695810317993\n",
      "2.2653608322143555\n",
      "2.2650327682495117\n",
      "2.264711856842041\n",
      "2.264397621154785\n",
      "2.264089822769165\n",
      "2.2637887001037598\n"
     ]
    }
   ],
   "source": [
    "# Initialize weights\n",
    "g = torch.Generator().manual_seed(42)\n",
    "W1 = torch.randn((27, 27), generator=g, requires_grad=True)\n",
    "W2 = torch.randn((27, 27), generator=g, requires_grad=True)\n",
    "\n",
    "# Training\n",
    "for k in range(100):\n",
    "\n",
    "    x1enc = F.one_hot(X1_train, num_classes=27).float()\n",
    "    x2enc = F.one_hot(X2_train, num_classes=27).float()\n",
    "    logits = x1enc @ W1 + x2enc @ W2\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(train_num), y_train].log().mean()\n",
    "    print(loss.item())\n",
    "\n",
    "    W1.grad = None\n",
    "    W2.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    W1.data += -50 * W1.grad\n",
    "    W2.data += -50 * W2.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Excercise 5: Using Cross entorpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.010873317718506\n",
      "3.4076032638549805\n",
      "3.15413236618042\n",
      "2.9991977214813232\n",
      "2.9097745418548584\n",
      "2.8524322509765625\n",
      "2.886544704437256\n",
      "2.8798012733459473\n",
      "3.034533977508545\n",
      "2.739417552947998\n",
      "2.788029193878174\n",
      "2.965228319168091\n",
      "2.7140512466430664\n",
      "2.7134718894958496\n",
      "2.8758227825164795\n",
      "2.6637253761291504\n",
      "2.659200429916382\n",
      "2.71647572517395\n",
      "2.909717321395874\n",
      "2.6512115001678467\n",
      "2.6702890396118164\n",
      "2.822051763534546\n",
      "2.6410651206970215\n",
      "2.648153781890869\n",
      "2.7309346199035645\n",
      "2.918083906173706\n",
      "2.6609389781951904\n",
      "2.705000162124634\n",
      "2.906630039215088\n",
      "2.6440060138702393\n",
      "2.69270658493042\n",
      "2.8810672760009766\n",
      "2.633471727371216\n",
      "2.6587555408477783\n",
      "2.8496081829071045\n",
      "2.6109635829925537\n",
      "2.6022067070007324\n",
      "2.6662981510162354\n",
      "2.7107372283935547\n",
      "2.881516933441162\n",
      "2.614396810531616\n",
      "2.6699440479278564\n",
      "2.874661445617676\n",
      "2.620964527130127\n",
      "2.6492550373077393\n",
      "2.822420835494995\n",
      "2.599946975708008\n",
      "2.5769643783569336\n",
      "2.5984396934509277\n",
      "2.6693248748779297\n",
      "2.867483377456665\n",
      "2.6032111644744873\n",
      "2.643613338470459\n",
      "2.8208200931549072\n",
      "2.603586435317993\n",
      "2.57338285446167\n",
      "2.5958125591278076\n",
      "2.6711690425872803\n",
      "2.865628480911255\n",
      "2.5956807136535645\n",
      "2.643315553665161\n",
      "2.823009490966797\n",
      "2.603006601333618\n",
      "2.5741806030273438\n",
      "2.617051601409912\n",
      "2.7031264305114746\n",
      "2.886925220489502\n",
      "2.6046271324157715\n",
      "2.672621726989746\n",
      "2.869542121887207\n",
      "2.6252925395965576\n",
      "2.654972553253174\n",
      "2.8491265773773193\n",
      "2.591374397277832\n",
      "2.6202797889709473\n",
      "2.777371644973755\n",
      "2.596475601196289\n",
      "2.599654197692871\n",
      "2.6865458488464355\n",
      "2.878162145614624\n",
      "2.625089645385742\n",
      "2.669663190841675\n",
      "2.872537136077881\n",
      "2.6108648777008057\n",
      "2.6602768898010254\n",
      "2.8503904342651367\n",
      "2.6070072650909424\n",
      "2.6271235942840576\n",
      "2.8129730224609375\n",
      "2.584796667098999\n",
      "2.5689759254455566\n",
      "2.5694408416748047\n",
      "2.618753671646118\n",
      "2.776904344558716\n",
      "2.587132215499878\n",
      "2.5904252529144287\n",
      "2.68121075630188\n",
      "2.8694896697998047\n",
      "2.610822916030884\n",
      "2.6558918952941895\n"
     ]
    }
   ],
   "source": [
    "# Initialize weights\n",
    "g = torch.Generator().manual_seed(42)\n",
    "W1 = torch.randn((27, 27), generator=g, requires_grad=True)\n",
    "W2 = torch.randn((27, 27), generator=g, requires_grad=True)\n",
    "\n",
    "# Training\n",
    "for k in range(100):\n",
    "\n",
    "    # x1enc = F.one_hot(X1_train, num_classes=27).float()\n",
    "    # x2enc = F.one_hot(X2_train, num_classes=27).float()\n",
    "    logits = W1[X1_train] + W2[X1_train]\n",
    "    loss = F.cross_entropy(logits, y_train)\n",
    "    print(loss.item())\n",
    "\n",
    "    W1.grad = None\n",
    "    W2.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    W1.data += -50 * W1.grad\n",
    "    W2.data += -50 * W2.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Oct  7 2022, 15:17:23) [Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "21a124e163c92121797d725bed844fa6fdaf2c4e47bf1f149ef174ae791c682a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
