{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bigram model using probablities based on normalized counts has it's limitations.\n",
    "\n",
    "To extend it to have more context like a two characters as input the probabalities matrix will have (27*27) possilities and for three characters (27 * 27 * 27) and becomes too big.\n",
    "\n",
    "To overcome this we're gonna try out [Bengion et al.2003 MLP model paper](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbVhPSHNrVkluYVY5elh4RDZrOWd4R2xVNVRQd3xBQ3Jtc0tsUDE0UFVRQURUTTlWckExZWp4eGxFa3lRYlQ3amtYX3kxdDI1ZW5uU1pxZERidUYyZkJjSVlzd21rMndCMFlYRW5kYmZISkxfSDR1TzhaOXI1bXptUnUxU0xyUXJYeEpTZlRrTkRjTS0wTkMxNjFnSQ&q=https%3A%2F%2Fwww.jmlr.org%2Fpapers%2Fvolume3%2Fbengio03a%2Fbengio03a.pdf&v=TCH_1BHY58I)\n",
    "\n",
    "* This paper uses words but we'll proceed with characters\n",
    "* Each character will be represented as a 30 dimensional vector \n",
    "* The advantages of embeddings is knowledge transference, for examples animals like dog, cat might be closer to each other in 30 dimensional space. If cat was not in training set but this knowledge transfer will help in this case.\n",
    "\n",
    "Let's implement the below architecture in this notebook\n",
    "![fully connected MLP](https://pbs.twimg.com/media/Fhzl42hVUAI9U8V?format=jpg&name=large)\n",
    "\n",
    "* Three input characters with 30 dimensional embedding each\n",
    "* A Lookup table for characters\n",
    "* Tanh activation connected to three inputs\n",
    "* since we have 27 characters a final layer with 27 units(logits)\n",
    "* softmax on top of it to normalize the probabality\n",
    "* pluck the label based on probabality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:31.313008Z",
     "start_time": "2023-02-08T05:47:25.987840Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rebuilding training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:31.330066Z",
     "start_time": "2023-02-08T05:47:31.317289Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read all words\n",
    "def read_words():\n",
    "    words = open(\"names.txt\").read().splitlines()\n",
    "    return words\n",
    "words = read_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:31.376304Z",
     "start_time": "2023-02-08T05:47:31.339831Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:31.403383Z",
     "start_time": "2023-02-08T05:47:31.390165Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi  = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:31.426980Z",
     "start_time": "2023-02-08T05:47:31.413084Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_dataset(block_size, number_of_words: int, logs=False):\n",
    "\n",
    "    block_size = block_size # Context ength: how many characters do we take to predict the next one?\n",
    "    X, Y = [], []\n",
    "    for w in words[:number_of_words]:\n",
    "        if logs:\n",
    "            print(w)\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            if logs:\n",
    "                print(f\"Context: {context}\")\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            if logs:\n",
    "                print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "            context = context[1:] + [ix]\n",
    "            if logs:\n",
    "                print(f\"Context after append: {context}\")\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:31.442227Z",
     "start_time": "2023-02-08T05:47:31.434452Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "Context: [0, 0, 0]\n",
      "... ---> e\n",
      "Context after append: [0, 0, 5]\n",
      "Context: [0, 0, 5]\n",
      "..e ---> m\n",
      "Context after append: [0, 5, 13]\n",
      "Context: [0, 5, 13]\n",
      ".em ---> m\n",
      "Context after append: [5, 13, 13]\n",
      "Context: [5, 13, 13]\n",
      "emm ---> a\n",
      "Context after append: [13, 13, 1]\n",
      "Context: [13, 13, 1]\n",
      "mma ---> .\n",
      "Context after append: [13, 1, 0]\n",
      "olivia\n",
      "Context: [0, 0, 0]\n",
      "... ---> o\n",
      "Context after append: [0, 0, 15]\n",
      "Context: [0, 0, 15]\n",
      "..o ---> l\n",
      "Context after append: [0, 15, 12]\n",
      "Context: [0, 15, 12]\n",
      ".ol ---> i\n",
      "Context after append: [15, 12, 9]\n",
      "Context: [15, 12, 9]\n",
      "oli ---> v\n",
      "Context after append: [12, 9, 22]\n",
      "Context: [12, 9, 22]\n",
      "liv ---> i\n",
      "Context after append: [9, 22, 9]\n",
      "Context: [9, 22, 9]\n",
      "ivi ---> a\n",
      "Context after append: [22, 9, 1]\n",
      "Context: [22, 9, 1]\n",
      "via ---> .\n",
      "Context after append: [9, 1, 0]\n",
      "ava\n",
      "Context: [0, 0, 0]\n",
      "... ---> a\n",
      "Context after append: [0, 0, 1]\n",
      "Context: [0, 0, 1]\n",
      "..a ---> v\n",
      "Context after append: [0, 1, 22]\n",
      "Context: [0, 1, 22]\n",
      ".av ---> a\n",
      "Context after append: [1, 22, 1]\n",
      "Context: [1, 22, 1]\n",
      "ava ---> .\n",
      "Context after append: [22, 1, 0]\n",
      "isabella\n",
      "Context: [0, 0, 0]\n",
      "... ---> i\n",
      "Context after append: [0, 0, 9]\n",
      "Context: [0, 0, 9]\n",
      "..i ---> s\n",
      "Context after append: [0, 9, 19]\n",
      "Context: [0, 9, 19]\n",
      ".is ---> a\n",
      "Context after append: [9, 19, 1]\n",
      "Context: [9, 19, 1]\n",
      "isa ---> b\n",
      "Context after append: [19, 1, 2]\n",
      "Context: [19, 1, 2]\n",
      "sab ---> e\n",
      "Context after append: [1, 2, 5]\n",
      "Context: [1, 2, 5]\n",
      "abe ---> l\n",
      "Context after append: [2, 5, 12]\n",
      "Context: [2, 5, 12]\n",
      "bel ---> l\n",
      "Context after append: [5, 12, 12]\n",
      "Context: [5, 12, 12]\n",
      "ell ---> a\n",
      "Context after append: [12, 12, 1]\n",
      "Context: [12, 12, 1]\n",
      "lla ---> .\n",
      "Context after append: [12, 1, 0]\n",
      "sophia\n",
      "Context: [0, 0, 0]\n",
      "... ---> s\n",
      "Context after append: [0, 0, 19]\n",
      "Context: [0, 0, 19]\n",
      "..s ---> o\n",
      "Context after append: [0, 19, 15]\n",
      "Context: [0, 19, 15]\n",
      ".so ---> p\n",
      "Context after append: [19, 15, 16]\n",
      "Context: [19, 15, 16]\n",
      "sop ---> h\n",
      "Context after append: [15, 16, 8]\n",
      "Context: [15, 16, 8]\n",
      "oph ---> i\n",
      "Context after append: [16, 8, 9]\n",
      "Context: [16, 8, 9]\n",
      "phi ---> a\n",
      "Context after append: [8, 9, 1]\n",
      "Context: [8, 9, 1]\n",
      "hia ---> .\n",
      "Context after append: [9, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "X, Y = build_dataset(block_size=3, number_of_words=5, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:31.459477Z",
     "start_time": "2023-02-08T05:47:31.448995Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:31.471780Z",
     "start_time": "2023-02-08T05:47:31.462583Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0],\n",
       "        [ 0,  0,  5],\n",
       "        [ 0,  5, 13],\n",
       "        [ 5, 13, 13],\n",
       "        [13, 13,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 15],\n",
       "        [ 0, 15, 12],\n",
       "        [15, 12,  9],\n",
       "        [12,  9, 22],\n",
       "        [ 9, 22,  9],\n",
       "        [22,  9,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  1],\n",
       "        [ 0,  1, 22],\n",
       "        [ 1, 22,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  9],\n",
       "        [ 0,  9, 19],\n",
       "        [ 9, 19,  1],\n",
       "        [19,  1,  2],\n",
       "        [ 1,  2,  5],\n",
       "        [ 2,  5, 12],\n",
       "        [ 5, 12, 12],\n",
       "        [12, 12,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 19],\n",
       "        [ 0, 19, 15],\n",
       "        [19, 15, 16],\n",
       "        [15, 16,  8],\n",
       "        [16,  8,  9],\n",
       "        [ 8,  9,  1]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:31.487751Z",
     "start_time": "2023-02-08T05:47:31.475262Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,\n",
       "         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've the dataset, let's build the embedding lookup table\n",
    "\n",
    "## Embedding lookup table\n",
    "\n",
    "For 1700 words, 30 dimension space was used in paper. For 27 possiblities(characters) let's try a 2 dimensionsal embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:31.528393Z",
     "start_time": "2023-02-08T05:47:31.509917Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.0654, -0.4189],\n",
       "         [-0.1886,  0.6077],\n",
       "         [-0.6493, -1.2236],\n",
       "         [ 0.0520, -1.1606],\n",
       "         [-0.2647,  0.2973],\n",
       "         [-0.8276,  0.2831],\n",
       "         [-0.0419,  0.4953],\n",
       "         [-0.5341, -0.0607],\n",
       "         [-0.8142,  1.5791],\n",
       "         [ 0.6127, -1.3213],\n",
       "         [ 1.3088, -0.8507],\n",
       "         [ 1.8939,  2.4032],\n",
       "         [ 1.0523,  0.7024],\n",
       "         [ 0.4999,  0.7069],\n",
       "         [ 0.7210,  0.6423],\n",
       "         [-0.4966, -0.9387],\n",
       "         [-0.4950, -0.1859],\n",
       "         [ 0.0176,  2.2793],\n",
       "         [ 0.2297,  2.8804],\n",
       "         [-0.9615, -0.6350],\n",
       "         [ 0.6027,  2.3288],\n",
       "         [ 1.5895,  1.1990],\n",
       "         [-0.1330,  0.0134],\n",
       "         [-0.8035,  0.2756],\n",
       "         [-1.5771,  0.3892],\n",
       "         [ 0.7817, -0.1250],\n",
       "         [ 0.2701, -1.2812]]),\n",
       " torch.Size([27, 2]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialized randomnly\n",
    "C = torch.randn((27, 2))\n",
    "C, C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:31.552439Z",
     "start_time": "2023-02-08T05:47:31.537264Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.8276,  0.2831])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The lookup of embedding for single character can be done two ways\n",
    "# 1. Indexing\n",
    "C[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:31.574585Z",
     "start_time": "2023-02-08T05:47:31.560462Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.8276,  0.2831])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Onehot\n",
    "F.one_hot(torch.tensor(5), num_classes=27).float() @ C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing and one hot encoding gives the same results. We'll use indexing as it's faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:31.600985Z",
     "start_time": "2023-02-08T05:47:31.585045Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8276,  0.2831],\n",
      "        [-0.0419,  0.4953],\n",
      "        [-0.5341, -0.0607]])\n",
      "tensor([[-0.8276,  0.2831],\n",
      "        [-0.0419,  0.4953],\n",
      "        [-0.5341, -0.0607]])\n"
     ]
    }
   ],
   "source": [
    "# Indexing multiple values\n",
    "# Singce our shape of input is 32, 3\n",
    "print(C[[5, 6, 7]])\n",
    "# Works also with tensor\n",
    "print(C[torch.tensor([5, 6, 7])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:31.623028Z",
     "start_time": "2023-02-08T05:47:31.607657Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0654, -0.4189],\n",
       "         [ 1.0654, -0.4189],\n",
       "         [ 1.0654, -0.4189]],\n",
       "\n",
       "        [[ 1.0654, -0.4189],\n",
       "         [ 1.0654, -0.4189],\n",
       "         [-0.8276,  0.2831]],\n",
       "\n",
       "        [[ 1.0654, -0.4189],\n",
       "         [-0.8276,  0.2831],\n",
       "         [ 0.4999,  0.7069]],\n",
       "\n",
       "        [[-0.8276,  0.2831],\n",
       "         [ 0.4999,  0.7069],\n",
       "         [ 0.4999,  0.7069]],\n",
       "\n",
       "        [[ 0.4999,  0.7069],\n",
       "         [ 0.4999,  0.7069],\n",
       "         [-0.1886,  0.6077]],\n",
       "\n",
       "        [[ 1.0654, -0.4189],\n",
       "         [ 1.0654, -0.4189],\n",
       "         [ 1.0654, -0.4189]],\n",
       "\n",
       "        [[ 1.0654, -0.4189],\n",
       "         [ 1.0654, -0.4189],\n",
       "         [-0.4966, -0.9387]],\n",
       "\n",
       "        [[ 1.0654, -0.4189],\n",
       "         [-0.4966, -0.9387],\n",
       "         [ 1.0523,  0.7024]],\n",
       "\n",
       "        [[-0.4966, -0.9387],\n",
       "         [ 1.0523,  0.7024],\n",
       "         [ 0.6127, -1.3213]],\n",
       "\n",
       "        [[ 1.0523,  0.7024],\n",
       "         [ 0.6127, -1.3213],\n",
       "         [-0.1330,  0.0134]],\n",
       "\n",
       "        [[ 0.6127, -1.3213],\n",
       "         [-0.1330,  0.0134],\n",
       "         [ 0.6127, -1.3213]],\n",
       "\n",
       "        [[-0.1330,  0.0134],\n",
       "         [ 0.6127, -1.3213],\n",
       "         [-0.1886,  0.6077]],\n",
       "\n",
       "        [[ 1.0654, -0.4189],\n",
       "         [ 1.0654, -0.4189],\n",
       "         [ 1.0654, -0.4189]],\n",
       "\n",
       "        [[ 1.0654, -0.4189],\n",
       "         [ 1.0654, -0.4189],\n",
       "         [-0.1886,  0.6077]],\n",
       "\n",
       "        [[ 1.0654, -0.4189],\n",
       "         [-0.1886,  0.6077],\n",
       "         [-0.1330,  0.0134]],\n",
       "\n",
       "        [[-0.1886,  0.6077],\n",
       "         [-0.1330,  0.0134],\n",
       "         [-0.1886,  0.6077]],\n",
       "\n",
       "        [[ 1.0654, -0.4189],\n",
       "         [ 1.0654, -0.4189],\n",
       "         [ 1.0654, -0.4189]],\n",
       "\n",
       "        [[ 1.0654, -0.4189],\n",
       "         [ 1.0654, -0.4189],\n",
       "         [ 0.6127, -1.3213]],\n",
       "\n",
       "        [[ 1.0654, -0.4189],\n",
       "         [ 0.6127, -1.3213],\n",
       "         [-0.9615, -0.6350]],\n",
       "\n",
       "        [[ 0.6127, -1.3213],\n",
       "         [-0.9615, -0.6350],\n",
       "         [-0.1886,  0.6077]],\n",
       "\n",
       "        [[-0.9615, -0.6350],\n",
       "         [-0.1886,  0.6077],\n",
       "         [-0.6493, -1.2236]],\n",
       "\n",
       "        [[-0.1886,  0.6077],\n",
       "         [-0.6493, -1.2236],\n",
       "         [-0.8276,  0.2831]],\n",
       "\n",
       "        [[-0.6493, -1.2236],\n",
       "         [-0.8276,  0.2831],\n",
       "         [ 1.0523,  0.7024]],\n",
       "\n",
       "        [[-0.8276,  0.2831],\n",
       "         [ 1.0523,  0.7024],\n",
       "         [ 1.0523,  0.7024]],\n",
       "\n",
       "        [[ 1.0523,  0.7024],\n",
       "         [ 1.0523,  0.7024],\n",
       "         [-0.1886,  0.6077]],\n",
       "\n",
       "        [[ 1.0654, -0.4189],\n",
       "         [ 1.0654, -0.4189],\n",
       "         [ 1.0654, -0.4189]],\n",
       "\n",
       "        [[ 1.0654, -0.4189],\n",
       "         [ 1.0654, -0.4189],\n",
       "         [-0.9615, -0.6350]],\n",
       "\n",
       "        [[ 1.0654, -0.4189],\n",
       "         [-0.9615, -0.6350],\n",
       "         [-0.4966, -0.9387]],\n",
       "\n",
       "        [[-0.9615, -0.6350],\n",
       "         [-0.4966, -0.9387],\n",
       "         [-0.4950, -0.1859]],\n",
       "\n",
       "        [[-0.4966, -0.9387],\n",
       "         [-0.4950, -0.1859],\n",
       "         [-0.8142,  1.5791]],\n",
       "\n",
       "        [[-0.4950, -0.1859],\n",
       "         [-0.8142,  1.5791],\n",
       "         [ 0.6127, -1.3213]],\n",
       "\n",
       "        [[-0.8142,  1.5791],\n",
       "         [ 0.6127, -1.3213],\n",
       "         [-0.1886,  0.6077]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The total equivalent would be\n",
    "C[X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:31.638506Z",
     "start_time": "2023-02-08T05:47:31.627352Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's verify this\n",
    "C[X].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "32 is total number of inputs with shape 3 and dimensional embedding 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:31.653540Z",
     "start_time": "2023-02-08T05:47:31.644716Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[13, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:31.678282Z",
     "start_time": "2023-02-08T05:47:31.662373Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1886,  0.6077])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X][13, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:31.697727Z",
     "start_time": "2023-02-08T05:47:31.684945Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1886,  0.6077])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:31.716937Z",
     "start_time": "2023-02-08T05:47:31.705431Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X]\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the embedding lookup table is completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the hidden layer plus internals of torch.Tensor, storage and views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:31.735053Z",
     "start_time": "2023-02-08T05:47:31.727489Z"
    }
   },
   "outputs": [],
   "source": [
    "# Intitializing weights and biases\n",
    "W1 = torch.randn((\n",
    "    6, # 3(inputs) * 2(embedding dim)\n",
    "    100 # Number of neurons\n",
    "))\n",
    "b1 = torch.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:31.753513Z",
     "start_time": "2023-02-08T05:47:31.742269Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 100])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:32.434474Z",
     "start_time": "2023-02-08T05:47:31.760805Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (96x2 and 6x100)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/jayaprakashsivagami/Documents/Tech/ML/makemore/3-mlp.ipynb Cell 31\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jayaprakashsivagami/Documents/Tech/ML/makemore/3-mlp.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Inputs * weights + bias will not work  now\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jayaprakashsivagami/Documents/Tech/ML/makemore/3-mlp.ipynb#X42sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# as dimensions of weighs and input doesn't abide\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jayaprakashsivagami/Documents/Tech/ML/makemore/3-mlp.ipynb#X42sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# by matrix multiplication rulees\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jayaprakashsivagami/Documents/Tech/ML/makemore/3-mlp.ipynb#X42sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# shape of input [32, 3, 2], weights [6, 100]\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jayaprakashsivagami/Documents/Tech/ML/makemore/3-mlp.ipynb#X42sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m emb \u001b[39m@\u001b[39;49m W1 \u001b[39m+\u001b[39m b1\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (96x2 and 6x100)"
     ]
    }
   ],
   "source": [
    "# # Inputs * weights + bias will not work  now\n",
    "# # as dimensions of weighs and input doesn't abide\n",
    "# # by matrix multiplication rulees\n",
    "# # shape of input [32, 3, 2], weights [6, 100]\n",
    "# emb @ W1 + b1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch's tensor's a really powerful, because ut has tons of methods to allow us to create modify and perfom lot's of operations on it.\n",
    "\n",
    "We're gonna use [torch.cat](https://pytorch.org/docs/stable/generated/torch.cat.html) to tackle the above problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:32.440581Z",
     "start_time": "2023-02-08T05:47:32.440548Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_tensors = torch.cat([emb[:, 0, :], emb[:, 1, :], emb[:, 2, :]], 1)\n",
    "cat_tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:32.444904Z",
     "start_time": "2023-02-08T05:47:32.444856Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To generalize this in case of diffrent block size\n",
    "# We'll use unbind with cat\n",
    "unbind_tensors = torch.unbind(emb, 1)\n",
    "# Gives a list which is exactly the same\n",
    "# as cat_tensors abov\n",
    "len(unbind_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:32.448000Z",
     "start_time": "2023-02-08T05:47:32.447968Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_unbind_tensors = torch.cat(unbind_tensors, 1)\n",
    "cat_unbind_tensors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now irrespective of block size the above code will run.\n",
    "\n",
    "But there's an efficient way to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:32.499894Z",
     "start_time": "2023-02-08T05:47:32.499844Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(18)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:32.504939Z",
     "start_time": "2023-02-08T05:47:32.504899Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:32.510427Z",
     "start_time": "2023-02-08T05:47:32.510355Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1],\n",
       "         [ 2,  3],\n",
       "         [ 4,  5]],\n",
       "\n",
       "        [[ 6,  7],\n",
       "         [ 8,  9],\n",
       "         [10, 11]],\n",
       "\n",
       "        [[12, 13],\n",
       "         [14, 15],\n",
       "         [16, 17]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(3, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:32.518799Z",
     "start_time": "2023-02-08T05:47:32.518763Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 6\n",
       " 7\n",
       " 8\n",
       " 9\n",
       " 10\n",
       " 11\n",
       " 12\n",
       " 13\n",
       " 14\n",
       " 15\n",
       " 16\n",
       " 17\n",
       "[torch.storage._TypedStorage(dtype=torch.int64, device=cpu) of size 18]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.storage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every tensor has view and storage,\n",
    "* Using tensor.view(shape) we can manipulate the shape of an tensor\n",
    "* But tensor.storage() in memory will still remain a single dimension vector\n",
    "* And using view just changes some attributes like offest etc and tensor in memory remains same to the multiples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:32.522136Z",
     "start_time": "2023-02-08T05:47:32.522104Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0654, -0.4189,  1.0654, -0.4189,  1.0654, -0.4189],\n",
       "        [ 1.0654, -0.4189,  1.0654, -0.4189, -0.8276,  0.2831],\n",
       "        [ 1.0654, -0.4189, -0.8276,  0.2831,  0.4999,  0.7069],\n",
       "        [-0.8276,  0.2831,  0.4999,  0.7069,  0.4999,  0.7069],\n",
       "        [ 0.4999,  0.7069,  0.4999,  0.7069, -0.1886,  0.6077],\n",
       "        [ 1.0654, -0.4189,  1.0654, -0.4189,  1.0654, -0.4189],\n",
       "        [ 1.0654, -0.4189,  1.0654, -0.4189, -0.4966, -0.9387],\n",
       "        [ 1.0654, -0.4189, -0.4966, -0.9387,  1.0523,  0.7024],\n",
       "        [-0.4966, -0.9387,  1.0523,  0.7024,  0.6127, -1.3213],\n",
       "        [ 1.0523,  0.7024,  0.6127, -1.3213, -0.1330,  0.0134],\n",
       "        [ 0.6127, -1.3213, -0.1330,  0.0134,  0.6127, -1.3213],\n",
       "        [-0.1330,  0.0134,  0.6127, -1.3213, -0.1886,  0.6077],\n",
       "        [ 1.0654, -0.4189,  1.0654, -0.4189,  1.0654, -0.4189],\n",
       "        [ 1.0654, -0.4189,  1.0654, -0.4189, -0.1886,  0.6077],\n",
       "        [ 1.0654, -0.4189, -0.1886,  0.6077, -0.1330,  0.0134],\n",
       "        [-0.1886,  0.6077, -0.1330,  0.0134, -0.1886,  0.6077],\n",
       "        [ 1.0654, -0.4189,  1.0654, -0.4189,  1.0654, -0.4189],\n",
       "        [ 1.0654, -0.4189,  1.0654, -0.4189,  0.6127, -1.3213],\n",
       "        [ 1.0654, -0.4189,  0.6127, -1.3213, -0.9615, -0.6350],\n",
       "        [ 0.6127, -1.3213, -0.9615, -0.6350, -0.1886,  0.6077],\n",
       "        [-0.9615, -0.6350, -0.1886,  0.6077, -0.6493, -1.2236],\n",
       "        [-0.1886,  0.6077, -0.6493, -1.2236, -0.8276,  0.2831],\n",
       "        [-0.6493, -1.2236, -0.8276,  0.2831,  1.0523,  0.7024],\n",
       "        [-0.8276,  0.2831,  1.0523,  0.7024,  1.0523,  0.7024],\n",
       "        [ 1.0523,  0.7024,  1.0523,  0.7024, -0.1886,  0.6077],\n",
       "        [ 1.0654, -0.4189,  1.0654, -0.4189,  1.0654, -0.4189],\n",
       "        [ 1.0654, -0.4189,  1.0654, -0.4189, -0.9615, -0.6350],\n",
       "        [ 1.0654, -0.4189, -0.9615, -0.6350, -0.4966, -0.9387],\n",
       "        [-0.9615, -0.6350, -0.4966, -0.9387, -0.4950, -0.1859],\n",
       "        [-0.4966, -0.9387, -0.4950, -0.1859, -0.8142,  1.5791],\n",
       "        [-0.4950, -0.1859, -0.8142,  1.5791,  0.6127, -1.3213],\n",
       "        [-0.8142,  1.5791,  0.6127, -1.3213, -0.1886,  0.6077]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's use view() to reshape the tensor from [32, 3, 2] to [32, 6]\n",
    "emb.view(32, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way this happens is dimension 1 get stacked up as a single dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:32.530603Z",
     "start_time": "2023-02-08T05:47:32.530557Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.view(32, 6).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:32.535899Z",
     "start_time": "2023-02-08T05:47:32.535862Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.view(32, 6) == torch.cat(torch.unbind(emb, 1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Element wise comparison proves that view is equal to cat(unbind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:32.538876Z",
     "start_time": "2023-02-08T05:47:32.538838Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4853,  0.2888, -0.2918,  ..., -0.8167,  0.9480,  0.9694],\n",
       "        [ 0.9108, -0.0636, -0.7560,  ...,  0.4664, -0.0676, -0.9405],\n",
       "        [ 0.4041, -0.9960,  0.0799,  ..., -0.9908, -0.1369,  0.9820],\n",
       "        ...,\n",
       "        [ 0.9995, -0.9983, -0.3057,  ..., -0.3614, -0.9605, -0.9880],\n",
       "        [-0.4247, -0.9971,  0.9978,  ...,  0.9490, -0.0748,  0.9997],\n",
       "        [ 0.9996,  0.9533, -0.9478,  ..., -0.8885,  0.3754, -0.2169]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:32.545348Z",
     "start_time": "2023-02-08T05:47:32.545317Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure broadcasting is done right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:32.550674Z",
     "start_time": "2023-02-08T05:47:32.550632Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(emb.view(32, 6) @ W1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T05:47:32.554705Z",
     "start_time": "2023-02-08T05:47:32.554665Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "32, 100\n",
    "1 , 100\n",
    "\n",
    "* broadcasting 32, 100 to 100\n",
    "* broadcasting aligns from right abd creates a  fake dimension (1)\n",
    "* Then 32 will be copied vertically for every element of 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = torch.randn((100, # Inputs layer size\n",
    "                  27 # Output layer 27 characters\n",
    "                 ))\n",
    "b2 = torch.randn(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = h @ W2 + b2\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implmenting negative log likelihood loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake counts -> logits exp\n",
    "counts = logits.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize fake counts\n",
    "prob = counts / counts.sum(1, keepdims=True)\n",
    "prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.3058e-08, 1.2767e-04, 8.5712e-07, 1.2972e-04, 5.1670e-07, 7.7393e-05,\n",
       "        4.4319e-05, 2.3391e-02, 4.8847e-05, 8.2222e-02, 1.3864e-03, 4.6708e-07,\n",
       "        1.8181e-07, 5.0070e-06, 6.5440e-02, 2.9453e-03, 1.9222e-03, 2.6597e-04,\n",
       "        7.5348e-03, 1.0611e-08, 1.5309e-09, 4.2748e-09, 5.2080e-08, 1.1132e-05,\n",
       "        7.7567e-08, 1.3574e-04, 1.5560e-02, 2.1154e-12, 8.6029e-03, 4.0230e-10,\n",
       "        2.1846e-06, 3.0058e-04])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indexing probabalites based on Y\n",
    "# This probabalities in future will be the probabalities by neural network\n",
    "prob[torch.arange(32), Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.4675)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = - prob[torch.arange(32), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of full network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "W1 = torch.randn((6, 100), generator=g)\n",
    "b1 = torch.randn(100, generator=g)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "paramerters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of parameters in total\n",
    "sum(p.nelement() for p in paramerters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.7697)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X]\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "counts = logits.exp()\n",
    "prob = counts / counts.sum(1, keepdim=True)\n",
    "loss = - prob[torch.arange(32), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross entropy why?\n",
    "\n",
    "```\n",
    "counts = logits.exp()\n",
    "prob = counts / counts.sum(1, keepdim=True)\n",
    "loss = - prob[torch.arange(32), Y].log().mean()\n",
    "```\n",
    "\n",
    "PyTorch creates a seperate tensor for each of these step\n",
    "\n",
    "1. Uses a fused kernel which combines all the above operations\n",
    "2. In backward pass, expression takes much simpler form mathametically\n",
    "3. Under the hood, this is numerically well behaved\n",
    "4. Forward pass and backward pass are much more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.7697)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = F.cross_entropy(logits, Y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.0466e-04, 3.3281e-04, 6.6846e-03, 9.9208e-01])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numerical stability difference\n",
    "logits = torch.tensor([-2, -3, 0, 5])\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum()\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts: tensor([3.7835e-44, 4.9787e-02, 1.0000e+00,        inf])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., nan])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numerical stability difference\n",
    "# With more extreme values, which will occur in backpropgation\n",
    "logits = torch.tensor([-100, -3, 0, 100])\n",
    "counts = logits.exp()\n",
    "print(f\"Counts: {counts}\")\n",
    "probs = counts / counts.sum()\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's happening above is the floating point ran out of dynamic range for exp(100) returning inf\n",
    "and for negative 100 the probs is near zero.\n",
    "\n",
    "So we cannot pass very larger number to our logits --> loss expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts: tensor([0.0000e+00, 1.4013e-45, 3.7835e-44, 1.0000e+00])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 1.4013e-45, 3.7835e-44, 1.0000e+00])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How PyTorch handles this is\n",
    "# By finding maximum of the logits and offsets it from the logits to avoid it\n",
    "logits = torch.tensor([-100, -3, 0, 100]) - 100\n",
    "counts = logits.exp()\n",
    "print(f\"Counts: {counts}\")\n",
    "probs = counts / counts.sum()\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing training loop, overfitting one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2561391294002533\n"
     ]
    }
   ],
   "source": [
    "# Set requires grad\n",
    "for p in paramerters:\n",
    "    p.requires_grad = True\n",
    "for _ in range(1000):\n",
    "    # Forward pass\n",
    "    emb = C[X] # [32, 3, 2]\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (100, 27)\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    # Backward pass\n",
    "    for p in paramerters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update parametrs\n",
    "    for p in paramerters:\n",
    "        p.data += -0.1 * p.grad\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've achieved a very good loss. Why?\n",
    "Because we're fitting the model for only 5 words i.e 32 inputs and with 3481 parameters.\n",
    "Lots of paramters for very less data.\n",
    "What we're doing it overfitting the model for one batch of data.\n",
    "\n",
    "> Note: Based on this overfitting can be defined as tuning many parameters for few samples or a batch of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why loss of 0 is not achieved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([13.3348, 17.7904, 20.6014, 20.6121, 16.7355, 13.3348, 15.9983, 14.1722,\n",
       "        15.9145, 18.3614, 15.9395, 20.9265, 13.3348, 17.1090, 17.1319, 20.0602,\n",
       "        13.3348, 16.5893, 15.1017, 17.0581, 18.5860, 15.9670, 10.8740, 10.6871,\n",
       "        15.5056, 13.3348, 16.1795, 16.9743, 12.7426, 16.2009, 19.0845, 16.0196],\n",
       "       grad_fn=<MaxBackward0>),\n",
       "indices=tensor([19, 13, 13,  1,  0, 19, 12,  9, 22,  9,  1,  0, 19, 22,  1,  0, 19, 19,\n",
       "         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,\n",
       "         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the indices of logits and output are same for most of the cases, these are inputs overfitted to outputs. But the missing ones are \n",
    "\n",
    "* ... --> e (emma)\n",
    "* ... --> o (olivia)\n",
    "* ... --> a (ava)\n",
    "* ... --> s (sophia)\n",
    "\n",
    "for different outptus for the same input.\n",
    "\n",
    "To overcome this, let's train on the full dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on full dataset, minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = build_dataset(block_size=3, number_of_words=len(words),logs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3]), torch.int64, torch.Size([228146]), torch.int64)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 3481\n",
      "19.505226135253906\n",
      "17.08448600769043\n",
      "15.01388168334961\n",
      "13.038187026977539\n",
      "11.437609672546387\n",
      "10.590084075927734\n",
      "9.906832695007324\n",
      "9.05928897857666\n",
      "8.551031112670898\n",
      "8.103048324584961\n",
      "Final loss: 8.103048324584961\n"
     ]
    }
   ],
   "source": [
    "# Reinitializing parameters for full batch training\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "W1 = torch.randn((6, 100), generator=g)\n",
    "b1 = torch.randn(100, generator=g)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "print(f\"Total parameters: {sum(p.nelement() for p in parameters)}\")\n",
    "\n",
    "# Setting requires_grad for forward and backward pass\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# Full training\n",
    "for _ in range(10):\n",
    "    \n",
    "    # Forward pass\n",
    "    emb = C[X]\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    print(loss.item())\n",
    "\n",
    "    # Set gradients to None\n",
    "    for p in paramerters:\n",
    "        p.grad = None\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights\n",
    "    for p in parameters:\n",
    "        p.data += -0.1 * p.grad\n",
    "\n",
    "print(f\"Final loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each epoch is taking more time, becuase forwarding and backwarding for 228146 samples compared to 32 samples.\n",
    "We can overcome this by training on minibatches instead of training on entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 11935, 124211,  72353, 134168,  19590, 179257,  35262, 151701, 207486,\n",
       "        209831, 160545, 200320, 216752, 131008, 162158,  14874, 155237,  69108,\n",
       "        134049, 175288,  36177, 191279, 220363,   8856, 157049, 155053,  47109,\n",
       "         39437, 165389, 176877,  49441, 180473])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mini batch size of 32\n",
    "# from 0 to number of samples which is X.shape[0]\n",
    "# This willl index 32 random samples in X\n",
    "torch.randint(0, X.shape[0], (32,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 3481\n",
      "19.702402114868164\n",
      "16.064176559448242\n",
      "14.076654434204102\n",
      "14.768239974975586\n",
      "12.371623039245605\n",
      "9.560648918151855\n",
      "10.0352783203125\n",
      "11.724791526794434\n",
      "8.231781959533691\n",
      "10.529947280883789\n",
      "9.846911430358887\n",
      "9.746484756469727\n",
      "10.548674583435059\n",
      "8.493870735168457\n",
      "11.265069007873535\n",
      "8.331384658813477\n",
      "13.268254280090332\n",
      "9.70434856414795\n",
      "11.153634071350098\n",
      "9.92152214050293\n",
      "10.438732147216797\n",
      "8.24794864654541\n",
      "7.637126922607422\n",
      "11.956585884094238\n",
      "10.920798301696777\n",
      "15.176382064819336\n",
      "13.008596420288086\n",
      "9.217453956604004\n",
      "8.517247200012207\n",
      "8.055900573730469\n",
      "10.785613059997559\n",
      "12.562047958374023\n",
      "14.770636558532715\n",
      "9.51086711883545\n",
      "9.841085433959961\n",
      "11.990396499633789\n",
      "11.04257869720459\n",
      "12.187171936035156\n",
      "9.408275604248047\n",
      "11.243504524230957\n",
      "8.065425872802734\n",
      "9.206563949584961\n",
      "10.430734634399414\n",
      "12.36136245727539\n",
      "10.052800178527832\n",
      "8.389205932617188\n",
      "9.108859062194824\n",
      "9.29414176940918\n",
      "10.46970272064209\n",
      "9.092045783996582\n",
      "9.868001937866211\n",
      "7.277458667755127\n",
      "9.16922664642334\n",
      "10.721840858459473\n",
      "8.645291328430176\n",
      "9.26922607421875\n",
      "9.469657897949219\n",
      "8.617011070251465\n",
      "10.779376029968262\n",
      "6.185026168823242\n",
      "7.496553897857666\n",
      "10.378456115722656\n",
      "7.76173734664917\n",
      "7.967223167419434\n",
      "8.624605178833008\n",
      "6.679420471191406\n",
      "10.068009376525879\n",
      "11.345060348510742\n",
      "11.627767562866211\n",
      "11.122184753417969\n",
      "8.022132873535156\n",
      "13.700371742248535\n",
      "9.163331031799316\n",
      "9.116392135620117\n",
      "8.774090766906738\n",
      "11.657366752624512\n",
      "9.19992446899414\n",
      "9.337817192077637\n",
      "7.812110424041748\n",
      "13.1530122756958\n",
      "9.239370346069336\n",
      "8.62656307220459\n",
      "12.806925773620605\n",
      "14.232527732849121\n",
      "14.52181625366211\n",
      "12.021632194519043\n",
      "7.837029933929443\n",
      "7.466141700744629\n",
      "9.435796737670898\n",
      "12.442342758178711\n",
      "16.11588478088379\n",
      "12.670682907104492\n",
      "20.90538787841797\n",
      "9.760855674743652\n",
      "12.509658813476562\n",
      "15.567769050598145\n",
      "15.035888671875\n",
      "17.027626037597656\n",
      "17.39141082763672\n",
      "12.330612182617188\n",
      "Final loss: 12.330612182617188\n"
     ]
    }
   ],
   "source": [
    "# Reinitializing parameters for full batch training\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "W1 = torch.randn((6, 100), generator=g)\n",
    "b1 = torch.randn(100, generator=g)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "print(f\"Total parameters: {sum(p.nelement() for p in parameters)}\")\n",
    "\n",
    "# Setting requires_grad for forward and backward pass\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# Full training\n",
    "for _ in range(100):\n",
    "\n",
    "    # mini batch indexes\n",
    "    ix = torch.randint(0, X.shape[0], (32,))\n",
    "    \n",
    "    # Forward pass\n",
    "    emb = C[X[ix]]\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Y[ix])\n",
    "    print(loss.item())\n",
    "\n",
    "    # Set gradients to None\n",
    "    for p in paramerters:\n",
    "        p.grad = None\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights\n",
    "    for p in parameters:\n",
    "        p.data += -0.1 * p.grad\n",
    "\n",
    "print(f\"Final loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adavantages with mini batching:\n",
    "\n",
    "* Faster training cycle\n",
    "* Random indexing leads to differnt data for each mini batch\n",
    "\n",
    "> Since model's trained on mini batches, the quality of gradient is lower and is not reliable. It's not the actual gradient direction. But the gradient direction is good enough even when estimated on a mini batch(32 samples) to be useful.\n",
    "It's much better to have a approximate gradient direction and more steps than an exact or actual gradient with much fewer steps.\n",
    "That's why this works in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.3061, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation loss for entire dataset\n",
    "emb = C[X]\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding a good learning rate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find a good learnning rate...\n",
    "\n",
    "We need to find a good lower limt and an upper limit where loss is improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 3481\n",
      "20.37920379638672\n",
      "15.407710075378418\n",
      "16.412675857543945\n",
      "17.54568099975586\n",
      "18.5762939453125\n",
      "22.405019760131836\n",
      "18.445533752441406\n",
      "20.647733688354492\n",
      "28.81016731262207\n",
      "25.190343856811523\n",
      "29.80751609802246\n",
      "26.821218490600586\n",
      "29.669536590576172\n",
      "37.06035614013672\n",
      "35.266849517822266\n",
      "40.93330001831055\n",
      "32.43244934082031\n",
      "34.60240173339844\n",
      "40.897247314453125\n",
      "36.45936965942383\n",
      "44.24363327026367\n",
      "66.030029296875\n",
      "55.85255813598633\n",
      "46.87792205810547\n",
      "50.18940353393555\n",
      "53.130191802978516\n",
      "53.6685791015625\n",
      "53.440494537353516\n",
      "72.30339050292969\n",
      "48.312564849853516\n",
      "51.28059005737305\n",
      "62.9600830078125\n",
      "65.65316772460938\n",
      "64.85567474365234\n",
      "104.67060852050781\n",
      "72.69252014160156\n",
      "57.01382827758789\n",
      "72.72966766357422\n",
      "73.5486068725586\n",
      "79.130615234375\n",
      "73.91726684570312\n",
      "71.82252502441406\n",
      "55.20343017578125\n",
      "81.92620849609375\n",
      "99.6612548828125\n",
      "80.14088439941406\n",
      "100.89100646972656\n",
      "109.7134780883789\n",
      "111.91908264160156\n",
      "150.85977172851562\n",
      "110.28511047363281\n",
      "101.30892944335938\n",
      "132.65773010253906\n",
      "117.78099822998047\n",
      "140.22377014160156\n",
      "124.61674499511719\n",
      "137.62283325195312\n",
      "133.80929565429688\n",
      "110.05054473876953\n",
      "147.5557861328125\n",
      "106.35201263427734\n",
      "132.02410888671875\n",
      "135.0079345703125\n",
      "129.6615753173828\n",
      "153.99136352539062\n",
      "175.47613525390625\n",
      "123.5595474243164\n",
      "117.9556655883789\n",
      "124.50348663330078\n",
      "151.954345703125\n",
      "161.3374481201172\n",
      "150.83029174804688\n",
      "170.9187774658203\n",
      "170.23631286621094\n",
      "180.96275329589844\n",
      "175.54428100585938\n",
      "180.71804809570312\n",
      "208.80101013183594\n",
      "176.5261993408203\n",
      "211.3900146484375\n",
      "171.9788818359375\n",
      "167.6028289794922\n",
      "245.276611328125\n",
      "191.60186767578125\n",
      "233.2962188720703\n",
      "213.4855194091797\n",
      "213.18031311035156\n",
      "235.92955017089844\n",
      "173.8316650390625\n",
      "179.3195343017578\n",
      "143.92782592773438\n",
      "196.8228302001953\n",
      "167.15057373046875\n",
      "256.45458984375\n",
      "200.33445739746094\n",
      "243.5708465576172\n",
      "182.8363800048828\n",
      "263.2546691894531\n",
      "191.7423095703125\n",
      "206.25723266601562\n",
      "Final loss: 206.25723266601562\n"
     ]
    }
   ],
   "source": [
    "# Reinitializing parameters for full batch training\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "W1 = torch.randn((6, 100), generator=g)\n",
    "b1 = torch.randn(100, generator=g)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "print(f\"Total parameters: {sum(p.nelement() for p in parameters)}\")\n",
    "\n",
    "# Setting requires_grad for forward and backward pass\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# Full training\n",
    "for _ in range(100):\n",
    "\n",
    "    # mini batch indexes\n",
    "    ix = torch.randint(0, X.shape[0], (32,))\n",
    "    \n",
    "    # Forward pass\n",
    "    emb = C[X[ix]]\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Y[ix])\n",
    "    print(loss.item())\n",
    "\n",
    "    # Set gradients to None\n",
    "    for p in paramerters:\n",
    "        p.grad = None\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights\n",
    "    for p in parameters:\n",
    "        p.data += -1 * p.grad\n",
    "\n",
    "print(f\"Final loss: {loss.item()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying out different learning rates from -0.0001 and -10.. Good lower limit and upper limit are -0.01 and -1 respectivley.\n",
    "With these learning rates the convergence is not really slow or exploding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "lre = torch.linspace(\n",
    "    -3, # 10**2-3 = 0.01\n",
    "    0, # 10**0 = 1\n",
    "    1000 # 1000 learning rates from upper limit and lower limit exponentially stepped\n",
    ")\n",
    "lrs = 10**lre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parametrs: 3481\n",
      "18.40856170654297\n",
      "17.719409942626953\n",
      "18.340105056762695\n",
      "17.293140411376953\n",
      "17.35254669189453\n",
      "15.357095718383789\n",
      "17.14730453491211\n",
      "16.20711898803711\n",
      "18.068002700805664\n",
      "18.942245483398438\n",
      "19.66343116760254\n",
      "18.771602630615234\n",
      "17.0129337310791\n",
      "18.918018341064453\n",
      "18.866239547729492\n",
      "19.30284309387207\n",
      "18.15118408203125\n",
      "17.497875213623047\n",
      "19.82731056213379\n",
      "14.825553894042969\n",
      "18.89386558532715\n",
      "16.869924545288086\n",
      "17.26644515991211\n",
      "19.305879592895508\n",
      "15.9006929397583\n",
      "17.038116455078125\n",
      "17.259765625\n",
      "14.113883018493652\n",
      "18.43065643310547\n",
      "17.829343795776367\n",
      "13.794561386108398\n",
      "18.443574905395508\n",
      "16.30689811706543\n",
      "17.15144920349121\n",
      "19.55670928955078\n",
      "16.40468978881836\n",
      "14.635644912719727\n",
      "19.18668556213379\n",
      "16.15546989440918\n",
      "15.72612476348877\n",
      "17.107269287109375\n",
      "14.026529312133789\n",
      "19.77104949951172\n",
      "16.71856689453125\n",
      "16.514358520507812\n",
      "13.421768188476562\n",
      "16.951494216918945\n",
      "16.240299224853516\n",
      "18.620641708374023\n",
      "16.506704330444336\n",
      "18.084793090820312\n",
      "17.371612548828125\n",
      "15.185467720031738\n",
      "13.040621757507324\n",
      "15.475895881652832\n",
      "15.277630805969238\n",
      "17.18817710876465\n",
      "14.203689575195312\n",
      "14.51739501953125\n",
      "16.529321670532227\n",
      "15.705306053161621\n",
      "17.077800750732422\n",
      "15.641571998596191\n",
      "16.51267433166504\n",
      "19.328643798828125\n",
      "17.49331283569336\n",
      "20.029712677001953\n",
      "14.438898086547852\n",
      "15.762077331542969\n",
      "18.13980484008789\n",
      "14.848148345947266\n",
      "19.40007781982422\n",
      "15.315601348876953\n",
      "14.561521530151367\n",
      "17.69639778137207\n",
      "16.860546112060547\n",
      "16.512067794799805\n",
      "17.659000396728516\n",
      "13.506077766418457\n",
      "16.304250717163086\n",
      "13.525729179382324\n",
      "17.444467544555664\n",
      "16.846817016601562\n",
      "16.15239715576172\n",
      "16.865020751953125\n",
      "16.00120735168457\n",
      "15.276840209960938\n",
      "15.401061058044434\n",
      "14.379570007324219\n",
      "18.187198638916016\n",
      "15.64793872833252\n",
      "13.537896156311035\n",
      "14.015091896057129\n",
      "15.845391273498535\n",
      "15.934653282165527\n",
      "14.444786071777344\n",
      "15.207313537597656\n",
      "15.364156723022461\n",
      "15.204218864440918\n",
      "15.692951202392578\n",
      "14.603323936462402\n",
      "17.462495803833008\n",
      "15.98267936706543\n",
      "14.885257720947266\n",
      "14.285956382751465\n",
      "18.49252700805664\n",
      "16.52735710144043\n",
      "13.94791030883789\n",
      "16.784881591796875\n",
      "14.29461669921875\n",
      "13.468986511230469\n",
      "17.010793685913086\n",
      "15.892274856567383\n",
      "15.02125358581543\n",
      "15.452902793884277\n",
      "13.465361595153809\n",
      "17.802745819091797\n",
      "14.887721061706543\n",
      "15.136423110961914\n",
      "17.35032844543457\n",
      "14.99093246459961\n",
      "16.26801300048828\n",
      "16.2004451751709\n",
      "14.440089225769043\n",
      "14.81385612487793\n",
      "15.850691795349121\n",
      "12.137360572814941\n",
      "14.367033004760742\n",
      "14.79954719543457\n",
      "16.417266845703125\n",
      "16.588857650756836\n",
      "16.08413314819336\n",
      "12.187387466430664\n",
      "14.919342041015625\n",
      "14.640918731689453\n",
      "14.169142723083496\n",
      "15.167892456054688\n",
      "14.990151405334473\n",
      "14.15903091430664\n",
      "14.0613374710083\n",
      "16.4175968170166\n",
      "16.239463806152344\n",
      "15.050797462463379\n",
      "13.644845008850098\n",
      "14.794093132019043\n",
      "16.06292152404785\n",
      "13.99741268157959\n",
      "16.737728118896484\n",
      "16.672683715820312\n",
      "15.83018970489502\n",
      "12.98117446899414\n",
      "14.287178993225098\n",
      "12.808745384216309\n",
      "15.762665748596191\n",
      "15.69675064086914\n",
      "14.843871116638184\n",
      "13.763543128967285\n",
      "12.845831871032715\n",
      "14.583086967468262\n",
      "14.918920516967773\n",
      "13.135716438293457\n",
      "12.856471061706543\n",
      "15.22501277923584\n",
      "16.492097854614258\n",
      "16.456491470336914\n",
      "16.307708740234375\n",
      "15.053966522216797\n",
      "15.740930557250977\n",
      "16.306970596313477\n",
      "13.982016563415527\n",
      "14.732074737548828\n",
      "17.297487258911133\n",
      "14.134751319885254\n",
      "10.024338722229004\n",
      "13.01278305053711\n",
      "16.397693634033203\n",
      "14.770896911621094\n",
      "14.789785385131836\n",
      "13.154217720031738\n",
      "11.140446662902832\n",
      "16.323808670043945\n",
      "18.62364959716797\n",
      "14.900196075439453\n",
      "14.9029541015625\n",
      "13.320035934448242\n",
      "15.995529174804688\n",
      "12.648664474487305\n",
      "14.308650016784668\n",
      "15.992435455322266\n",
      "14.587789535522461\n",
      "16.77167320251465\n",
      "13.445528030395508\n",
      "14.618427276611328\n",
      "12.539555549621582\n",
      "13.150721549987793\n",
      "16.36263084411621\n",
      "11.44878101348877\n",
      "12.493728637695312\n",
      "13.184026718139648\n",
      "10.933272361755371\n",
      "14.363658905029297\n",
      "14.045995712280273\n",
      "12.349481582641602\n",
      "10.799763679504395\n",
      "15.279589653015137\n",
      "11.174098014831543\n",
      "17.296581268310547\n",
      "13.909006118774414\n",
      "13.403569221496582\n",
      "10.751524925231934\n",
      "13.351290702819824\n",
      "13.225395202636719\n",
      "12.943554878234863\n",
      "14.336538314819336\n",
      "11.569571495056152\n",
      "14.43799114227295\n",
      "15.568169593811035\n",
      "14.454416275024414\n",
      "12.772139549255371\n",
      "14.235771179199219\n",
      "12.840085983276367\n",
      "14.807738304138184\n",
      "14.989779472351074\n",
      "14.089458465576172\n",
      "12.900117874145508\n",
      "14.903120994567871\n",
      "13.080528259277344\n",
      "13.898828506469727\n",
      "11.936121940612793\n",
      "13.307964324951172\n",
      "13.088813781738281\n",
      "14.1068696975708\n",
      "14.343193054199219\n",
      "13.54862117767334\n",
      "13.338561058044434\n",
      "12.961875915527344\n",
      "15.455774307250977\n",
      "12.932028770446777\n",
      "11.387681007385254\n",
      "11.288647651672363\n",
      "13.077885627746582\n",
      "13.799422264099121\n",
      "14.007343292236328\n",
      "10.145062446594238\n",
      "13.261534690856934\n",
      "13.585504531860352\n",
      "13.38049602508545\n",
      "12.11259937286377\n",
      "12.337858200073242\n",
      "12.215519905090332\n",
      "11.842535018920898\n",
      "11.667643547058105\n",
      "13.615538597106934\n",
      "14.870823860168457\n",
      "10.626153945922852\n",
      "12.449109077453613\n",
      "12.287506103515625\n",
      "12.444025039672852\n",
      "11.217907905578613\n",
      "11.704901695251465\n",
      "12.135913848876953\n",
      "12.807548522949219\n",
      "9.140632629394531\n",
      "14.243926048278809\n",
      "12.460529327392578\n",
      "13.343295097351074\n",
      "13.282486915588379\n",
      "10.747922897338867\n",
      "12.578084945678711\n",
      "12.19099235534668\n",
      "12.55887222290039\n",
      "7.657298564910889\n",
      "11.44111442565918\n",
      "12.694293975830078\n",
      "11.78170108795166\n",
      "11.57570743560791\n",
      "12.672210693359375\n",
      "10.04702091217041\n",
      "11.045687675476074\n",
      "9.083961486816406\n",
      "12.263649940490723\n",
      "10.935042381286621\n",
      "10.222566604614258\n",
      "10.922508239746094\n",
      "13.1536226272583\n",
      "10.75554370880127\n",
      "11.312844276428223\n",
      "12.632792472839355\n",
      "10.89857006072998\n",
      "9.688342094421387\n",
      "11.78468132019043\n",
      "9.401041030883789\n",
      "9.104962348937988\n",
      "12.106501579284668\n",
      "10.747014999389648\n",
      "10.65234088897705\n",
      "10.806781768798828\n",
      "9.296135902404785\n",
      "10.448583602905273\n",
      "9.8267822265625\n",
      "11.854891777038574\n",
      "11.544097900390625\n",
      "10.830656051635742\n",
      "10.718188285827637\n",
      "11.946939468383789\n",
      "10.751418113708496\n",
      "12.485061645507812\n",
      "10.815839767456055\n",
      "10.638511657714844\n",
      "11.023995399475098\n",
      "12.45511531829834\n",
      "10.327531814575195\n",
      "14.094819068908691\n",
      "9.857378005981445\n",
      "10.796282768249512\n",
      "8.948864936828613\n",
      "10.675904273986816\n",
      "10.757243156433105\n",
      "9.724085807800293\n",
      "8.513525009155273\n",
      "9.626110076904297\n",
      "10.313959121704102\n",
      "9.816570281982422\n",
      "9.147692680358887\n",
      "11.01689338684082\n",
      "9.785416603088379\n",
      "8.516109466552734\n",
      "7.539109706878662\n",
      "8.717226028442383\n",
      "11.223974227905273\n",
      "9.714492797851562\n",
      "11.02951717376709\n",
      "10.03664779663086\n",
      "10.030768394470215\n",
      "7.5223002433776855\n",
      "9.46426010131836\n",
      "10.94852066040039\n",
      "11.172480583190918\n",
      "11.097553253173828\n",
      "9.410770416259766\n",
      "9.41028118133545\n",
      "9.249074935913086\n",
      "8.975244522094727\n",
      "10.312887191772461\n",
      "10.856061935424805\n",
      "9.232939720153809\n",
      "10.585594177246094\n",
      "7.14732027053833\n",
      "11.508423805236816\n",
      "8.106534004211426\n",
      "9.348033905029297\n",
      "11.362343788146973\n",
      "6.84049129486084\n",
      "10.817841529846191\n",
      "9.269675254821777\n",
      "9.94784164428711\n",
      "9.882887840270996\n",
      "10.685049057006836\n",
      "7.2797651290893555\n",
      "9.70179271697998\n",
      "11.488687515258789\n",
      "8.231929779052734\n",
      "9.795042991638184\n",
      "7.492391109466553\n",
      "7.7062788009643555\n",
      "9.317342758178711\n",
      "9.595948219299316\n",
      "7.916853427886963\n",
      "10.217494010925293\n",
      "8.679940223693848\n",
      "8.341797828674316\n",
      "7.152782917022705\n",
      "8.815406799316406\n",
      "8.99419116973877\n",
      "7.522397518157959\n",
      "7.062939643859863\n",
      "7.9471869468688965\n",
      "7.338494300842285\n",
      "6.6394734382629395\n",
      "8.023626327514648\n",
      "6.614532947540283\n",
      "10.711403846740723\n",
      "9.113948822021484\n",
      "6.643902778625488\n",
      "8.312013626098633\n",
      "5.84282922744751\n",
      "7.739659786224365\n",
      "7.642482757568359\n",
      "6.543283462524414\n",
      "7.623591423034668\n",
      "6.824116230010986\n",
      "6.682894706726074\n",
      "6.913642883300781\n",
      "6.507900238037109\n",
      "8.490614891052246\n",
      "7.085048675537109\n",
      "5.784600257873535\n",
      "6.436953067779541\n",
      "6.892958164215088\n",
      "7.727288722991943\n",
      "6.116581439971924\n",
      "5.510592460632324\n",
      "7.316522121429443\n",
      "7.132019996643066\n",
      "7.098186492919922\n",
      "5.983424663543701\n",
      "6.462995529174805\n",
      "7.745397567749023\n",
      "7.031184196472168\n",
      "6.113436698913574\n",
      "6.237428665161133\n",
      "7.329853534698486\n",
      "7.238510608673096\n",
      "6.088775634765625\n",
      "10.143604278564453\n",
      "5.885068893432617\n",
      "7.908700466156006\n",
      "5.570178985595703\n",
      "6.84552001953125\n",
      "5.340991020202637\n",
      "7.273758411407471\n",
      "7.640613079071045\n",
      "5.658376216888428\n",
      "6.117895126342773\n",
      "5.242367744445801\n",
      "5.320322513580322\n",
      "5.707038402557373\n",
      "6.581857681274414\n",
      "6.650576114654541\n",
      "6.025746822357178\n",
      "5.175989151000977\n",
      "5.904669284820557\n",
      "5.653076171875\n",
      "7.041077136993408\n",
      "6.972332000732422\n",
      "7.487135410308838\n",
      "5.997528553009033\n",
      "7.1285176277160645\n",
      "6.659600734710693\n",
      "7.652913570404053\n",
      "5.672374725341797\n",
      "6.230837345123291\n",
      "5.988131999969482\n",
      "5.56312894821167\n",
      "6.913629531860352\n",
      "5.54844856262207\n",
      "7.057718276977539\n",
      "5.840906143188477\n",
      "6.671653747558594\n",
      "5.458735942840576\n",
      "7.564720153808594\n",
      "7.232619285583496\n",
      "6.1416802406311035\n",
      "5.07193660736084\n",
      "3.8308892250061035\n",
      "4.7371673583984375\n",
      "4.838964939117432\n",
      "5.750948905944824\n",
      "4.7327423095703125\n",
      "5.501468181610107\n",
      "6.795473098754883\n",
      "3.8115060329437256\n",
      "6.8184123039245605\n",
      "5.712019920349121\n",
      "6.276068687438965\n",
      "5.069484710693359\n",
      "5.2428717613220215\n",
      "4.08136510848999\n",
      "6.028714656829834\n",
      "4.588868618011475\n",
      "6.212089538574219\n",
      "5.041914463043213\n",
      "6.159596920013428\n",
      "4.6420087814331055\n",
      "6.245718002319336\n",
      "4.780372142791748\n",
      "5.2802653312683105\n",
      "4.710434436798096\n",
      "5.486145973205566\n",
      "3.9937655925750732\n",
      "5.9279327392578125\n",
      "5.239286422729492\n",
      "3.8652994632720947\n",
      "3.4117989540100098\n",
      "5.00499153137207\n",
      "4.158966541290283\n",
      "5.508181095123291\n",
      "6.513759613037109\n",
      "4.405284881591797\n",
      "5.14586877822876\n",
      "3.7409050464630127\n",
      "3.9124462604522705\n",
      "4.7141900062561035\n",
      "4.780683517456055\n",
      "5.294166088104248\n",
      "4.4462103843688965\n",
      "3.8820509910583496\n",
      "5.264283180236816\n",
      "4.233163833618164\n",
      "4.916440010070801\n",
      "5.0952606201171875\n",
      "4.595503807067871\n",
      "4.365120887756348\n",
      "4.691381931304932\n",
      "5.0077009201049805\n",
      "3.9373669624328613\n",
      "4.561709403991699\n",
      "4.280008792877197\n",
      "5.199629306793213\n",
      "4.139196395874023\n",
      "5.259610176086426\n",
      "4.393763065338135\n",
      "4.483091831207275\n",
      "4.912574768066406\n",
      "4.402356147766113\n",
      "5.489288330078125\n",
      "5.292395114898682\n",
      "4.797728061676025\n",
      "4.962027549743652\n",
      "3.5516209602355957\n",
      "4.854546070098877\n",
      "4.941614151000977\n",
      "4.665370941162109\n",
      "4.338842391967773\n",
      "4.18110466003418\n",
      "3.322399377822876\n",
      "4.69537353515625\n",
      "4.121555805206299\n",
      "4.973320484161377\n",
      "3.404341459274292\n",
      "5.194975852966309\n",
      "4.823459625244141\n",
      "3.147590398788452\n",
      "4.8230180740356445\n",
      "4.636656761169434\n",
      "3.539262533187866\n",
      "3.124363422393799\n",
      "3.6558961868286133\n",
      "4.298759937286377\n",
      "4.106279373168945\n",
      "4.430235385894775\n",
      "4.745416164398193\n",
      "3.932953357696533\n",
      "4.816493034362793\n",
      "3.274017810821533\n",
      "4.145781993865967\n",
      "3.1956210136413574\n",
      "5.129812717437744\n",
      "4.1670355796813965\n",
      "4.1447930335998535\n",
      "4.337336540222168\n",
      "3.4680771827697754\n",
      "3.985111951828003\n",
      "4.165875434875488\n",
      "4.060172080993652\n",
      "4.203032970428467\n",
      "3.8853280544281006\n",
      "3.962561845779419\n",
      "4.575880527496338\n",
      "3.506380319595337\n",
      "3.5180580615997314\n",
      "4.910589694976807\n",
      "3.156216859817505\n",
      "3.9804694652557373\n",
      "3.6472418308258057\n",
      "3.720679521560669\n",
      "3.493952989578247\n",
      "4.091154098510742\n",
      "3.662816286087036\n",
      "3.3935248851776123\n",
      "3.4204139709472656\n",
      "4.054011821746826\n",
      "3.0683226585388184\n",
      "3.851916551589966\n",
      "3.402296304702759\n",
      "4.501344203948975\n",
      "3.872035026550293\n",
      "3.7012526988983154\n",
      "3.9847967624664307\n",
      "4.124131202697754\n",
      "3.791757106781006\n",
      "4.08024263381958\n",
      "3.8128113746643066\n",
      "2.8364508152008057\n",
      "3.70023775100708\n",
      "3.624866008758545\n",
      "3.328779458999634\n",
      "3.415194511413574\n",
      "3.0788567066192627\n",
      "3.2298338413238525\n",
      "2.9698002338409424\n",
      "3.082777261734009\n",
      "3.752819061279297\n",
      "4.752536296844482\n",
      "4.090614318847656\n",
      "3.7028181552886963\n",
      "3.572092294692993\n",
      "3.1335506439208984\n",
      "3.432230234146118\n",
      "2.9082300662994385\n",
      "3.6425719261169434\n",
      "3.9003617763519287\n",
      "3.1694233417510986\n",
      "3.4197704792022705\n",
      "3.9261250495910645\n",
      "3.504655361175537\n",
      "2.966402292251587\n",
      "2.602017402648926\n",
      "4.134428024291992\n",
      "5.205659866333008\n",
      "3.5917022228240967\n",
      "3.092893123626709\n",
      "3.134664535522461\n",
      "2.9559574127197266\n",
      "4.041383266448975\n",
      "4.934363842010498\n",
      "3.182105302810669\n",
      "3.404606819152832\n",
      "3.8770813941955566\n",
      "3.0556352138519287\n",
      "3.126648426055908\n",
      "2.9338219165802\n",
      "3.802443027496338\n",
      "3.4116451740264893\n",
      "3.2764313220977783\n",
      "3.7230262756347656\n",
      "3.420316696166992\n",
      "3.527808904647827\n",
      "2.824198007583618\n",
      "2.786712408065796\n",
      "3.1745831966400146\n",
      "2.879678249359131\n",
      "3.8843395709991455\n",
      "2.8080179691314697\n",
      "2.886155605316162\n",
      "3.631784677505493\n",
      "3.6199073791503906\n",
      "2.8942246437072754\n",
      "2.827833414077759\n",
      "2.583588123321533\n",
      "2.790894031524658\n",
      "2.8086538314819336\n",
      "4.052684783935547\n",
      "3.0323219299316406\n",
      "3.478381872177124\n",
      "3.253450632095337\n",
      "3.414245128631592\n",
      "2.7064952850341797\n",
      "2.591726541519165\n",
      "3.061180353164673\n",
      "3.4143755435943604\n",
      "3.067478656768799\n",
      "3.27402925491333\n",
      "2.741150379180908\n",
      "3.3555238246917725\n",
      "3.340144634246826\n",
      "3.0440754890441895\n",
      "2.680009365081787\n",
      "3.4850735664367676\n",
      "3.876116991043091\n",
      "2.7165141105651855\n",
      "3.123551368713379\n",
      "3.3032774925231934\n",
      "3.4447476863861084\n",
      "3.3237175941467285\n",
      "3.228152275085449\n",
      "2.559340238571167\n",
      "3.4037673473358154\n",
      "2.493147373199463\n",
      "2.6870200634002686\n",
      "3.037076473236084\n",
      "3.44100284576416\n",
      "2.8614890575408936\n",
      "2.768899440765381\n",
      "2.9465794563293457\n",
      "3.1065711975097656\n",
      "3.3522486686706543\n",
      "3.473801374435425\n",
      "3.2375340461730957\n",
      "3.402247428894043\n",
      "2.931101083755493\n",
      "2.607149124145508\n",
      "2.8261847496032715\n",
      "3.268670082092285\n",
      "3.1372408866882324\n",
      "2.8335912227630615\n",
      "2.8152871131896973\n",
      "3.3082878589630127\n",
      "3.286792755126953\n",
      "3.8926377296447754\n",
      "2.937842845916748\n",
      "3.343235969543457\n",
      "3.378918409347534\n",
      "3.5437071323394775\n",
      "3.249493360519409\n",
      "2.956608533859253\n",
      "2.533949375152588\n",
      "2.9303033351898193\n",
      "3.162491798400879\n",
      "3.284301280975342\n",
      "2.953711986541748\n",
      "3.3476006984710693\n",
      "2.56770658493042\n",
      "3.273019313812256\n",
      "3.0329580307006836\n",
      "2.7773818969726562\n",
      "2.6709518432617188\n",
      "2.8874382972717285\n",
      "2.782214879989624\n",
      "3.0453248023986816\n",
      "4.07985258102417\n",
      "3.492722749710083\n",
      "3.0238962173461914\n",
      "3.7618026733398438\n",
      "3.2650368213653564\n",
      "3.2597198486328125\n",
      "3.1022913455963135\n",
      "2.610294818878174\n",
      "4.0307745933532715\n",
      "3.1899802684783936\n",
      "2.74399733543396\n",
      "2.5985348224639893\n",
      "2.8930013179779053\n",
      "3.0645742416381836\n",
      "3.6967785358428955\n",
      "3.5656516551971436\n",
      "3.5606369972229004\n",
      "2.9955177307128906\n",
      "2.9555180072784424\n",
      "3.319535493850708\n",
      "2.8828256130218506\n",
      "3.026362419128418\n",
      "2.795319080352783\n",
      "2.860213279724121\n",
      "3.8617286682128906\n",
      "3.143113851547241\n",
      "3.476536750793457\n",
      "3.5462753772735596\n",
      "2.561617136001587\n",
      "2.439570903778076\n",
      "3.094974994659424\n",
      "2.343700408935547\n",
      "3.2005910873413086\n",
      "2.804602861404419\n",
      "3.252902030944824\n",
      "3.271667957305908\n",
      "3.5457115173339844\n",
      "3.691164016723633\n",
      "3.1616873741149902\n",
      "3.6955909729003906\n",
      "3.5022871494293213\n",
      "2.81069278717041\n",
      "3.0936992168426514\n",
      "2.9288482666015625\n",
      "3.2970967292785645\n",
      "2.6630349159240723\n",
      "3.0097482204437256\n",
      "3.0640029907226562\n",
      "3.627645969390869\n",
      "3.7542269229888916\n",
      "3.782548427581787\n",
      "3.597501516342163\n",
      "2.8858377933502197\n",
      "2.6797738075256348\n",
      "2.403812885284424\n",
      "3.062069892883301\n",
      "2.7374110221862793\n",
      "3.03902268409729\n",
      "3.0228490829467773\n",
      "2.6275405883789062\n",
      "3.5522074699401855\n",
      "3.4372966289520264\n",
      "3.4874446392059326\n",
      "4.348320960998535\n",
      "4.424899101257324\n",
      "3.248392105102539\n",
      "2.768378496170044\n",
      "3.7645084857940674\n",
      "3.810471773147583\n",
      "2.9748356342315674\n",
      "3.1028144359588623\n",
      "4.342587471008301\n",
      "4.88463020324707\n",
      "3.0152218341827393\n",
      "3.4046332836151123\n",
      "3.364896059036255\n",
      "3.2649221420288086\n",
      "3.1879618167877197\n",
      "3.1478562355041504\n",
      "3.7806146144866943\n",
      "4.350872993469238\n",
      "4.093657970428467\n",
      "3.623396396636963\n",
      "3.6238770484924316\n",
      "3.061932325363159\n",
      "2.685920238494873\n",
      "3.593657970428467\n",
      "2.583578586578369\n",
      "3.3311526775360107\n",
      "3.174792766571045\n",
      "3.3493781089782715\n",
      "2.862717866897583\n",
      "3.130369186401367\n",
      "4.275666236877441\n",
      "3.827807664871216\n",
      "3.4023773670196533\n",
      "3.3126916885375977\n",
      "3.5971903800964355\n",
      "3.1494688987731934\n",
      "4.228878974914551\n",
      "3.3575851917266846\n",
      "2.9763946533203125\n",
      "4.6081438064575195\n",
      "4.222837448120117\n",
      "3.774507522583008\n",
      "3.6313400268554688\n",
      "3.0761044025421143\n",
      "3.795215129852295\n",
      "2.8879947662353516\n",
      "3.391639471054077\n",
      "3.563333749771118\n",
      "3.322439193725586\n",
      "3.7046096324920654\n",
      "2.935865879058838\n",
      "3.0306286811828613\n",
      "4.4952545166015625\n",
      "6.666348934173584\n",
      "3.8337652683258057\n",
      "3.4597926139831543\n",
      "4.032570838928223\n",
      "3.86635684967041\n",
      "4.0791473388671875\n",
      "4.459021091461182\n",
      "3.325119972229004\n",
      "4.887445449829102\n",
      "3.764955997467041\n",
      "4.995646953582764\n",
      "4.879306316375732\n",
      "3.2193877696990967\n",
      "3.6392951011657715\n",
      "3.655754804611206\n",
      "3.5678603649139404\n",
      "3.641043186187744\n",
      "3.143179416656494\n",
      "3.523768186569214\n",
      "3.443964958190918\n",
      "3.181264877319336\n",
      "3.163816213607788\n",
      "2.881279230117798\n",
      "3.7011618614196777\n",
      "4.380613327026367\n",
      "4.83017635345459\n",
      "3.937605142593384\n",
      "4.499337196350098\n",
      "4.460763931274414\n",
      "4.349172115325928\n",
      "3.465439796447754\n",
      "3.6921679973602295\n",
      "3.147054672241211\n",
      "3.191957473754883\n",
      "3.3855178356170654\n",
      "3.9701907634735107\n",
      "4.059905529022217\n",
      "5.470564365386963\n",
      "3.859811782836914\n",
      "3.4984378814697266\n",
      "3.7478644847869873\n",
      "3.98930287361145\n",
      "3.581118583679199\n",
      "4.445273399353027\n",
      "3.834737539291382\n",
      "4.178360462188721\n",
      "3.9035141468048096\n",
      "4.184782028198242\n",
      "4.04674768447876\n",
      "4.349334716796875\n",
      "5.791870594024658\n",
      "7.2491631507873535\n",
      "4.757250785827637\n",
      "4.7839484214782715\n",
      "4.421754360198975\n",
      "3.8965141773223877\n",
      "3.990248203277588\n",
      "3.810070753097534\n",
      "4.071158409118652\n",
      "4.70634126663208\n",
      "5.62563943862915\n",
      "3.5376851558685303\n",
      "3.9212260246276855\n",
      "3.64125919342041\n",
      "3.2774643898010254\n",
      "4.312809944152832\n",
      "6.006783962249756\n",
      "4.128579139709473\n",
      "4.7215752601623535\n",
      "6.4241814613342285\n",
      "4.404778480529785\n",
      "4.23411226272583\n",
      "4.711030960083008\n",
      "3.9998528957366943\n",
      "4.405123233795166\n",
      "3.6565608978271484\n",
      "4.4241533279418945\n",
      "4.912998199462891\n",
      "5.468926429748535\n",
      "4.237133026123047\n",
      "4.72775936126709\n",
      "4.231461524963379\n",
      "4.749027252197266\n",
      "4.105771064758301\n",
      "4.375158309936523\n",
      "3.978790760040283\n",
      "5.777502059936523\n",
      "3.8091516494750977\n",
      "4.279061317443848\n",
      "4.726765155792236\n",
      "4.994780540466309\n",
      "4.775427341461182\n",
      "5.48806619644165\n",
      "4.147103309631348\n",
      "5.390119552612305\n",
      "4.276027202606201\n",
      "4.126376152038574\n",
      "4.492622375488281\n",
      "4.394174575805664\n",
      "3.8615596294403076\n",
      "5.3506388664245605\n",
      "5.214042663574219\n",
      "6.498241424560547\n",
      "5.980344772338867\n",
      "6.783231258392334\n",
      "5.517638683319092\n",
      "6.608185291290283\n",
      "5.382946014404297\n",
      "5.702431678771973\n",
      "4.110931396484375\n",
      "5.605212211608887\n",
      "5.874320983886719\n",
      "4.5511064529418945\n",
      "7.6456732749938965\n",
      "5.986496448516846\n",
      "6.455755233764648\n",
      "7.734212875366211\n",
      "6.0388336181640625\n",
      "6.480405807495117\n",
      "5.505479335784912\n",
      "7.40268611907959\n",
      "7.992928981781006\n",
      "8.823745727539062\n",
      "6.7332963943481445\n",
      "6.5259552001953125\n",
      "5.660985946655273\n",
      "7.005611419677734\n",
      "7.305471897125244\n",
      "10.981298446655273\n",
      "7.716037273406982\n",
      "6.169497966766357\n",
      "7.16051721572876\n",
      "6.134083271026611\n",
      "5.650358200073242\n",
      "7.391262054443359\n",
      "5.319962501525879\n",
      "6.571099281311035\n",
      "7.7840776443481445\n",
      "6.40421724319458\n",
      "4.662928104400635\n",
      "4.816431999206543\n",
      "5.11085319519043\n",
      "7.930141448974609\n",
      "5.219119071960449\n",
      "10.519536972045898\n",
      "6.871555805206299\n",
      "5.703021049499512\n",
      "4.548720359802246\n",
      "6.378541946411133\n",
      "5.075074195861816\n",
      "5.152325630187988\n",
      "5.182839870452881\n",
      "5.5526442527771\n",
      "6.107744216918945\n",
      "6.445827484130859\n",
      "6.7906999588012695\n",
      "4.7066192626953125\n",
      "5.505977153778076\n",
      "5.544979572296143\n",
      "7.560120582580566\n",
      "7.288840293884277\n",
      "6.472123146057129\n",
      "8.164451599121094\n",
      "7.016399383544922\n",
      "6.658350944519043\n",
      "7.054104804992676\n",
      "6.413156986236572\n",
      "6.81736421585083\n",
      "5.818386554718018\n",
      "9.020242691040039\n",
      "7.508543491363525\n",
      "6.001935005187988\n",
      "6.6922454833984375\n",
      "9.2393798828125\n",
      "Final loss: 9.2393798828125\n"
     ]
    }
   ],
   "source": [
    "# Finding the ideal learning rate\n",
    "# Parameters\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "W1 = torch.randn((6, 100), generator=g)\n",
    "b1 = torch.randn(100, generator=g)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "print(f\"Total parametrs: {sum(p.nelement() for p in parameters)}\")\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# Metrics\n",
    "lri = []\n",
    "lossi = []\n",
    "for i in range(1000):\n",
    "\n",
    "    ix = torch.randint(0, X.shape[0], (32,))\n",
    "    \n",
    "    # Forward pass\n",
    "    emb = C[X[ix]]\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Y[ix])\n",
    "    print(loss.item())\n",
    "\n",
    "    # Set gradient to None\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights\n",
    "    lr = lrs[i]\n",
    "    for p in parameters:\n",
    "        p.data += - lr * p.grad\n",
    "\n",
    "    lri.append(lre[i])\n",
    "    lossi.append(loss.item())\n",
    "print(f\"Final loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fac9afe2950>]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lri, lossi)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base on the plot, loss is less before exploding is exponent of -1.0 which is 0.1 which was our initial learning rate.\n",
    "Now we can be certain that the learning rate is good.\n",
    "Let's set lr to 0.1 and train for a longer period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parametrs: 3481\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "W1 = torch.randn((6, 100), generator=g)\n",
    "b1 = torch.randn(100, generator=g)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "print(f\"Total parametrs: {sum(p.nelement() for p in parameters)}\")\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss: 2.0804283618927\n"
     ]
    }
   ],
   "source": [
    "# Good learning rate from metrics\n",
    "for i in range(10000):\n",
    "\n",
    "    ix = torch.randint(0, X.shape[0], (32,))\n",
    "    \n",
    "    # Forward pass\n",
    "    emb = C[X[ix]]\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Y[ix])\n",
    "\n",
    "    # Set gradient to None\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights\n",
    "    lr = 0.1\n",
    "    for p in parameters:\n",
    "        p.data += - lr * p.grad\n",
    "\n",
    "\n",
    "print(f\"Final loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3025, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation loss for entire dataset\n",
    "emb = C[X]\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running traing for more than 10000 steps multiple times, we've overcome the loss of bigram model.\n",
    "Now we've arrived at a point where we are near the minima. Let's do a decay of learning rate to slowly reach there.\n",
    "\n",
    "And train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss: 2.0946085453033447\n"
     ]
    }
   ],
   "source": [
    "# Learning rate Decay\n",
    "for i in range(10000):\n",
    "\n",
    "    ix = torch.randint(0, X.shape[0], (32,))\n",
    "    \n",
    "    # Forward pass\n",
    "    emb = C[X[ix]]\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Y[ix])\n",
    "\n",
    "    # Set gradient to None\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights\n",
    "    lr = 0.01\n",
    "    for p in parameters:\n",
    "        p.data += - lr * p.grad\n",
    "\n",
    "\n",
    "print(f\"Final loss: {loss.item()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "* Find a good leaarning rate between lower and upper limit\n",
    "* Train on the ideal learning rate and check for improvement in loss\n",
    "* Verify that and do a learning rate decay by a small factor and train again\n",
    "* Validate loss again on entitre dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "206px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "21a124e163c92121797d725bed844fa6fdaf2c4e47bf1f149ef174ae791c682a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
