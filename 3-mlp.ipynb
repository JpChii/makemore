{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bigram model using probablities based on normalized counts has it's limitations.\n",
    "\n",
    "To extend it to have more context like a two characters as input the probabalities matrix will have (27*27) possilities and for three characters (27 * 27 * 27) and becomes too big.\n",
    "\n",
    "To overcome this we're gonna try out [Bengion et al.2003 MLP model paper](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbVhPSHNrVkluYVY5elh4RDZrOWd4R2xVNVRQd3xBQ3Jtc0tsUDE0UFVRQURUTTlWckExZWp4eGxFa3lRYlQ3amtYX3kxdDI1ZW5uU1pxZERidUYyZkJjSVlzd21rMndCMFlYRW5kYmZISkxfSDR1TzhaOXI1bXptUnUxU0xyUXJYeEpTZlRrTkRjTS0wTkMxNjFnSQ&q=https%3A%2F%2Fwww.jmlr.org%2Fpapers%2Fvolume3%2Fbengio03a%2Fbengio03a.pdf&v=TCH_1BHY58I)\n",
    "\n",
    "* This paper uses words but we'll proceed with characters\n",
    "* Each character will be represented as a 30 dimensional vector \n",
    "* The advantages of embeddings is knowledge transference, for examples animals like dog, cat might be closer to each other in 30 dimensional space. If cat was not in training set but this knowledge transfer will help in this case.\n",
    "\n",
    "Let's implement the below architecture in this notebook\n",
    "![fully connected MLP](https://pbs.twimg.com/media/Fhzl42hVUAI9U8V?format=jpg&name=large)\n",
    "\n",
    "* Three input characters with 30 dimensional embedding each\n",
    "* A Lookup table for characters\n",
    "* Tanh activation connected to three inputs\n",
    "* since we have 27 characters a final layer with 27 units(logits)\n",
    "* softmax on top of it to normalize the probabality\n",
    "* pluck the label based on probabality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rebuilding training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all words\n",
    "def read_words():\n",
    "    words = open(\"names.txt\").read().splitlines()\n",
    "    return words\n",
    "words = read_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi  = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "print(itos)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(block_size, number_of_words: int, logs=False):\n",
    "\n",
    "    block_size = block_size # Context ength: how many characters do we take to predict the next one?\n",
    "    X, Y = [], []\n",
    "    for w in words[:number_of_words]:\n",
    "\n",
    "        print(w)\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            print(f\"Context: {context}\")\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            if logs:\n",
    "                print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "            context = context[1:] + [ix]\n",
    "            print(f\"Context after append: {context}\")\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "Context: [0, 0, 0]\n",
      "... ---> e\n",
      "Context after append: [0, 0, 5]\n",
      "Context: [0, 0, 5]\n",
      "..e ---> m\n",
      "Context after append: [0, 5, 13]\n",
      "Context: [0, 5, 13]\n",
      ".em ---> m\n",
      "Context after append: [5, 13, 13]\n",
      "Context: [5, 13, 13]\n",
      "emm ---> a\n",
      "Context after append: [13, 13, 1]\n",
      "Context: [13, 13, 1]\n",
      "mma ---> .\n",
      "Context after append: [13, 1, 0]\n",
      "olivia\n",
      "Context: [0, 0, 0]\n",
      "... ---> o\n",
      "Context after append: [0, 0, 15]\n",
      "Context: [0, 0, 15]\n",
      "..o ---> l\n",
      "Context after append: [0, 15, 12]\n",
      "Context: [0, 15, 12]\n",
      ".ol ---> i\n",
      "Context after append: [15, 12, 9]\n",
      "Context: [15, 12, 9]\n",
      "oli ---> v\n",
      "Context after append: [12, 9, 22]\n",
      "Context: [12, 9, 22]\n",
      "liv ---> i\n",
      "Context after append: [9, 22, 9]\n",
      "Context: [9, 22, 9]\n",
      "ivi ---> a\n",
      "Context after append: [22, 9, 1]\n",
      "Context: [22, 9, 1]\n",
      "via ---> .\n",
      "Context after append: [9, 1, 0]\n",
      "ava\n",
      "Context: [0, 0, 0]\n",
      "... ---> a\n",
      "Context after append: [0, 0, 1]\n",
      "Context: [0, 0, 1]\n",
      "..a ---> v\n",
      "Context after append: [0, 1, 22]\n",
      "Context: [0, 1, 22]\n",
      ".av ---> a\n",
      "Context after append: [1, 22, 1]\n",
      "Context: [1, 22, 1]\n",
      "ava ---> .\n",
      "Context after append: [22, 1, 0]\n",
      "isabella\n",
      "Context: [0, 0, 0]\n",
      "... ---> i\n",
      "Context after append: [0, 0, 9]\n",
      "Context: [0, 0, 9]\n",
      "..i ---> s\n",
      "Context after append: [0, 9, 19]\n",
      "Context: [0, 9, 19]\n",
      ".is ---> a\n",
      "Context after append: [9, 19, 1]\n",
      "Context: [9, 19, 1]\n",
      "isa ---> b\n",
      "Context after append: [19, 1, 2]\n",
      "Context: [19, 1, 2]\n",
      "sab ---> e\n",
      "Context after append: [1, 2, 5]\n",
      "Context: [1, 2, 5]\n",
      "abe ---> l\n",
      "Context after append: [2, 5, 12]\n",
      "Context: [2, 5, 12]\n",
      "bel ---> l\n",
      "Context after append: [5, 12, 12]\n",
      "Context: [5, 12, 12]\n",
      "ell ---> a\n",
      "Context after append: [12, 12, 1]\n",
      "Context: [12, 12, 1]\n",
      "lla ---> .\n",
      "Context after append: [12, 1, 0]\n",
      "sophia\n",
      "Context: [0, 0, 0]\n",
      "... ---> s\n",
      "Context after append: [0, 0, 19]\n",
      "Context: [0, 0, 19]\n",
      "..s ---> o\n",
      "Context after append: [0, 19, 15]\n",
      "Context: [0, 19, 15]\n",
      ".so ---> p\n",
      "Context after append: [19, 15, 16]\n",
      "Context: [19, 15, 16]\n",
      "sop ---> h\n",
      "Context after append: [15, 16, 8]\n",
      "Context: [15, 16, 8]\n",
      "oph ---> i\n",
      "Context after append: [16, 8, 9]\n",
      "Context: [16, 8, 9]\n",
      "phi ---> a\n",
      "Context after append: [8, 9, 1]\n",
      "Context: [8, 9, 1]\n",
      "hia ---> .\n",
      "Context after append: [9, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "X, Y = build_dataset(block_size=3, number_of_words=5, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0],\n",
       "        [ 0,  0,  5],\n",
       "        [ 0,  5, 13],\n",
       "        [ 5, 13, 13],\n",
       "        [13, 13,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 15],\n",
       "        [ 0, 15, 12],\n",
       "        [15, 12,  9],\n",
       "        [12,  9, 22],\n",
       "        [ 9, 22,  9],\n",
       "        [22,  9,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  1],\n",
       "        [ 0,  1, 22],\n",
       "        [ 1, 22,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  9],\n",
       "        [ 0,  9, 19],\n",
       "        [ 9, 19,  1],\n",
       "        [19,  1,  2],\n",
       "        [ 1,  2,  5],\n",
       "        [ 2,  5, 12],\n",
       "        [ 5, 12, 12],\n",
       "        [12, 12,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 19],\n",
       "        [ 0, 19, 15],\n",
       "        [19, 15, 16],\n",
       "        [15, 16,  8],\n",
       "        [16,  8,  9],\n",
       "        [ 8,  9,  1]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,\n",
       "         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've the dataset, let's build the embedding lookup table\n",
    "\n",
    "## Embedding lookup table\n",
    "\n",
    "For 1700 words, 30 dimension space was used in paper. For 27 possiblities(characters) let's try a 2 dimensionsal embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.1322, -0.7408],\n",
       "         [ 0.3019,  0.8261],\n",
       "         [ 0.6245, -1.2015],\n",
       "         [-0.4238,  0.3620],\n",
       "         [ 1.4434,  0.6574],\n",
       "         [ 1.9904,  1.3509],\n",
       "         [-2.2949,  1.2116],\n",
       "         [ 0.1021,  0.7517],\n",
       "         [-1.6855,  0.0709],\n",
       "         [-0.5005,  0.2273],\n",
       "         [-0.1834, -0.3516],\n",
       "         [-0.9227,  0.4773],\n",
       "         [-1.1198,  0.1138],\n",
       "         [ 0.1110,  1.2119],\n",
       "         [-0.0134,  0.5650],\n",
       "         [-0.2966,  0.4494],\n",
       "         [ 2.3767, -0.2638],\n",
       "         [-0.3663, -1.1388],\n",
       "         [-0.0167, -1.3415],\n",
       "         [-0.5355, -0.4040],\n",
       "         [-1.7162,  0.2149],\n",
       "         [ 0.0129,  0.6510],\n",
       "         [-0.5705, -0.2869],\n",
       "         [ 2.1717, -0.0403],\n",
       "         [-0.4718,  1.3111],\n",
       "         [ 0.4909, -0.4099],\n",
       "         [ 2.1120,  0.9359]]),\n",
       " torch.Size([27, 2]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialized randomnly\n",
    "C = torch.randn((27, 2))\n",
    "C, C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.9904, 1.3509])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The lookup of embedding for single character can be done two ways\n",
    "# 1. Indexing\n",
    "C[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.9904, 1.3509])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Onehot\n",
    "F.one_hot(torch.tensor(5), num_classes=27).float() @ C"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing and one hot encoding gives the same results. We'll use indexing as it's faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.9904,  1.3509],\n",
      "        [-2.2949,  1.2116],\n",
      "        [ 0.1021,  0.7517]])\n",
      "tensor([[ 1.9904,  1.3509],\n",
      "        [-2.2949,  1.2116],\n",
      "        [ 0.1021,  0.7517]])\n"
     ]
    }
   ],
   "source": [
    "# Indexing multiple values\n",
    "# Singce our shape of input is 32, 3\n",
    "print(C[[5, 6, 7]])\n",
    "# Works also with tensor\n",
    "print(C[torch.tensor([5, 6, 7])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1322, -0.7408],\n",
       "         [-1.1322, -0.7408],\n",
       "         [-1.1322, -0.7408]],\n",
       "\n",
       "        [[-1.1322, -0.7408],\n",
       "         [-1.1322, -0.7408],\n",
       "         [ 1.9904,  1.3509]],\n",
       "\n",
       "        [[-1.1322, -0.7408],\n",
       "         [ 1.9904,  1.3509],\n",
       "         [ 0.1110,  1.2119]],\n",
       "\n",
       "        [[ 1.9904,  1.3509],\n",
       "         [ 0.1110,  1.2119],\n",
       "         [ 0.1110,  1.2119]],\n",
       "\n",
       "        [[ 0.1110,  1.2119],\n",
       "         [ 0.1110,  1.2119],\n",
       "         [ 0.3019,  0.8261]],\n",
       "\n",
       "        [[-1.1322, -0.7408],\n",
       "         [-1.1322, -0.7408],\n",
       "         [-1.1322, -0.7408]],\n",
       "\n",
       "        [[-1.1322, -0.7408],\n",
       "         [-1.1322, -0.7408],\n",
       "         [-0.2966,  0.4494]],\n",
       "\n",
       "        [[-1.1322, -0.7408],\n",
       "         [-0.2966,  0.4494],\n",
       "         [-1.1198,  0.1138]],\n",
       "\n",
       "        [[-0.2966,  0.4494],\n",
       "         [-1.1198,  0.1138],\n",
       "         [-0.5005,  0.2273]],\n",
       "\n",
       "        [[-1.1198,  0.1138],\n",
       "         [-0.5005,  0.2273],\n",
       "         [-0.5705, -0.2869]],\n",
       "\n",
       "        [[-0.5005,  0.2273],\n",
       "         [-0.5705, -0.2869],\n",
       "         [-0.5005,  0.2273]],\n",
       "\n",
       "        [[-0.5705, -0.2869],\n",
       "         [-0.5005,  0.2273],\n",
       "         [ 0.3019,  0.8261]],\n",
       "\n",
       "        [[-1.1322, -0.7408],\n",
       "         [-1.1322, -0.7408],\n",
       "         [-1.1322, -0.7408]],\n",
       "\n",
       "        [[-1.1322, -0.7408],\n",
       "         [-1.1322, -0.7408],\n",
       "         [ 0.3019,  0.8261]],\n",
       "\n",
       "        [[-1.1322, -0.7408],\n",
       "         [ 0.3019,  0.8261],\n",
       "         [-0.5705, -0.2869]],\n",
       "\n",
       "        [[ 0.3019,  0.8261],\n",
       "         [-0.5705, -0.2869],\n",
       "         [ 0.3019,  0.8261]],\n",
       "\n",
       "        [[-1.1322, -0.7408],\n",
       "         [-1.1322, -0.7408],\n",
       "         [-1.1322, -0.7408]],\n",
       "\n",
       "        [[-1.1322, -0.7408],\n",
       "         [-1.1322, -0.7408],\n",
       "         [-0.5005,  0.2273]],\n",
       "\n",
       "        [[-1.1322, -0.7408],\n",
       "         [-0.5005,  0.2273],\n",
       "         [-0.5355, -0.4040]],\n",
       "\n",
       "        [[-0.5005,  0.2273],\n",
       "         [-0.5355, -0.4040],\n",
       "         [ 0.3019,  0.8261]],\n",
       "\n",
       "        [[-0.5355, -0.4040],\n",
       "         [ 0.3019,  0.8261],\n",
       "         [ 0.6245, -1.2015]],\n",
       "\n",
       "        [[ 0.3019,  0.8261],\n",
       "         [ 0.6245, -1.2015],\n",
       "         [ 1.9904,  1.3509]],\n",
       "\n",
       "        [[ 0.6245, -1.2015],\n",
       "         [ 1.9904,  1.3509],\n",
       "         [-1.1198,  0.1138]],\n",
       "\n",
       "        [[ 1.9904,  1.3509],\n",
       "         [-1.1198,  0.1138],\n",
       "         [-1.1198,  0.1138]],\n",
       "\n",
       "        [[-1.1198,  0.1138],\n",
       "         [-1.1198,  0.1138],\n",
       "         [ 0.3019,  0.8261]],\n",
       "\n",
       "        [[-1.1322, -0.7408],\n",
       "         [-1.1322, -0.7408],\n",
       "         [-1.1322, -0.7408]],\n",
       "\n",
       "        [[-1.1322, -0.7408],\n",
       "         [-1.1322, -0.7408],\n",
       "         [-0.5355, -0.4040]],\n",
       "\n",
       "        [[-1.1322, -0.7408],\n",
       "         [-0.5355, -0.4040],\n",
       "         [-0.2966,  0.4494]],\n",
       "\n",
       "        [[-0.5355, -0.4040],\n",
       "         [-0.2966,  0.4494],\n",
       "         [ 2.3767, -0.2638]],\n",
       "\n",
       "        [[-0.2966,  0.4494],\n",
       "         [ 2.3767, -0.2638],\n",
       "         [-1.6855,  0.0709]],\n",
       "\n",
       "        [[ 2.3767, -0.2638],\n",
       "         [-1.6855,  0.0709],\n",
       "         [-0.5005,  0.2273]],\n",
       "\n",
       "        [[-1.6855,  0.0709],\n",
       "         [-0.5005,  0.2273],\n",
       "         [ 0.3019,  0.8261]]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The total equivalent would be\n",
    "C[X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's verify this\n",
    "C[X].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "32 is total number of inputs with shape 3 and dimensional embedding 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[13, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3019, 0.8261])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X][13, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3019, 0.8261])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X]\n",
    "emb.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the embedding lookup table is completed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the hidden layer plus internals of torch.Tensor, storage and views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intitializing weights and biases\n",
    "W1 = torch.randn((\n",
    "    6, # 3(inputs) * 2(embedding dim)\n",
    "    100 # Number of neurons\n",
    "))\n",
    "b1 = torch.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 100])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (96x2 and 6x100)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/jayaprakashsivagami/Documents/Tech/ML/makemore/3-mlp.ipynb Cell 31\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jayaprakashsivagami/Documents/Tech/ML/makemore/3-mlp.ipynb#X63sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Inputs * weights + bias will not work  now\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jayaprakashsivagami/Documents/Tech/ML/makemore/3-mlp.ipynb#X63sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# as dimensions of weighs and input doesn't abide\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jayaprakashsivagami/Documents/Tech/ML/makemore/3-mlp.ipynb#X63sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# by matrix multiplication rulees\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jayaprakashsivagami/Documents/Tech/ML/makemore/3-mlp.ipynb#X63sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# shape of input [32, 3, 2], weights [6, 100]\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jayaprakashsivagami/Documents/Tech/ML/makemore/3-mlp.ipynb#X63sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m emb \u001b[39m@\u001b[39;49m W1 \u001b[39m+\u001b[39m b1\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (96x2 and 6x100)"
     ]
    }
   ],
   "source": [
    "# Inputs * weights + bias will not work  now\n",
    "# as dimensions of weighs and input doesn't abide\n",
    "# by matrix multiplication rulees\n",
    "# shape of input [32, 3, 2], weights [6, 100]\n",
    "emb @ W1 + b1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch's tensor's a really powerful, because ut has tons of methods to allow us to create modify and perfom lot's of operations on it.\n",
    "\n",
    "We're gonna use [torch.cat](https://pytorch.org/docs/stable/generated/torch.cat.html) to tackle the above problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_tensors = torch.cat([emb[:, 0, :], emb[:, 1, :], emb[:, 2, :]], 1)\n",
    "cat_tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To generalize this in case of diffrent block size\n",
    "# We'll use unbind with cat\n",
    "unbind_tensors = torch.unbind(emb, 1)\n",
    "# Gives a list which is exactly the same\n",
    "# as cat_tensors abov\n",
    "len(unbind_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_unbind_tensors = torch.cat(unbind_tensors, 1)\n",
    "cat_unbind_tensors.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now irrespective of block size the above code will run.\n",
    "\n",
    "But there's an efficient way to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(18)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1],\n",
       "         [ 2,  3],\n",
       "         [ 4,  5]],\n",
       "\n",
       "        [[ 6,  7],\n",
       "         [ 8,  9],\n",
       "         [10, 11]],\n",
       "\n",
       "        [[12, 13],\n",
       "         [14, 15],\n",
       "         [16, 17]]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(3, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 6\n",
       " 7\n",
       " 8\n",
       " 9\n",
       " 10\n",
       " 11\n",
       " 12\n",
       " 13\n",
       " 14\n",
       " 15\n",
       " 16\n",
       " 17\n",
       "[torch.storage._TypedStorage(dtype=torch.int64, device=cpu) of size 18]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.storage()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every tensor has view and storage,\n",
    "* Using tensor.view(shape) we can manipulate the shape of an tensor\n",
    "* But tensor.storage() in memory will still remain a single dimension vector\n",
    "* And using view just changes some attributes like offest etc and tensor in memory remains same to the multiples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1322, -0.7408, -1.1322, -0.7408, -1.1322, -0.7408],\n",
       "        [-1.1322, -0.7408, -1.1322, -0.7408,  1.9904,  1.3509],\n",
       "        [-1.1322, -0.7408,  1.9904,  1.3509,  0.1110,  1.2119],\n",
       "        [ 1.9904,  1.3509,  0.1110,  1.2119,  0.1110,  1.2119],\n",
       "        [ 0.1110,  1.2119,  0.1110,  1.2119,  0.3019,  0.8261],\n",
       "        [-1.1322, -0.7408, -1.1322, -0.7408, -1.1322, -0.7408],\n",
       "        [-1.1322, -0.7408, -1.1322, -0.7408, -0.2966,  0.4494],\n",
       "        [-1.1322, -0.7408, -0.2966,  0.4494, -1.1198,  0.1138],\n",
       "        [-0.2966,  0.4494, -1.1198,  0.1138, -0.5005,  0.2273],\n",
       "        [-1.1198,  0.1138, -0.5005,  0.2273, -0.5705, -0.2869],\n",
       "        [-0.5005,  0.2273, -0.5705, -0.2869, -0.5005,  0.2273],\n",
       "        [-0.5705, -0.2869, -0.5005,  0.2273,  0.3019,  0.8261],\n",
       "        [-1.1322, -0.7408, -1.1322, -0.7408, -1.1322, -0.7408],\n",
       "        [-1.1322, -0.7408, -1.1322, -0.7408,  0.3019,  0.8261],\n",
       "        [-1.1322, -0.7408,  0.3019,  0.8261, -0.5705, -0.2869],\n",
       "        [ 0.3019,  0.8261, -0.5705, -0.2869,  0.3019,  0.8261],\n",
       "        [-1.1322, -0.7408, -1.1322, -0.7408, -1.1322, -0.7408],\n",
       "        [-1.1322, -0.7408, -1.1322, -0.7408, -0.5005,  0.2273],\n",
       "        [-1.1322, -0.7408, -0.5005,  0.2273, -0.5355, -0.4040],\n",
       "        [-0.5005,  0.2273, -0.5355, -0.4040,  0.3019,  0.8261],\n",
       "        [-0.5355, -0.4040,  0.3019,  0.8261,  0.6245, -1.2015],\n",
       "        [ 0.3019,  0.8261,  0.6245, -1.2015,  1.9904,  1.3509],\n",
       "        [ 0.6245, -1.2015,  1.9904,  1.3509, -1.1198,  0.1138],\n",
       "        [ 1.9904,  1.3509, -1.1198,  0.1138, -1.1198,  0.1138],\n",
       "        [-1.1198,  0.1138, -1.1198,  0.1138,  0.3019,  0.8261],\n",
       "        [-1.1322, -0.7408, -1.1322, -0.7408, -1.1322, -0.7408],\n",
       "        [-1.1322, -0.7408, -1.1322, -0.7408, -0.5355, -0.4040],\n",
       "        [-1.1322, -0.7408, -0.5355, -0.4040, -0.2966,  0.4494],\n",
       "        [-0.5355, -0.4040, -0.2966,  0.4494,  2.3767, -0.2638],\n",
       "        [-0.2966,  0.4494,  2.3767, -0.2638, -1.6855,  0.0709],\n",
       "        [ 2.3767, -0.2638, -1.6855,  0.0709, -0.5005,  0.2273],\n",
       "        [-1.6855,  0.0709, -0.5005,  0.2273,  0.3019,  0.8261]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's use view() to reshape the tensor from [32, 3, 2] to [32, 6]\n",
    "emb.view(32, 6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way this happens is dimension 1 get stacked up as a single dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.view(32, 6).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.view(32, 6) == torch.cat(torch.unbind(emb, 1), 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Element wise comparison proves that view is equal to cat(unbind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9346,  0.9008,  0.9908,  ..., -0.9937, -0.2760, -0.2226],\n",
       "        [-0.9857,  0.9408,  0.9988,  ..., -0.9584, -0.9922,  0.1073],\n",
       "        [-0.9947,  0.9016,  0.9900,  ...,  0.3018, -0.0542,  0.9769],\n",
       "        ...,\n",
       "        [-0.8981,  0.9899,  0.9898,  ...,  0.9079,  0.9961,  0.0368],\n",
       "        [-0.9951,  1.0000,  0.6883,  ..., -0.9892, -0.3642,  0.7588],\n",
       "        [-0.9949,  0.9719,  0.8945,  ..., -0.9817, -0.7537,  0.7307]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure broadcasting is done right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(emb.view(32, 6) @ W1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "32, 100\n",
    "1 , 100\n",
    "\n",
    "* broadcasting 32, 100 to 100\n",
    "* broadcasting aligns from right abd creates a  fake dimension (1)\n",
    "* Then 32 will be copied vertically for every element of 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "21a124e163c92121797d725bed844fa6fdaf2c4e47bf1f149ef174ae791c682a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
