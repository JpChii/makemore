{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context:\n",
    "\n",
    "In previous notebook (4-activations...ipynb), we utilized PyTorch's autograd(loss.backwards()) for backpropogation. It's bad to use autograd from frameworks without learning it's internals, becuase we won't know why it's performing well or not. We've implemented our own backpropogation for scalars in micrograd but implementing backpropogation instead of frameworks autograd will help to improve debugging neural nets.\n",
    "As we'll learn the internals of backpropgation it will help more on our undersanding.\n",
    "\n",
    "# Makemore 5: Becoming a backprop ninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open(\"names.txt\").read().splitlines()\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build dataset\n",
    "block_size = 3 # contet length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boilerplate done,to the action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to compare manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad)\n",
    "  maxdiff = (dt - t.grad).abs().max().item()\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embed = 10 # dimensionality of character embedding vectors\n",
    "n_hidden = 64 # number of neurons in hidden layer of MLP\n",
    "torch_seed = 2147483647\n",
    "\n",
    "g = torch.Generator().manual_seed(torch_seed) # for reproducability\n",
    "C = torch.randn(vocab_size, n_embed)\n",
    "\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embed * block_size, n_hidden), generator=g) * (5/3)/((n_embed * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden, generator=g) * 0.1 # just for understanding, useless because of batch normalization\n",
    "#Layer 2 \n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size, generator=g) * 0.1\n",
    "# Batch norm paramters\n",
    "bngain = torch.randn((1, n_hidden)) * 0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden)) * 0.1 \n",
    "\n",
    "# Instead of zeros, retaining a samll number, \n",
    "# because sometimes initializing with all zeros could mask an incorrect implementation of backward pass\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size # shorter variable for conveniance\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3206, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
    "emb = C[Xb] # embed chars into vectos\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenat the vectors\n",
    "\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "\n",
    "# BatchNorm layer\n",
    "bnmeani = 1 / n*hprebn.sum(0, keepdim=True) # equivalvelnt of torch.mean(0, keepdim=True)\n",
    "\n",
    "# hprebn - hprebn_mean\n",
    "bndiff = hprebn - bnmeani\n",
    "\n",
    "# Variance - average squared deviations from mean\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction dividing by n-1 not n\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5 # 1 /square roor -> -0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact) # hidden layer\n",
    "\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2 # output layer\n",
    "\n",
    "# Cross entropy loss (same as F.cross_entropy(logits))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # Subrac max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv,logits,\n",
    "          norm_logits, logit_maxes, h, hpreact, bnraw,\n",
    "          bnvar_inv, bnvar, bndiff2,bndiff, hprebn, bnmeani, embcat, emb]:\n",
    "    t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercis 1: Backpropogating atomic compute graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dlogprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dlogprobs\n",
    "# dlogprobs is logprobs derivate with respect to loss\n",
    "# how loss is influenced by dlogprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32]))"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs.shape, Yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.5019, -3.0272, -3.4331, -2.5303, -2.8199, -3.7948, -2.9462, -3.2546,\n",
       "        -2.3255, -2.9462, -3.8624, -4.1164, -2.6722, -2.3651, -2.7131, -4.1171,\n",
       "        -2.5303, -3.4098, -3.4174, -3.5334, -2.7322, -3.6698, -3.7884, -3.3887,\n",
       "        -3.6381, -3.3902, -3.0043, -3.5387, -3.9678, -4.4241, -3.5814, -3.8184],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plucking out correct index for character out of (27) for each input in the batch based on Yb(correct index)\n",
    "# doing a mean of these values and negative\n",
    "logprobs[range(n), Yb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = -(a + b + c)/3\n",
    "# We've 32 characters so which is batch\n",
    "# loss = -(a + b + ....)/32\n",
    "# loss = -a/32 + -b/32 +......\n",
    "# dloss/da = -1/32\n",
    "# -1/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of logprobs where indexes are plucked out is -1/n. What about the other indexes which are not plucked out. Since they don't participate in loss. The derivative of those indices will be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "# Setting those indices to 1/n\n",
    "dlogprobs[range(n), Yb] = -1.0/n\n",
    "cmp('logprobs', dlogprobs, logprobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probs\n",
    "# how probs is affecting logprobs\n",
    "# logprobs is log of probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_10(x)\n",
    "# 1 / x * ln(10)\n",
    "# ln(10) = log_e(10)\n",
    "# e = 2.71288\n",
    "# log(x)\n",
    "# 1 / x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logprobs = probs.log()\n",
    "# dlogprobs/respect to probs == 1/probs ln(probs)\n",
    "# ln(probs) = log_e(probs) where e = 2.71288\n",
    "# torch.log() - uses natural log\n",
    "# So dlogprobs/probs = 1 / probs * local_derivative(by chain rule\n",
    "# dlogprobs/probs = 1 / probs * dlogporbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above derivative, i initially assume torch.log() is base 10 and concluded the derivative of torch.log(x) as 1 / x ln (10).\n",
    "After reading [torch.log](https://pytorch.org/docs/stable/generated/torch.log.html#torch.log) the implementation itself is natural log. Derivate of log(x) will simply be 1/x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "dprobs = 1 / probs * dlogprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dprobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dprobs          | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "cmp('dprobs', dprobs, probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### counts_sum_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how probs is affected by counts_sum_inv\n",
    "# probs = counts * counts_sum_inv\n",
    "# dprobs / counts_sum_inv = counts * local_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcounts_sum_inv = counts * dprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32, 1]))"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.shape, counts_sum_inv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the shapes, in forward pass implict tensor broadcasting is done by PyTorch to perform matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1]), torch.Size([32, 27]))"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_sum_inv.shape, dcounts_sum_inv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_sum_inv.grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Doing a sum at dim 1 to match shape to accomodate tensor broadcasting\n",
    "dcounts_sum_inv = dcounts_sum_inv.sum(1, keepdims=True)\n",
    "dcounts_sum_inv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dcounts_sum_inv | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "cmp('dcounts_sum_inv', dcounts_sum_inv, counts_sum_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### counts_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts_sum with respect to counts_sum_inv\n",
    "# dcounts_sum_inv / dcounts_sum = ??\n",
    "# counts_sum_inv = counts_sum**-1\n",
    "# derivative of x**-1 -> -1/x**2\n",
    "# dcounts_sum_inv / dcounts_sum = -1 / counts_sum ** 2 * dcounts_sum_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1]), torch.Size([32, 1]))"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_sum.shape, counts_sum_inv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcounts_sum = (-1.0/counts_sum**2) * dcounts_sum_inv\n",
    "dcounts_sum_inv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shapes hold good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "cmp('counts_sum', dcounts_sum, counts_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dcounts\n",
    "# dcounts has two gradients as it influences probs and counts_sum\n",
    "# we'll need dprobs/dcounts and dcounts_sum/dcounts\n",
    "# dprobs/dcounts It's multiplication\n",
    "# dprobs/dcounts = counts_sum_inv * local_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcounts = counts_sum_inv * dprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32, 27]))"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcounts.shape, counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dcounts_sum/dcounts\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "# Derivative of addition is 1, so gradients just passes through\n",
    "# to keep shapes, we'll create ones of counts shape and multiply local gradient with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32, 1]))"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.shape, counts_sum.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've to derive array of gradients from counts_sum.\n",
    "Addition just routes local gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# local gradient\n",
    "dcounts_sum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ones of counts shape\n",
    "torch.ones_like(counts).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcasting [32, 27] with [32, 1] to create array of gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# += to add previous gradinet dprobs/dcount\n",
    "dcounts1 = torch.ones_like(counts) * dcounts_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32, 27]))"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcounts.shape, dcounts1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcounts = dcounts + dcounts1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dcounts         | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "cmp('dcounts', dcounts, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### norm_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts = norm_logits.exp()\n",
    "# derivative of exp() is exp() itself\n",
    "# dcounts / dnorm_logits = norm_logits.exp() * dcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcounts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnorm_logits = norm_logits.exp() * dcounts\n",
    "dnorm_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dnorm_logits    | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "cmp('dnorm_logits', dnorm_logits, norm_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logit_maxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm_logits = logits - logit_maxes\n",
    "# dnorm_logits / dlogit_maxes = logits/dlogit_maxes - dlogit_maxex/dlogit_maxes\n",
    "# = - 1 * dnorm_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1]), torch.Size([32, 27]), torch.Size([32, 27]))"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_maxes.shape,logits.shape, dnorm_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogit_maxes = -1 * dnorm_logits\n",
    "dlogit_maxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogit_maxes = dlogit_maxes.sum(1, keepdim=True)\n",
    "dlogit_maxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dlogit_maxes    | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "cmp('dlogit_maxes', dlogit_maxes, logit_maxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dlogits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits has two dreivatives\n",
    "# dnorm_logits/dlogits and dlogit_maxes / dlogits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dnorm_logits/dlogits = 1 * dnorm_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits = 1.0 * dnorm_logits\n",
    "dlogits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dlogit_maxes / dlogits\n",
    "# torch.max() takes max of the dimension\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# 0th dimension has zero gradients and 1th dimension has gradient of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_maxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits_logit_maxes = torch.zeros_like(logits)\n",
    "dlogits_logit_maxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.]])"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits_logit_maxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits_logit_maxes[range(n), logits.max(1).indices] = 1.0\n",
    "dlogits_logit_maxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits_logit_maxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa56c9e1db0>"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(dlogits_logit_maxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32, 1]))"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits_logit_maxes.shape, dlogit_maxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor broadcasting makse sure that dlogit_maxes values are forwarded only\n",
    "# on the bits turned on in dlogits_logit_maxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlogits_logit_maxes = dlogits_logit_maxes * dlogit_maxes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlogits += dlogits_logit_maxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dlogits         | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "cmp('dlogits', dlogits, logits)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dh, dW2, db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how h inflluences logits\n",
    "# local gradient --> dlogits\n",
    "# logits = h @ W2 + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 27]),\n",
       " torch.Size([27]),\n",
       " torch.Size([32, 64]),\n",
       " torch.Size([32, 27]))"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2.shape, b2.shape, h.shape, logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0143, -0.0895, -0.0872,  ...,  0.0151, -0.0417,  0.0593],\n",
       "        [-0.0297,  0.0812, -0.0663,  ..., -0.0527,  0.1308,  0.2315],\n",
       "        [ 0.0256,  0.0271,  0.2262,  ...,  0.1242,  0.0248, -0.0531],\n",
       "        ...,\n",
       "        [-0.1393,  0.1201, -0.1010,  ..., -0.0118, -0.0138,  0.0103],\n",
       "        [-0.1787, -0.1160,  0.1549,  ..., -0.0052, -0.1179, -0.1299],\n",
       "        [-0.0749, -0.0890, -0.1501,  ..., -0.0905, -0.1335,  0.0851]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt dh1](images/dh1.jpeg)\n",
    "![Alt dh2](images/dh2.jpeg)\n",
    "![Alt dh3](images/dh3.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 27]),\n",
       " torch.Size([27]),\n",
       " torch.Size([32, 64]),\n",
       " torch.Size([32, 27]))"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at the shapes again\n",
    "W2.shape, b2.shape, h.shape, logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We know local gradient is dlogits-> [32, 27]\n",
    "# shape of h which is shape of dh -> [32, 64]\n",
    "# To get this shape, we've to transpose W2 [64, 27] -> [27, 64]\n",
    "# Multiplying these which is dlogits @ W2T we arrive at the derived formula above\n",
    "dh = dlogits @ W2.T\n",
    "# Similarly\n",
    "dW2 = h.T @ dlogits\n",
    "# db2 -> sum of dlogits\n",
    "# b2 shape -> [27]\n",
    "# dlogits shape -> [32, 27]\n",
    "# sum at 0 axis will give desired shape which is deerivate as well\n",
    "db2 = dlogits.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 27]), torch.Size([64, 27]))"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dW2.shape, W2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dhpreact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dhpreact == ?\n",
    "# localgradient = dh\n",
    "# h = torch.tanh(hpreact)\n",
    "# dh/dhpreact = 1 - h**2 * dh"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt derivativeoftanh](http://ronny.rest/media/blog/2017/2017_08_16_tanh/tanh_and_derivative_formulas.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derivative of tanh is 1 - output**2\n",
    "# here output is h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64]), torch.Size([32, 64]))"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpreact.shape, dh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "dhpreact = (1 - h**2) * dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dhpreact        | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "cmp(\"dhpreact\", dhpreact, hpreact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dbngain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how bngain impacts hpreact\n",
    "# localgradient -> dhpreact\n",
    "# dhpreact/dbngain = bnraw * dhpreact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64]), torch.Size([32, 64]))"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking out shapes\n",
    "bngain.shape, dhpreact.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64])"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbngain = bnraw * dhpreact\n",
    "dbngain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64])"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbngain = dbngain.sum(0, keepdim=True)\n",
    "dbngain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbngain         | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "cmp(\"dbngain\", dbngain, bngain)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dbnbias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how bnbias impacts hpreact\n",
    "# local gradient -> dhpreact\n",
    "# dhpreact/dbnbias = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64]), torch.Size([32, 64]))"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at shapes\n",
    "bnbias.shape, dhpreact.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbnbias = dhpreact.clone().sum(0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbnbias         | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "cmp(\"dbnbias\", dbnbias, bnbias)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dbnraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How bnraw impacts hpreact\n",
    "# localgradient = dhpreact\n",
    "# dhpreact / dbnraw = bngain * dhpreact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64]), torch.Size([32, 64]))"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking out shpaes\n",
    "dhpreact.shape, bnraw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbnraw = bngain * dhpreact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbnraw          | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "cmp(\"dbnraw\", dbnraw, bnraw)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bnvar_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how bnvar_inv impacts bnraw\n",
    "# local gradient -> dbnraw\n",
    "# dbnraw/dbnvar_inv = bndiff * dbnraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64]), torch.Size([32, 64]), torch.Size([1, 64]))"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at shapes\n",
    "bndiff.shape, dbnraw.shape, bnvar_inv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbnvar_inv      | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "cmp(\"dbnvar_inv\", dbnvar_inv, bnvar_inv)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dbnvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how bnvar affects bnvar_inv\n",
    "# local gradient -> dbnvar_inv\n",
    "# dbnvar = -0.5 * (bnvar + 1e-5)**-1.5\n",
    "# power rule d/dx x**n = nx**n-1\n",
    "# plus chain rule on bnvar + 1e-5\n",
    "# Since it's addition derivative of this is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64]), torch.Size([1, 64]))"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking shapes\n",
    "dbnvar_inv.shape,bnvar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbnvar = (-0.5 * (bnvar + 1e-5)**-1.5) * dbnvar_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbnvar          | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "cmp(\"dbnvar\", dbnvar, bnvar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bndiff2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64]), torch.Size([1, 64]))"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bndiff2.shape,bnvar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([171.2661], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bndiff2[0].sum(0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's essentially occuring is sum along 0'th axis of bnvar\n",
    "# multiplied by 1 /(n-1)\n",
    "# differentiation of addition is 1\n",
    "# local gradient -> bnvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the shape of bndiff, since derivatives are 1\n",
    "dbndiff2 = (1.0/(n-1)) * torch.ones_like(bndiff2) * dbnvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Above we allowef broadcasting to take care of shape for us\n",
    "# bndiff2 -> (32, 64)\n",
    "# dbnvar -> (1, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bndiff2         | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "cmp(\"bndiff2\", dbndiff2, bndiff2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dbndiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How bndiff impacts bndiff2\n",
    "# bndiff2 = bnidff**2\n",
    "# local gradient = dbndiff2\n",
    "# dbndiff2/dbndiff = 2nbdiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64]), torch.Size([32, 64]))"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at shapes\n",
    "bndiff.shape, dbndiff2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbndiff = (2.0 * bndiff) * dbndiff2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbndiff         | exact: False | approximate: False | maxdiff: 0.006379772908985615\n"
     ]
    }
   ],
   "source": [
    "cmp(\"dbndiff\", dbndiff, bndiff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hpreact         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bngain          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnbias          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnraw           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff2         | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(n), Yb] = -1.0/n\n",
    "dprobs = 1/probs * dlogprobs\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdims=True)\n",
    "dcounts_sum = (-1.0/counts_sum**2) * dcounts_sum_inv\n",
    "dcounts = counts_sum_inv * dprobs\n",
    "dcounts += torch.ones_like(counts) * dcounts_sum\n",
    "dnorm_logits = counts * dcounts\n",
    "dlogits =  dnorm_logits.clone() # equivalent to 1 * dnorm_logits\n",
    "dlogit_maxes = (-dnorm_logits).sum(1, keepdims=True)\n",
    "# Another way of implementing \n",
    "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
    "dh = dlogits @ W2.T\n",
    "dW2 = h.T @ dlogits\n",
    "db2 = dlogits.sum(0)\n",
    "dhpreact = (1 - h**2) * dh\n",
    "dbngain = (bnraw * dhpreact).sum(0, keepdims=True)\n",
    "dbnbias = dhpreact.clone().sum(0, keepdims=True)\n",
    "dbnraw = bngain * dhpreact\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdims=True)\n",
    "dbnvar = (-0.5 * (bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "dbndiff2 = (1.0/(n-1)) * torch.ones_like(bndiff2) * dbnvar\n",
    "\n",
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "cmp('counts', dcounts, counts)\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('logits', dlogits, logits)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "# cmp('bndiff', dbndiff, bndiff)\n",
    "# cmp('bnmeani', dbnmeani, bnmeani)\n",
    "# cmp('hprebn', dhprebn, hprebn)\n",
    "# cmp('embcat', dembcat, embcat)\n",
    "# cmp('W1', dW1, W1)\n",
    "# cmp('b1', db1, b1)\n",
    "# cmp('emb', demb, emb)\n",
    "# cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "21a124e163c92121797d725bed844fa6fdaf2c4e47bf1f149ef174ae791c682a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
